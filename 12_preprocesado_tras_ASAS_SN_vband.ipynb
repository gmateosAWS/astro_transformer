{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155786fb",
   "metadata": {},
   "source": [
    "# 12. Preprocesado tras incorporar ASAS-SN v band al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0d3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de entorno aplicada.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n general para evitar errores de warnings y compatibilidad\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "print(\"Configuraci√≥n de entorno aplicada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f675b",
   "metadata": {},
   "source": [
    "Dentro del script, este es el preprocesado que se hace por cada curva:\n",
    "\n",
    "| Paso                                   | ¬øIncluido?    | Descripci√≥n                                                               |\n",
    "| -------------------------------------- | ------------- | ------------------------------------------------------------------------- |\n",
    "| Orden temporal                         | ‚ùå (impl√≠cito) | No se fuerza expl√≠citamente `sort_values('tiempo')`, pero puede a√±adirse. |\n",
    "| Sustituci√≥n de `NaN`                   | ‚úÖ             | Usa `np.nan_to_num` con la mediana.                                       |\n",
    "| Filtro por longitud m√≠nima             | ‚úÖ             | `if len(magnitudes) < MIN_POINTS`                                         |\n",
    "| Filtro por dispersi√≥n m√≠nima (`std`)   | ‚úÖ             | `if np.std(magnitudes) < MIN_STD`                                         |\n",
    "| Normalizaci√≥n robusta (mediana/IQR)    | ‚úÖ             | `(magnitudes - median) / iqr`                                             |\n",
    "| Clip de valores extremos               | ‚úÖ             | `np.clip(..., -1000, 1000)`                                               |\n",
    "| Padding y atenci√≥n mask                | ‚úÖ             | Rellena hasta `seq_length`, y genera `attention_mask`                     |\n",
    "| Validaci√≥n de `nan`/`inf` tras normal. | ‚úÖ             | Verifica si hay valores no v√°lidos despu√©s de normalizar.                 |\n",
    "| Normalizaci√≥n de clase                 | ‚úÖ             | Aplica `normalize_label` a la clase de entrada.                           |\n",
    "\n",
    "¬øQu√© se podr√≠a mejorar?\n",
    "\n",
    "1. **Orden temporal expl√≠cito**:\n",
    "   Actualmente **no se aplica `sort_values(\"tiempo\")`** sobre cada curva antes de procesarla. Aunque muchas curvas ya vienen ordenadas, ser√≠a m√°s robusto a√±adir:\n",
    "\n",
    "   ```python\n",
    "   df = df.sort_values(by=find_column(df.columns, \"tiempo\"))\n",
    "   ```\n",
    "\n",
    "   ...como primer paso dentro de `process_single_curve()`.\n",
    "\n",
    "2. **Soporte para features adicionales**:\n",
    "   Ahora solo se usa `mag` (magnitud), pero si en el futuro deseas usar tambi√©n `flux`, `mag_err`, etc., habr√≠a que adaptar esta funci√≥n o a√±adir variantes.\n",
    "\n",
    "Como est√° ahora, los **√∫nicos datos de entrada al modelo** son:\n",
    "\n",
    "* `magnitudes_norm`: vector de `seq_length` elementos (curva preprocesada)\n",
    "* `attention_mask`: vector binario que indica datos v√°lidos\n",
    "* `clase`: etiqueta de clase codificada (para entrenamiento o evaluaci√≥n)\n",
    "\n",
    "Esto es **lo correcto** para un modelo tipo Transformer que espera curvas de magnitud como secuencia 1D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0413610b-89ee-451d-bf88-819cfcbaf07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset completo en memoria (requiere ~40-60GB)...\n",
      "Guardando dataset consolidado directamente en S3 (sin pasar por disco local)...\n",
      "‚úÖ ¬°Dataset optimizado guardado correctamente en S3!\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "from src.utils.dataset_paths import DATASET_PATHS_AWS\n",
    "\n",
    "s3_bucket = \"sagemaker-eu-west-3-478344394470\"\n",
    "s3_path = \"datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "print(\"Cargando dataset completo en memoria (requiere ~40-60GB)...\")\n",
    "dataset = ds.dataset(DATASET_PATHS_AWS, format=\"parquet\")\n",
    "tabla = dataset.to_table()\n",
    "\n",
    "print(\"Guardando dataset consolidado directamente en S3 (sin pasar por disco local)...\")\n",
    "s3_filesystem = fs.S3FileSystem()\n",
    "with s3_filesystem.open_output_stream(f\"{s3_bucket}/{s3_path}\") as s3_file:\n",
    "    pq.write_table(tabla, s3_file, row_group_size=10_000_000)\n",
    "\n",
    "print(\"‚úÖ ¬°Dataset optimizado guardado correctamente en S3!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bb6288-931b-4b94-bfb9-b03502ae240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo le√≠do correctamente\n",
      "üßÆ N√∫mero de filas: 626,189,090\n",
      "üìä N√∫mero de columnas: 13\n",
      "üßæ Columnas: ['id', 'time', 'mag', 'mag_err', 'flux', 'flux_err', 'clase_variable', 'clase_variable_normalizada', 'mission', 'mission_id', 'source_dataset', 'label_source', 'band']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Ruta local del parquet consolidado\n",
    "parquet_path = \"/mnt/data/datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "# Leer solo la metadata (no carga datos)\n",
    "metadata = pq.read_metadata(parquet_path)\n",
    "\n",
    "print(\"‚úÖ Archivo le√≠do correctamente\")\n",
    "print(f\"üßÆ N√∫mero de filas: {metadata.num_rows:,}\")\n",
    "print(f\"üìä N√∫mero de columnas: {metadata.num_columns}\")\n",
    "print(f\"üßæ Columnas: {[metadata.schema.column(i).name for i in range(metadata.num_columns)]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c416ee-fb15-4e5c-b7a3-d204b1e24dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             id           time     mag  mag_err     flux  \\\n",
      "0  ASASSN-V J000000.19+320847.2            HJD     mag  mag_err     flux   \n",
      "1  ASASSN-V J000000.19+320847.2  2458017.73473  15.468  0.05599  2.35900   \n",
      "2  ASASSN-V J000000.19+320847.2  2458018.75446   15.39  0.05292  2.53600   \n",
      "3  ASASSN-V J000000.19+320847.2  2458034.87939  15.276  0.04876  2.81700   \n",
      "4  ASASSN-V J000000.19+320847.2  2458035.92739  15.405  0.05350  2.50100   \n",
      "\n",
      "   flux_err clase_variable clase_variable_normalizada mission    mission_id  \\\n",
      "0  flux_err             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "1   0.12152             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "2   0.12347             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "3   0.12638             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "4   0.12309             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "\n",
      "  source_dataset    label_source band  \n",
      "0   asassn_gband  ASASSN_Catalog    g  \n",
      "1   asassn_gband  ASASSN_Catalog    g  \n",
      "2   asassn_gband  ASASSN_Catalog    g  \n",
      "3   asassn_gband  ASASSN_Catalog    g  \n",
      "4   asassn_gband  ASASSN_Catalog    g  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al parquet\n",
    "parquet_path = \"/mnt/data/datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "# Abrir el archivo como ParquetFile\n",
    "pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "# Leer las primeras N filas del primer row group (sin cargar todo)\n",
    "sample_table = pf.read_row_group(0)\n",
    "sample_df = sample_table.to_pandas()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(sample_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0413610b-89ee-451d-bf88-819cfcbaf07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset completo en memoria (requiere ~40-60GB)...\n",
      "Guardando dataset consolidado directamente en S3 (sin pasar por disco local)...\n",
      "‚úÖ ¬°Dataset optimizado guardado correctamente en S3!\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "from src.utils.dataset_paths import DATASET_PATHS_AWS\n",
    "\n",
    "s3_bucket = \"sagemaker-eu-west-3-478344394470\"\n",
    "s3_path = \"datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "print(\"Cargando dataset completo en memoria (requiere ~40-60GB)...\")\n",
    "dataset = ds.dataset(DATASET_PATHS_AWS, format=\"parquet\")\n",
    "tabla = dataset.to_table()\n",
    "\n",
    "print(\"Guardando dataset consolidado directamente en S3 (sin pasar por disco local)...\")\n",
    "s3_filesystem = fs.S3FileSystem()\n",
    "with s3_filesystem.open_output_stream(f\"{s3_bucket}/{s3_path}\") as s3_file:\n",
    "    pq.write_table(tabla, s3_file, row_group_size=10_000_000)\n",
    "\n",
    "print(\"‚úÖ ¬°Dataset optimizado guardado correctamente en S3!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bb6288-931b-4b94-bfb9-b03502ae240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo le√≠do correctamente\n",
      "üßÆ N√∫mero de filas: 626,189,090\n",
      "üìä N√∫mero de columnas: 13\n",
      "üßæ Columnas: ['id', 'time', 'mag', 'mag_err', 'flux', 'flux_err', 'clase_variable', 'clase_variable_normalizada', 'mission', 'mission_id', 'source_dataset', 'label_source', 'band']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Ruta local del parquet consolidado\n",
    "parquet_path = \"/mnt/data/datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "# Leer solo la metadata (no carga datos)\n",
    "metadata = pq.read_metadata(parquet_path)\n",
    "\n",
    "print(\"‚úÖ Archivo le√≠do correctamente\")\n",
    "print(f\"üßÆ N√∫mero de filas: {metadata.num_rows:,}\")\n",
    "print(f\"üìä N√∫mero de columnas: {metadata.num_columns}\")\n",
    "print(f\"üßæ Columnas: {[metadata.schema.column(i).name for i in range(metadata.num_columns)]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c416ee-fb15-4e5c-b7a3-d204b1e24dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             id           time     mag  mag_err     flux  \\\n",
      "0  ASASSN-V J000000.19+320847.2            HJD     mag  mag_err     flux   \n",
      "1  ASASSN-V J000000.19+320847.2  2458017.73473  15.468  0.05599  2.35900   \n",
      "2  ASASSN-V J000000.19+320847.2  2458018.75446   15.39  0.05292  2.53600   \n",
      "3  ASASSN-V J000000.19+320847.2  2458034.87939  15.276  0.04876  2.81700   \n",
      "4  ASASSN-V J000000.19+320847.2  2458035.92739  15.405  0.05350  2.50100   \n",
      "\n",
      "   flux_err clase_variable clase_variable_normalizada mission    mission_id  \\\n",
      "0  flux_err             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "1   0.12152             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "2   0.12347             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "3   0.12638             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "4   0.12309             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "\n",
      "  source_dataset    label_source band  \n",
      "0   asassn_gband  ASASSN_Catalog    g  \n",
      "1   asassn_gband  ASASSN_Catalog    g  \n",
      "2   asassn_gband  ASASSN_Catalog    g  \n",
      "3   asassn_gband  ASASSN_Catalog    g  \n",
      "4   asassn_gband  ASASSN_Catalog    g  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al parquet\n",
    "parquet_path = \"/mnt/data/datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "# Abrir el archivo como ParquetFile\n",
    "pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "# Leer las primeras N filas del primer row group (sin cargar todo)\n",
    "sample_table = pf.read_row_group(0)\n",
    "sample_df = sample_table.to_pandas()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(sample_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando datos en lotes con PyArrow...\n",
      "üíæ [INFO] Cargando agrupaci√≥n de curvas desde cache: data/train/grouped_data.pkl\n",
      "‚úÖ [INFO] Agrupaci√≥n cargada desde cache. Total objetos: 88765\n",
      "‚è≥ [INFO] Tiempo en agrupaci√≥n de datos: 17.3 segundos\n",
      "üöÄ Procesando 88765 curvas en paralelo usando 72 CPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n",
      "/home/ec2-user/SageMaker/astro_transformer/src/fase2/script_1_transformer_preprocessing_optimizado_2.py:164: RuntimeWarning: overflow encountered in divide\n",
      "  magnitudes_norm = (magnitudes - median) / iqr if iqr != 0 else magnitudes - median\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ [INFO] Tiempo en procesamiento paralelo: 89.8 segundos\n",
      "üîã [INFO] Curvas v√°lidas tras filtrado: 85016\n",
      "[INFO] Uso de memoria de sequences: 4053.88 MB\n",
      "[INFO] Uso de memoria de masks: 4053.88 MB\n",
      "[INFO] Uso de memoria de labels: 0.32 MB\n",
      "üíæ [INFO] Guardando label_encoder.pkl...\n",
      "üìä Recuento por clase codificada:\n",
      " 3 (Irregular): 9000\n",
      " 4 (RR Lyrae): 37569\n",
      " 2 (Eclipsing Binary): 9000\n",
      " 5 (Rotational): 9000\n",
      " 0 (Cataclysmic): 2080\n",
      " 8 (Young Stellar Object): 9799\n",
      " 1 (Delta Scuti): 7450\n",
      " 6 (Variable): 1000\n",
      " 7 (White Dwarf): 118\n",
      "[INFO] N curvas: 85016, seq_length: 25000\n",
      "[INFO] Estimaci√≥n memoria sequences (float16): 3.96 GB\n",
      "[INFO] Estimaci√≥n memoria sequences (float32): 7.92 GB\n",
      "[INFO] Si tienes problemas de memoria, considera usar almacenamiento en disco y Dataset bajo demanda.\n",
      "üìù [INFO] Realizando split train/val...\n",
      "üíæ [INFO] Guardando datasets serializados...\n",
      "\n",
      "üìâ Resumen de curvas descartadas:\n",
      "üî∏ All nan                       : 0\n",
      "üî∏ Low std                       : 0\n",
      "üî∏ Short curve                   : 0\n",
      "üî∏ Nan or inf after norm         : 0\n",
      "üî∏ Unknown class                 : 0\n",
      "üî∏ Ok                            : 0\n",
      "‚úÖ Datos preparados como secuencias normalizadas y m√°scaras.\n",
      "‚è≥ [INFO] Tiempo total de ejecuci√≥n: 3.90 minutos (234.0 segundos)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fdfbdcbee00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fdfbdcbfe80>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fdfbdcbee00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fdfbdcbfe80>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Ignorar solo los RuntimeWarning de numpy (como overflows en reduce)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy\")\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Ignorar solo los RuntimeWarning de numpy (como overflows en reduce)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy\")\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing_optimizado_2 import main\n",
    "\n",
    "max_per_class_override={\n",
    "    \"Irregular\": 9000,\n",
    "    \"Rotational\": 9000,\n",
    "    \"Eclipsing Binary\": 9000,\n",
    "    \"Delta Scuti\": None,               # 7.550 ‚Üí TODAS\n",
    "    \"RR Lyrae\": 9000,                    # 41.208 ‚Üí TODAS NO\n",
    "    \"Young Stellar Object\": None,      # 9.809 ‚Üí TODAS\n",
    "    \"Cataclysmic\": None,               # 2.080 ‚Üí TODAS\n",
    "    \"White Dwarf\": None,               # 118 ‚Üí TODAS\n",
    "    \"Variable\": 1000                   # limitada por ser gen√©rica\n",
    "}\n",
    "\n",
    "main(\n",
    "    seq_length=25000,\n",
    "    max_per_class=None, # usamos override completo\n",
    "    max_per_class_override=max_per_class_override,\n",
    "    parquet_batch_size=10_000_000,\n",
    "    dataloader_batch_size=128,\n",
    "    num_workers=72,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407f219-bebd-4c84-bb68-03d0981f81cb",
   "metadata": {},
   "source": [
    "Todo ha funcionado correctamente y con m√©tricas excelentes:\n",
    "\n",
    "#### ‚úÖ **Preprocesado finalizado con √©xito**\n",
    "\n",
    "* üß™ **Curvas v√°lidas**: `85.016` (tras filtrado y eliminaci√≥n de clase `Unknown`)\n",
    "* üíæ **Uso de memoria estimado**:\n",
    "\n",
    "  * `sequences` (float16): **3.96 GB**\n",
    "  * `masks` (float16): **3.96 GB**\n",
    "  * `labels` (int32): **0.32 MB**\n",
    "* üí° `float16` ha reducido significativamente el uso de RAM (hubiera sido casi 8‚ÄØGB con float32).\n",
    "* üìä **Distribuci√≥n final por clase**:\n",
    "\n",
    "| Clase codificada | Clase                | N¬∫ curvas |\n",
    "| ---------------- | -------------------- | --------- |\n",
    "| 0                | Cataclysmic          | 2.080     |\n",
    "| 1                | Delta Scuti          | 7.450     |\n",
    "| 2                | Eclipsing Binary     | 9.000     |\n",
    "| 3                | Irregular            | 9.000     |\n",
    "| 4                | RR Lyrae             | 37.569    |\n",
    "| 5                | Rotational           | 9.000     |\n",
    "| 6                | Variable             | 1.000     |\n",
    "| 7                | White Dwarf          | 118       |\n",
    "| 8                | Young Stellar Object | 9.799     |\n",
    "\n",
    "**Curvas descartadas** (todos los motivos controlados):\n",
    "\n",
    "```plaintext\n",
    "üî∏ All nan                       : 0\n",
    "üî∏ Low std                       : 0\n",
    "üî∏ Short curve                   : 0\n",
    "üî∏ Nan or inf after norm         : 0\n",
    "üî∏ Unknown class                 : 0\n",
    "üî∏ Ok                            : 0\n",
    "```\n",
    "\n",
    "Esto √∫ltimo indica que **todas las curvas procesadas fueron v√°lidas**, y que el filtrado previo fue preciso y efectivo.\n",
    "\n",
    "#### üì¶ **Salidas generadas** (en data/train)\n",
    "\n",
    "* `label_encoder.pkl`\n",
    "* `train_dataset.pt`\n",
    "* `val_dataset.pt`\n",
    "* `debug_clases_codificadas.csv`\n",
    "* `debug_descartes.csv`\n",
    "\n",
    "üïì **Tiempo total de ejecuci√≥n**: `3.9 minutos` (en una m√°quina de alto rendimiento, 72 CPUs, 140‚ÄØGB RAM).\n",
    "\n",
    "Este resultado es **√≥ptimo**, y confirma que la configuraci√≥n actual del pipeline es estable, eficiente y lista para el entrenamiento final del modelo Transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd710f27-d4f6-49c8-8b89-2e9d7901a91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
