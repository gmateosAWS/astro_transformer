{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f6f4e81f",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "_Máster Universitario en Inteligencia Artificial_\n",
    "\n",
    "_Trabajo Fin de Máster_\n",
    "\n",
    "- Gustavo Mateos Santos\n",
    "- gustavo.mateos830@comunidadunir.net\n",
    "\n",
    "# Clasificación automática de estrellas variables con modelos Transformer aplicados a series temporales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62128cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting lightkurve\n",
      "  Downloading lightkurve-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Collecting astropy>=5.0 (from lightkurve)\n",
      "  Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting astroquery>=0.3.10 (from lightkurve)\n",
      "  Downloading astroquery-0.4.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.13.4)\n",
      "Requirement already satisfied: bokeh>=2.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.7.2)\n",
      "Collecting fbpca>=1.0 (from lightkurve)\n",
      "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.10.1)\n",
      "Collecting memoization>=0.3.1 (from lightkurve)\n",
      "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.26.4)\n",
      "Collecting oktopus>=0.1.2 (from lightkurve)\n",
      "  Downloading oktopus-0.1.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.32.3)\n",
      "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.67.1)\n",
      "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
      "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.23 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.4.0)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy>=5.0->lightkurve)\n",
      "  Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting astropy-iers-data>=0.2024.10.28.0.34.7 (from astropy>=5.0->lightkurve)\n",
      "  Downloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (6.0.2)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (24.2)\n",
      "Collecting html5lib>=0.999 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting keyring>=15.0 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading pyvo-1.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.5)\n",
      "Requirement already satisfied: contourpy>=1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.3.2)\n",
      "Requirement already satisfied: narwhals>=1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.35.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (11.2.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (2025.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
      "Collecting autograd (from oktopus>=0.1.2->lightkurve)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (2025.1.31)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.0.1)\n",
      "Collecting multidict<7.0.0,>=6.0.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
      "Collecting SecretStorage>=3.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading SecretStorage-3.3.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jeepney>=0.4.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jeepney-0.9.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: importlib_metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.11.0)\n",
      "Collecting jaraco.classes (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib_metadata>=4.11.4->keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.21.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (44.0.2)\n",
      "Collecting more-itertools (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.22)\n",
      "Downloading lightkurve-2.5.0-py3-none-any.whl (270 kB)\n",
      "Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astroquery-0.4.10-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
      "Downloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
      "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.7/738.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyvo-1.6.2-py3-none-any.whl (999 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.4/999.4 kB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading jeepney-0.9.0-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Building wheels for collected packages: fbpca, memoization, oktopus\n",
      "  Building wheel for fbpca (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11428 sha256=7a1c0c198fcd5b6d325211ca045f7e490a1f57d14f4fd405ac819d3c8488574e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3c/ea/60/8d1c9fbbc99492a1775b36a5e29c8c1ef309cc5821bd5a219d\n",
      "  Building wheel for memoization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50537 sha256=db994a6d6c135ea1fa23778fd6df8bb1ae62f672728f823114592e7ad00d4162\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3e/b8/c5/b553d5e8b0249bd2859b3b6d7bb2a1849e7b01c6e8b64f6e87\n",
      "  Building wheel for oktopus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for oktopus: filename=oktopus-0.1.2-py3-none-any.whl size=12817 sha256=4397a316aadeda57340417b4534cf34779c5dd2f8c9e0519ca0c4b36debe9cbb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b6/55/53/8d7c151deab7a11f182e5a69feb906cab95123da7a2438ed9d\n",
      "Successfully built fbpca memoization oktopus\n",
      "Installing collected packages: fbpca, wrapt, uncertainties, pyerfa, propcache, multidict, more-itertools, memoization, jeepney, html5lib, frozenlist, backports.tarfile, autograd, async-timeout, astropy-iers-data, aioitertools, aiohappyeyeballs, yarl, oktopus, jaraco.functools, jaraco.context, jaraco.classes, botocore, astropy, aiosignal, SecretStorage, pyvo, aiohttp, keyring, aiobotocore, s3fs, astroquery, lightkurve\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.38.1\n",
      "    Uninstalling botocore-1.38.1:\n",
      "      Successfully uninstalled botocore-1.38.1\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 0.4.2\n",
      "    Uninstalling s3fs-0.4.2:\n",
      "      Successfully uninstalled s3fs-0.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.40.0 requires botocore==1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "boto3 1.38.1 requires botocore<1.39.0,>=1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "s3transfer 0.12.0 requires botocore<2.0a.0,>=1.37.4, but you have botocore 1.37.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SecretStorage-3.3.3 aiobotocore-2.22.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aioitertools-0.12.0 aiosignal-1.3.2 astropy-6.1.7 astropy-iers-data-0.2025.5.12.0.38.29 astroquery-0.4.10 async-timeout-5.0.1 autograd-1.8.0 backports.tarfile-1.2.0 botocore-1.37.3 fbpca-1.0 frozenlist-1.6.0 html5lib-1.1 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jeepney-0.9.0 keyring-25.6.0 lightkurve-2.5.0 memoization-0.4.0 more-itertools-10.7.0 multidict-6.4.3 oktopus-0.1.2 propcache-0.3.1 pyerfa-2.0.1.5 pyvo-1.6.2 s3fs-2025.3.2 uncertainties-3.2.3 wrapt-1.17.2 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages\n",
    "%pip install torch lightkurve\n",
    "%pip install -q pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455e0895-292c-4535-9e86-1e21e24e186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.2\n",
      "Lightkurve: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch, lightkurve as lk\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Lightkurve:\", lk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb5af2",
   "metadata": {},
   "source": [
    "## **Fase 2: Diseño e Implementación del Modelo Transformer**\n",
    "\n",
    "Tras haber finalizado la **Fase 1 (Recopilación y preparación de datos)**, donde hemos generado un conjunto consolidado y etiquetado a partir de diversas misiones espaciales (Kepler, K2, TESS), el siguiente paso definido claramente en la memoria es la implementación de la **Fase 2**:\n",
    "\n",
    "Aquí se menciona específicamente:\n",
    "\n",
    "* **Arquitectura Transformer especializada**, adaptada a series temporales astronómicas.\n",
    "* Implementación técnica con PyTorch en entorno local (VSCode).\n",
    "* Exploración e incorporación de técnicas avanzadas:\n",
    "  * Codificación posicional rotatoria (*Rotary Positional Encoding*).\n",
    "  * Mecanismos de atención jerárquica (*Multi-head Self-Attention*).\n",
    "  * Técnicas de *transfer learning* usando modelos preentrenados como **ASTROMER** para mejorar la generalización en casos de datos limitados o escasos.\n",
    "\n",
    "### ⚙️ Pasos concretos ajustados al estado actual del trabajo:\n",
    "\n",
    "1. **Preparación final del dataset consolidado** (*ya realizada con éxito*):\n",
    "   * `all_missions_labeled.parquet` generado, validado y listo.\n",
    "\n",
    "2. **Preprocesamiento para Modelo Transformer** (*inmediato*):\n",
    "   * Normalización individual por curva (por ejemplo, estándar Z-Score o min-max).\n",
    "   * Segmentación o padding de las curvas para homogeneizar longitud.\n",
    "   * Opcional: generación de embeddings iniciales (si decides usar ASTROMER).\n",
    "\n",
    "3. **Diseño Arquitectónico Inicial del Modelo**:\n",
    "   * Entrada: secuencia temporal de magnitudes (+ potencialmente metadatos adicionales).\n",
    "   * Embedding lineal inicial.\n",
    "   * Codificación posicional rotatoria.\n",
    "   * Bloques de atención jerárquica multi-cabezal.\n",
    "   * Capas finales para clasificación multiclase usando salida \"softmax\".\n",
    "\n",
    "4. **Implementación técnica**:\n",
    "   * Desarrollo del modelo en PyTorch (en tu entorno local VSCode ya configurado).\n",
    "   * Uso de GPU en AWS SageMaker para entrenamientos largos o con muchos datos.\n",
    "\n",
    "5. **Entrenamiento y validación inicial del modelo**:\n",
    "   * División del dataset en entrenamiento, validación y prueba.\n",
    "   * Evaluación sistemática usando métricas definidas (Accuracy, Recall, F1-score).\n",
    "\n",
    "El paso más lógico y urgente, dado el avance actual, es comenzar inmediatamente la implementación técnica del modelo Transformer descrito anteriormente. De hecho, sería idóneo empezar con un script base (`script_8_train_transformer.py`) en PyTorch que:\n",
    "\n",
    "* Cargue datos desde el fichero final consolidado (`all_missions_labeled.parquet`).\n",
    "* Realice la normalización y segmentación (preprocesamiento).\n",
    "* Construya la arquitectura Transformer básica propuesta.\n",
    "* Ejecute entrenamiento inicial y validación para comprobar su funcionamiento.\n",
    "\n",
    "#### 🚨 Consideración importante sobre transfer learning:\n",
    "\n",
    "En la memoria se destaca claramente la relevancia del uso de modelos preentrenados, particularmente **ASTROMER**, para aprovechar embeddings preaprendidos y mejorar la generalización del modelo. Se recomendaría evaluar claramente incorporar este paso desde ahora, ya que podría acelerar la convergencia del entrenamiento y mejorar resultados iniciales.\n",
    "\n",
    "#### 📋 Plan posterior (según tu TFM):\n",
    "\n",
    "Tras la implementación y validación básica del Transformer, seguirían:\n",
    "\n",
    "* **Fase 3**: Evaluación experimental comparativa contra CNN y LSTM.\n",
    "* **Fase 4**: Evaluación de robustez ante incertidumbres (curvas incompletas, ruido, muestreo irregular).\n",
    "* **Fase 5**: Análisis crítico, síntesis y propuestas futuras de mejora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c69b60",
   "metadata": {},
   "source": [
    "\n",
    "#### 📚 **1. Citación del uso de ASTROMER en la Memoria (Capítulo 6)**\n",
    "\n",
    "Correcto. El uso de ASTROMER debe ser claramente citado en la memoria del TFM. En el capítulo 6, donde describas el diseño e implementación del modelo, deberías incluir explícitamente algo similar a:\n",
    "\n",
    "> Para aprovechar las ventajas del aprendizaje por transferencia en series temporales astronómicas, se ha utilizado el modelo preentrenado **ASTROMER** desarrollado por Fang et al. (2022), disponible públicamente en el repositorio de GitHub ([https://github.com/IShengFang/ASTROMER](https://github.com/IShengFang/ASTROMER)).\n",
    "\n",
    "Luego, en la bibliografía añadirás la referencia completa, por ejemplo en formato APA:\n",
    "\n",
    "> Fang, I. S., Teyssier, D., & Longo, G. (2022). **ASTROMER**: A Transformer-based Embedding for Astronomical Time Series. Recuperado de [https://github.com/IShengFang/ASTROMER](https://github.com/IShengFang/ASTROMER)\n",
    "\n",
    "Esto garantiza la transparencia y buenas prácticas en tu trabajo académico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428c9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     26437.000000\n",
      "mean       9514.005523\n",
      "std        7929.330408\n",
      "min           3.000000\n",
      "50%        8242.000000\n",
      "75%       13771.000000\n",
      "90%       17275.000000\n",
      "95%       19273.400000\n",
      "99%       36005.720000\n",
      "max      131072.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "\n",
    "dataset = ds.dataset(\"data/processed/all_missions_labeled.parquet\")\n",
    "scanner = dataset.scanner(columns=[\"id_objeto\"])\n",
    "\n",
    "lengths = []\n",
    "for batch in scanner.to_batches():\n",
    "    df_batch = batch.to_pandas()\n",
    "    lengths.append(df_batch.groupby('id_objeto').size())\n",
    "\n",
    "all_lengths = pd.concat(lengths)\n",
    "print(all_lengths.describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cf0ba",
   "metadata": {},
   "source": [
    "\n",
    "📌 **Adaptaciones necesarias para la memoria del TFM (capítulo 6):**\n",
    "\n",
    "En la próxima redacción de tu capítulo 6 debes mencionar explícitamente:\n",
    "\n",
    "-   **Cambio de ASTROMER a AstroConformer** (motivado por disponibilidad).\n",
    "-   **Justificación técnica** del uso de máscaras de atención (gestión eficiente de curvas de longitud variable).\n",
    "-   **Detalles técnicos de implementación** de AstroConformer y la gestión eficiente de memoria usando PyArrow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c33b",
   "metadata": {},
   "source": [
    "#### 🎓 Objetivo del TFM:\n",
    "\n",
    "**Entrenar tu propio Transformer sobre curvas de luz completas**, usando **transfer learning con AstroConformer**.\n",
    "\n",
    "#### ¿Qué significa entonces aplicar *transfer learning* desde AstroConformer?\n",
    "\n",
    "1. **Usar AstroConformer preentrenado como bloque de codificación de secuencias**:\n",
    "\n",
    "   * No lo usas solo como extractor de un vector resumen por curva.\n",
    "   * Lo usas **para procesar secuencias completas**: pasa la curva entera, y extrae **embeddings intermedios** (por ejemplo, las salidas de cada paso de tiempo).\n",
    "\n",
    "2. **Congelar o afinar (fine-tune) sus pesos** durante el entrenamiento de tu modelo.\n",
    "\n",
    "   * Si congelas: aprovechas lo aprendido sin tocarlo.\n",
    "   * Si haces fine-tuning: permites que se adapte a tu nuevo conjunto de clases.\n",
    "\n",
    "3. **Añadir una cabeza de clasificación propia** sobre el output del encoder de AstroConformer.\n",
    "\n",
    "   * Es decir, el modelo final sería algo como:\n",
    "\n",
    "     ```\n",
    "     curva de luz --> AstroConformer --> [CLS] embedding o secuencia completa --> capa lineal --> clases\n",
    "     ```\n",
    "\n",
    "La **opción correcta** en lugar de usar *nuestro propio Transformer desde cero*, el **bloque encoder será AstroConformer**, cargado con pesos preentrenados.\n",
    "\n",
    "En `script_2_transformer_training.py`:\n",
    "\n",
    "* Cargas AstroConformer como `encoder`.\n",
    "* Le pasas `x` y `mask` como entrada.\n",
    "* Añades tu cabeza de clasificación (`nn.Linear(...)`).\n",
    "* Entrenas usando `train_loader` y `val_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a289da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n",
      "🔍 Limitando procesamiento a los primeros 200 objetos\n",
      "🚀 Procesando 200 curvas en paralelo usando 8 CPUs...\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing import main as transformer_preprocessing\n",
    "\n",
    "# Ejecutar una prueba limitada a 200 objetos\n",
    "train_loader, val_loader = transformer_preprocessing(\n",
    "    seq_length=20000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    limit_objects=200,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8c7e4",
   "metadata": {},
   "source": [
    "**Validación del primer batch de entrenamiento**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d3527d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extraer el primer batch del loader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[32m      3\u001b[39m     x, y, mask = batch\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔍 Batch de entrenamiento:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Extraer el primer batch del loader\n",
    "for batch in train_loader:\n",
    "    x, y, mask = batch\n",
    "    print(\"🔍 Batch de entrenamiento:\")\n",
    "    print(f\"- x.shape       : {x.shape}\")\n",
    "    print(f\"- y.shape       : {y.shape}\")\n",
    "    print(f\"- mask.shape    : {mask.shape}\")\n",
    "    print(f\"- x[0, :10]     : {x[0, :10]}\")  # primeros 10 pasos de la curva 0\n",
    "    print(f\"- mask[0, :20]  : {mask[0, :20]}\")  # primeros 20 elementos de la máscara\n",
    "    print(f\"- y[0] (clase)  : {y[0]}\")\n",
    "    break  # solo inspeccionar el primer batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeaeba",
   "metadata": {},
   "source": [
    "| Variable    | Forma esperada             | Resultado     | ✅ Estado |\n",
    "| ----------- | -------------------------- | ------------- | -------- |\n",
    "| `x`         | `(batch_size, seq_length)` | `(64, 20000)` | ✅ Ok     |\n",
    "| `mask`      | `(batch_size, seq_length)` | `(64, 20000)` | ✅ Ok     |\n",
    "| `y`         | `(batch_size,)`            | `(64,)`       | ✅ Ok     |\n",
    "| `x[0][:10]` | valores normalizados       | valores ∈ ℝ   | ✅ Ok     |\n",
    "| `mask[0]`   | 1s en datos reales         | todo `1.0`    | ✅ Ok     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ffdec",
   "metadata": {},
   "source": [
    "**Ejecución para todos los objetos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bba571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Objetos únicos detectados: 13415\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\"data/processed/all_missions_labeled.parquet\", format=\"parquet\")\n",
    "scanner = dataset.scanner(columns=[\"id_objeto\"])\n",
    "unique_ids = set()\n",
    "\n",
    "for batch in scanner.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    unique_ids.update(df[\"id_objeto\"].dropna().unique())\n",
    "\n",
    "print(f\"🔎 Objetos únicos detectados: {len(unique_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524f3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n",
      "🚀 Procesando 13415 curvas en paralelo usando 8 CPUs...\n",
      "📊 Recuento por clase codificada:\n",
      " 2 (Eclipsing Binary): 7033\n",
      " 4 (Other): 4970\n",
      " 6 (Rotational): 1139\n",
      " 3 (Irregular): 48\n",
      " 1 (Delta Scuti): 98\n",
      " 9 (Young Stellar Object): 11\n",
      " 7 (Variable): 43\n",
      " 5 (RR Lyrae): 19\n",
      " 0 (Cataclysmic): 3\n",
      " 8 (White Dwarf): 4\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing import main as transformer_preprocessing\n",
    "\n",
    "# Ejecutar una prueba con todos los objetos\n",
    "train_loader, val_loader = transformer_preprocessing(\n",
    "    seq_length=20000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    device=\"cpu\",\n",
    "    limit_objects=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardado de los datasets de la celda anterior, serializados para poder recuperarlos si se reinicia el Kernel**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2452f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Guardar datasets serializados para no perderlos al reiniciar el kernel\n",
    "torch.save(train_loader.dataset, \"data/train/train_dataset.pt\")\n",
    "torch.save(val_loader.dataset, \"data/train/val_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0169d",
   "metadata": {},
   "source": [
    "🧠 `script_2_transformer_training.py`:\n",
    "\n",
    "#### ✅ ¿Qué hace este script?\n",
    "\n",
    "1. Carga **AstroConformer** como encoder con la configuración oficial (`default_config.yaml`).\n",
    "2. Añade una **capa lineal de clasificación**.\n",
    "3. Utiliza los **DataLoaders preparados** (`train_loader`, `val_loader`).\n",
    "4. Soporta entrenamiento en **CPU o GPU (`device`)**.\n",
    "5. Muestra **métricas detalladas** por clase en validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066e88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Clases codificadas presentes en train_loader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "🔢 Número de clases distintas detectadas: 10\n",
      "🎯 Clases codificadas presentes en val_loader: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "🔢 Número de clases distintas detectadas: 9\n"
     ]
    }
   ],
   "source": [
    "# Mostrar clases codificadas presentes en el dataset\n",
    "clase_ids_unicos = sorted(set([y.item() for _, y, _ in train_loader.dataset]))\n",
    "print(f\"🎯 Clases codificadas presentes en train_loader: {clase_ids_unicos}\")\n",
    "print(f\"🔢 Número de clases distintas detectadas: {len(clase_ids_unicos)}\")\n",
    "\n",
    "clase_ids_unicos = sorted(set([y.item() for _, y, _ in val_loader.dataset]))\n",
    "print(f\"🎯 Clases codificadas presentes en val_loader: {clase_ids_unicos}\")\n",
    "print(f\"🔢 Número de clases distintas detectadas: {len(clase_ids_unicos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a62cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comprobando valores infinitos en el dataset de entrenamiento...\n",
      "🔍 Comprobando valores infinitos en el dataset de validación...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = torch.load(\"data/train/train_dataset.pt\", weights_only=False)\n",
    "val_dataset = torch.load(\"data/train/val_dataset.pt\", weights_only=False)\n",
    "\n",
    "# Comprobar si hay valores infinitos en el dataset de entrenamiento\n",
    "# y mostrar estadísticas de los tensores\n",
    "print(\"🔍 Comprobando valores infinitos en el dataset de entrenamiento...\")\n",
    "for i in range(len(train_dataset)):\n",
    "    x, y, mask = train_dataset[i]\n",
    "    if torch.isinf(x).any():\n",
    "        print(f\"⚠️ Inf detectado en muestra {i} (clase {y})\")\n",
    "        print(f\"x.mean: {x.mean()}, std: {x.std()}, max: {x.max()}, min: {x.min()}\")\n",
    "\n",
    "# Comprobar si hay valores infinitos en el dataset de validación\n",
    "print(\"🔍 Comprobando valores infinitos en el dataset de validación...\")\n",
    "for i in range(len(val_dataset)):\n",
    "    x, y, mask = val_dataset[i]\n",
    "    if torch.isinf(x).any():\n",
    "        print(f\"⚠️ Inf detectado en muestra {i} (clase {y})\")\n",
    "        print(f\"x.mean: {x.mean()}, std: {x.std()}, max: {x.max()}, min: {x.min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add5ca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Restaurando datasets...\n",
      "🔄 Cargando datasets completos...\n",
      "🚀 Entrenando modelo...\n",
      "Label encoder (clase → índice): {'Cataclysmic': 0, 'Delta Scuti': 1, 'Eclipsing Binary': 2, 'Irregular': 3, 'Other': 4, 'RR Lyrae': 5, 'Rotational': 6, 'Variable': 7, 'White Dwarf': 8, 'Young Stellar Object': 9}\n",
      "Número de clases: 10\n",
      "Valores únicos en etiquetas: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Máximo índice: 9\n",
      "📊 x.mean: -0.1136, std: 21.4503, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7010\n",
      "📊 x.mean: 0.0601, std: 12.0855, max: 1000.0000, min: -280.8261\n",
      "📈 out.mean: 0.0000, std: 0.6812\n",
      "📊 x.mean: -0.1513, std: 32.2348, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6942\n",
      "📊 x.mean: -0.2297, std: 32.5002, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6798\n",
      "📊 x.mean: 0.1087, std: 46.4718, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6876\n",
      "📊 x.mean: -0.2644, std: 8.1806, max: 1000.0000, min: -415.9753\n",
      "📈 out.mean: 0.0000, std: 0.6973\n",
      "📊 x.mean: -0.0452, std: 50.6088, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7092\n",
      "📊 x.mean: -0.1061, std: 31.7154, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6985\n",
      "📊 x.mean: -0.2046, std: 56.8181, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7419\n",
      "📊 x.mean: -0.1472, std: 10.4717, max: 1000.0000, min: -280.9021\n",
      "📈 out.mean: -0.0000, std: 0.6426\n",
      "📊 x.mean: 0.2763, std: 50.7907, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7199\n",
      "📊 x.mean: -0.1230, std: 58.2973, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6872\n",
      "📊 x.mean: -0.1582, std: 6.3296, max: 1000.0000, min: -361.7283\n",
      "📈 out.mean: 0.0000, std: 0.6874\n",
      "📊 x.mean: -0.1795, std: 8.3842, max: 1000.0000, min: -261.9434\n",
      "📈 out.mean: -0.0000, std: 0.6387\n",
      "📊 x.mean: -0.2260, std: 7.6037, max: 1000.0000, min: -641.1680\n",
      "📈 out.mean: -0.0000, std: 0.6475\n",
      "📊 x.mean: -0.3288, std: 11.9838, max: 1000.0000, min: -456.7870\n",
      "📈 out.mean: 0.0000, std: 0.6765\n",
      "📊 x.mean: -0.2992, std: 8.8697, max: 1000.0000, min: -455.7710\n",
      "📈 out.mean: 0.0000, std: 0.6695\n",
      "📊 x.mean: -0.0875, std: 32.3426, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6863\n",
      "📊 x.mean: -0.1673, std: 6.5477, max: 1000.0000, min: -382.9652\n",
      "📈 out.mean: 0.0000, std: 0.6339\n",
      "📊 x.mean: -0.3716, std: 11.9420, max: 1000.0000, min: -764.1227\n",
      "📈 out.mean: 0.0000, std: 0.6881\n",
      "📊 x.mean: -0.1232, std: 54.7661, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7153\n",
      "📊 x.mean: -0.1779, std: 54.0811, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6732\n",
      "📊 x.mean: -0.1176, std: 44.5559, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6980\n",
      "📊 x.mean: -0.1728, std: 40.6089, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7002\n",
      "📊 x.mean: -0.1520, std: 53.4830, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7112\n",
      "📊 x.mean: 0.0425, std: 10.1352, max: 1000.0000, min: -458.8007\n",
      "📈 out.mean: 0.0000, std: 0.6564\n",
      "📊 x.mean: -0.0982, std: 10.9766, max: 1000.0000, min: -512.7254\n",
      "📈 out.mean: 0.0000, std: 0.7151\n",
      "📊 x.mean: -0.3239, std: 33.8728, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6843\n",
      "📊 x.mean: -0.0762, std: 52.9077, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6782\n",
      "📊 x.mean: -0.1578, std: 10.2127, max: 1000.0000, min: -179.6501\n",
      "📈 out.mean: 0.0000, std: 0.6883\n",
      "📊 x.mean: -0.1559, std: 44.3477, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7149\n",
      "📊 x.mean: -0.0468, std: 32.9360, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7031\n",
      "📊 x.mean: 0.0807, std: 34.8031, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7198\n",
      "📊 x.mean: -0.1208, std: 5.0520, max: 1000.0000, min: -179.5443\n",
      "📈 out.mean: 0.0000, std: 0.6224\n",
      "📊 x.mean: -0.2580, std: 35.4603, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6831\n",
      "📊 x.mean: -0.1188, std: 32.0414, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6719\n",
      "📊 x.mean: -0.0803, std: 33.8833, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7136\n",
      "📊 x.mean: -0.1657, std: 10.5412, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6458\n",
      "📊 x.mean: 0.2355, std: 55.8063, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7154\n",
      "📊 x.mean: -0.1098, std: 5.0356, max: 1000.0000, min: -192.2224\n",
      "📈 out.mean: 0.0000, std: 0.6640\n",
      "📊 x.mean: -0.3047, std: 31.3363, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6875\n",
      "📊 x.mean: -0.2080, std: 6.3172, max: 1000.0000, min: -174.5821\n",
      "📈 out.mean: 0.0000, std: 0.6643\n",
      "📊 x.mean: -0.2781, std: 7.0980, max: 1000.0000, min: -335.7483\n",
      "📈 out.mean: -0.0000, std: 0.6823\n",
      "📊 x.mean: -0.2116, std: 7.2351, max: 1000.0000, min: -384.4211\n",
      "📈 out.mean: 0.0000, std: 0.6667\n",
      "📊 x.mean: 0.0347, std: 45.0504, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6832\n",
      "📊 x.mean: -0.3691, std: 9.6523, max: 1000.0000, min: -492.2906\n",
      "📈 out.mean: 0.0000, std: 0.6864\n",
      "📊 x.mean: -0.0707, std: 47.5653, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6807\n",
      "📊 x.mean: -0.0231, std: 45.5030, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6804\n",
      "📊 x.mean: -0.0492, std: 6.4955, max: 1000.0000, min: -381.6771\n",
      "📈 out.mean: 0.0000, std: 0.6868\n",
      "📊 x.mean: -0.1242, std: 64.7107, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7225\n",
      "📊 x.mean: -0.3633, std: 7.4007, max: 1000.0000, min: -324.7403\n",
      "📈 out.mean: 0.0000, std: 0.6665\n",
      "📊 x.mean: -0.1695, std: 6.2373, max: 1000.0000, min: -288.3318\n",
      "📈 out.mean: 0.0000, std: 0.6944\n",
      "📊 x.mean: -0.0735, std: 54.4470, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6716\n",
      "📊 x.mean: -0.4747, std: 15.9543, max: 1000.0000, min: -277.5518\n",
      "📈 out.mean: 0.0000, std: 0.6954\n",
      "📊 x.mean: -0.0513, std: 10.8710, max: 1000.0000, min: -173.8785\n",
      "📈 out.mean: 0.0000, std: 0.6668\n",
      "📊 x.mean: -0.2331, std: 41.4145, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6556\n",
      "📊 x.mean: -0.2268, std: 10.5383, max: 1000.0000, min: -499.0820\n",
      "📈 out.mean: 0.0000, std: 0.7036\n",
      "📊 x.mean: -0.0569, std: 6.7761, max: 1000.0000, min: -121.1715\n",
      "📈 out.mean: -0.0000, std: 0.6675\n",
      "📊 x.mean: -0.1987, std: 25.2625, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6885\n",
      "📊 x.mean: -0.2632, std: 6.2704, max: 1000.0000, min: -478.7686\n",
      "📈 out.mean: 0.0000, std: 0.6545\n",
      "📊 x.mean: -0.3330, std: 31.5627, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7142\n",
      "📊 x.mean: -0.1537, std: 25.4236, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6425\n",
      "📊 x.mean: -0.1388, std: 11.9492, max: 1000.0000, min: -675.6462\n",
      "📈 out.mean: 0.0000, std: 0.6876\n",
      "📊 x.mean: -0.1809, std: 61.8429, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7121\n",
      "📊 x.mean: -0.1515, std: 32.0779, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6931\n",
      "📊 x.mean: -0.1470, std: 5.6545, max: 1000.0000, min: -249.1899\n",
      "📈 out.mean: 0.0000, std: 0.6580\n",
      "📊 x.mean: -0.2889, std: 32.4673, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7019\n",
      "📊 x.mean: -0.1073, std: 33.4712, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7118\n",
      "📊 x.mean: -0.2014, std: 45.2025, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6880\n",
      "📊 x.mean: -0.2916, std: 32.0667, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7050\n",
      "📊 x.mean: 0.0339, std: 53.4550, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7059\n",
      "📊 x.mean: -0.1255, std: 54.7859, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7058\n",
      "📊 x.mean: -0.1139, std: 31.7229, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7163\n",
      "📊 x.mean: -0.0683, std: 8.2964, max: 1000.0000, min: -264.0690\n",
      "📈 out.mean: 0.0000, std: 0.6891\n",
      "📊 x.mean: -0.1448, std: 44.9381, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7056\n",
      "📊 x.mean: -0.2858, std: 32.7325, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6836\n",
      "📊 x.mean: -0.0601, std: 20.4414, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6767\n",
      "📊 x.mean: -0.1912, std: 31.9769, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6823\n",
      "📊 x.mean: -0.1182, std: 53.3193, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6811\n",
      "📊 x.mean: -0.3334, std: 11.1535, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6676\n",
      "📊 x.mean: -0.2041, std: 7.3973, max: 1000.0000, min: -177.5592\n",
      "📈 out.mean: -0.0000, std: 0.6575\n",
      "📊 x.mean: -0.1478, std: 33.5209, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7057\n",
      "📊 x.mean: -0.1106, std: 52.2391, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6692\n",
      "📊 x.mean: -0.3284, std: 44.9294, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6913\n",
      "📊 x.mean: -0.1230, std: 43.7422, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7152\n",
      "📊 x.mean: -0.1097, std: 8.2134, max: 1000.0000, min: -281.4992\n",
      "📈 out.mean: 0.0000, std: 0.6754\n",
      "📊 x.mean: -0.2785, std: 54.5434, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7072\n",
      "📊 x.mean: -0.3516, std: 43.1298, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6797\n",
      "📊 x.mean: -0.2093, std: 32.3371, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7043\n",
      "📊 x.mean: -0.3164, std: 6.6848, max: 1000.0000, min: -356.8628\n",
      "📈 out.mean: 0.0000, std: 0.6815\n",
      "📊 x.mean: -0.0843, std: 11.4149, max: 1000.0000, min: -249.1083\n",
      "📈 out.mean: 0.0000, std: 0.7241\n",
      "📊 x.mean: -0.0628, std: 7.8148, max: 1000.0000, min: -238.0651\n",
      "📈 out.mean: -0.0000, std: 0.6454\n",
      "📊 x.mean: -0.0852, std: 7.7133, max: 1000.0000, min: -333.5802\n",
      "📈 out.mean: 0.0000, std: 0.6807\n",
      "📊 x.mean: -0.2338, std: 45.2294, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6908\n",
      "📊 x.mean: -0.1044, std: 6.2439, max: 1000.0000, min: -217.4268\n",
      "📈 out.mean: 0.0000, std: 0.6681\n",
      "📊 x.mean: -0.0080, std: 54.4337, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7274\n",
      "📊 x.mean: -0.1451, std: 57.7252, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7069\n",
      "📊 x.mean: -0.4562, std: 7.4179, max: 1000.0000, min: -457.2573\n",
      "📈 out.mean: 0.0000, std: 0.6679\n",
      "📊 x.mean: -0.1643, std: 33.4960, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7070\n",
      "📊 x.mean: -0.2692, std: 31.9005, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6770\n",
      "📊 x.mean: -0.3193, std: 6.5950, max: 1000.0000, min: -252.5141\n",
      "📈 out.mean: 0.0000, std: 0.6856\n",
      "📊 x.mean: -0.2195, std: 52.8779, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6872\n",
      "📊 x.mean: -0.1631, std: 33.4981, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6980\n",
      "📊 x.mean: -0.1128, std: 2.0046, max: 57.4743, min: -170.8985\n",
      "📈 out.mean: 0.0000, std: 0.6603\n",
      "📊 x.mean: -0.2116, std: 10.6165, max: 1000.0000, min: -313.7356\n",
      "📈 out.mean: 0.0000, std: 0.6557\n",
      "📊 x.mean: -0.1767, std: 10.0796, max: 1000.0000, min: -319.7947\n",
      "📈 out.mean: 0.0000, std: 0.6913\n",
      "📊 x.mean: -0.2085, std: 6.3100, max: 1000.0000, min: -425.0544\n",
      "📈 out.mean: 0.0000, std: 0.6489\n",
      "📊 x.mean: -0.1716, std: 59.5921, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7091\n",
      "📊 x.mean: -0.1640, std: 48.3987, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7391\n",
      "📊 x.mean: 0.0034, std: 45.2182, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7039\n",
      "📊 x.mean: -0.2780, std: 32.6836, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6651\n",
      "📊 x.mean: -0.1907, std: 53.3861, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7120\n",
      "📊 x.mean: -0.0298, std: 10.6372, max: 1000.0000, min: -440.3049\n",
      "📈 out.mean: 0.0000, std: 0.6464\n",
      "📊 x.mean: -0.1374, std: 8.7118, max: 1000.0000, min: -392.5119\n",
      "📈 out.mean: 0.0000, std: 0.6568\n",
      "📊 x.mean: -0.0866, std: 32.7646, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6723\n",
      "📊 x.mean: -0.0580, std: 35.2749, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7031\n",
      "📊 x.mean: -0.3728, std: 8.2479, max: 1000.0000, min: -172.0515\n",
      "📈 out.mean: 0.0000, std: 0.6952\n",
      "📊 x.mean: -0.1192, std: 8.3491, max: 1000.0000, min: -207.2340\n",
      "📈 out.mean: 0.0000, std: 0.7002\n",
      "📊 x.mean: -0.2120, std: 8.5956, max: 1000.0000, min: -352.5999\n",
      "📈 out.mean: -0.0000, std: 0.6964\n",
      "📊 x.mean: -0.1711, std: 23.7350, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6819\n",
      "📊 x.mean: -0.3543, std: 32.6087, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7213\n",
      "📊 x.mean: -0.0780, std: 12.6701, max: 1000.0000, min: -557.7393\n",
      "📈 out.mean: 0.0000, std: 0.6927\n",
      "📊 x.mean: -0.3343, std: 46.1332, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7134\n",
      "📊 x.mean: -0.5060, std: 33.1248, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6955\n",
      "📊 x.mean: -0.2899, std: 4.9683, max: 1000.0000, min: -419.3389\n",
      "📈 out.mean: 0.0000, std: 0.6063\n",
      "📊 x.mean: -0.0489, std: 44.8850, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6742\n",
      "📊 x.mean: -0.1908, std: 7.7372, max: 1000.0000, min: -182.3484\n",
      "📈 out.mean: 0.0000, std: 0.6576\n",
      "📊 x.mean: -0.1543, std: 45.4944, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7182\n",
      "📊 x.mean: -0.1524, std: 43.1948, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6675\n",
      "📊 x.mean: -0.3784, std: 32.4024, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6623\n",
      "📊 x.mean: -0.3238, std: 4.5439, max: 94.3127, min: -506.3114\n",
      "📈 out.mean: -0.0000, std: 0.6560\n",
      "📊 x.mean: -0.0273, std: 40.4406, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7223\n",
      "📊 x.mean: -0.2227, std: 6.6870, max: 1000.0000, min: -325.1273\n",
      "📈 out.mean: -0.0000, std: 0.6245\n",
      "📊 x.mean: -0.3381, std: 44.7795, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7080\n",
      "📊 x.mean: -0.2613, std: 4.4282, max: 227.5272, min: -461.0430\n",
      "📈 out.mean: 0.0000, std: 0.6591\n",
      "📊 x.mean: -0.3184, std: 7.6435, max: 1000.0000, min: -269.4308\n",
      "📈 out.mean: 0.0000, std: 0.6449\n",
      "📊 x.mean: -0.4304, std: 31.4003, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6775\n",
      "📊 x.mean: -0.1240, std: 43.2393, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6584\n",
      "📊 x.mean: -0.0319, std: 44.5939, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7177\n",
      "📊 x.mean: -0.2856, std: 44.8146, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6853\n",
      "📊 x.mean: -0.0475, std: 44.6662, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6613\n",
      "📊 x.mean: -0.1699, std: 44.5174, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6924\n",
      "📊 x.mean: -0.1855, std: 8.9247, max: 1000.0000, min: -575.1288\n",
      "📈 out.mean: 0.0000, std: 0.6631\n",
      "📊 x.mean: -0.2122, std: 44.8782, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6869\n",
      "📊 x.mean: -0.0295, std: 9.5029, max: 1000.0000, min: -431.5179\n",
      "📈 out.mean: 0.0000, std: 0.6721\n",
      "📊 x.mean: -0.2402, std: 11.5050, max: 1000.0000, min: -654.7923\n",
      "📈 out.mean: 0.0000, std: 0.6761\n",
      "📊 x.mean: -0.2024, std: 11.6295, max: 1000.0000, min: -334.1123\n",
      "📈 out.mean: 0.0000, std: 0.6893\n",
      "📊 x.mean: 0.0255, std: 16.9705, max: 1000.0000, min: -491.9918\n",
      "📈 out.mean: 0.0000, std: 0.6743\n",
      "📊 x.mean: -0.0906, std: 32.4511, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7079\n",
      "📊 x.mean: 0.1447, std: 45.5226, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7167\n",
      "📊 x.mean: -0.0244, std: 10.8333, max: 1000.0000, min: -320.7720\n",
      "📈 out.mean: 0.0000, std: 0.6753\n",
      "📊 x.mean: -0.1108, std: 45.2617, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7129\n",
      "📊 x.mean: -0.1215, std: 31.8587, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7125\n",
      "📊 x.mean: -0.4102, std: 6.3738, max: 1000.0000, min: -403.8243\n",
      "📈 out.mean: 0.0000, std: 0.6754\n",
      "📊 x.mean: -0.4328, std: 9.5023, max: 1000.0000, min: -645.8140\n",
      "📈 out.mean: -0.0000, std: 0.6688\n",
      "📊 x.mean: -0.0153, std: 45.2452, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6920\n",
      "📊 x.mean: 0.0071, std: 15.6890, max: 1000.0000, min: -277.0338\n",
      "📈 out.mean: 0.0000, std: 0.7143\n",
      "📊 x.mean: -0.3954, std: 31.8918, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6777\n",
      "📊 x.mean: -0.2157, std: 23.2859, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6651\n",
      "📊 x.mean: -0.2218, std: 4.1827, max: 1000.0000, min: -184.5072\n",
      "📈 out.mean: 0.0000, std: 0.6438\n",
      "📊 x.mean: -0.1407, std: 32.6605, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6833\n",
      "📊 x.mean: -0.3002, std: 33.0739, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7246\n",
      "📊 x.mean: -0.3164, std: 7.8565, max: 1000.0000, min: -617.2565\n",
      "📈 out.mean: 0.0000, std: 0.6790\n",
      "📊 x.mean: -0.1661, std: 58.2273, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7091\n",
      "📊 x.mean: -0.2411, std: 9.2360, max: 1000.0000, min: -175.3758\n",
      "📈 out.mean: 0.0000, std: 0.6518\n",
      "📊 x.mean: -0.1102, std: 6.5272, max: 1000.0000, min: -194.9218\n",
      "📈 out.mean: 0.0000, std: 0.6926\n",
      "📊 x.mean: -0.1891, std: 53.5432, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7326\n",
      "📊 x.mean: -0.0752, std: 0.6025, max: 88.4155, min: -4.0722\n",
      "📈 out.mean: 0.0000, std: 0.6435\n",
      "📊 x.mean: -0.0996, std: 9.4184, max: 1000.0000, min: -905.9398\n",
      "📈 out.mean: 0.0000, std: 0.7846\n",
      "📊 x.mean: -0.1865, std: 10.1813, max: 1000.0000, min: -503.8100\n",
      "📈 out.mean: 0.0000, std: 0.7851\n",
      "📊 x.mean: -0.1867, std: 32.2871, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8102\n",
      "📊 x.mean: -0.1921, std: 11.6156, max: 1000.0000, min: -370.2814\n",
      "📈 out.mean: -0.0000, std: 0.7966\n",
      "📊 x.mean: -0.1150, std: 32.0145, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8216\n",
      "📊 x.mean: -0.2310, std: 6.9207, max: 1000.0000, min: -728.9986\n",
      "📈 out.mean: 0.0000, std: 0.8123\n",
      "📊 x.mean: -0.1160, std: 5.7803, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8050\n",
      "📊 x.mean: -0.2061, std: 53.9520, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7836\n",
      "📊 x.mean: -0.3497, std: 44.4879, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7980\n",
      "📊 x.mean: -0.1838, std: 12.9944, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7790\n",
      "📊 x.mean: -0.4383, std: 44.8396, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8009\n",
      "📊 x.mean: -0.1365, std: 9.8898, max: 1000.0000, min: -296.7798\n",
      "📈 out.mean: 0.0000, std: 0.7868\n",
      "📊 x.mean: -0.1040, std: 31.9115, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8111\n",
      "📊 x.mean: -0.3741, std: 12.5224, max: 1000.0000, min: -785.3012\n",
      "📈 out.mean: 0.0000, std: 0.8093\n",
      "📊 x.mean: -0.0343, std: 33.2179, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8097\n",
      "📊 x.mean: -0.4159, std: 45.0037, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7811\n",
      "📊 x.mean: -0.2497, std: 19.3441, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7982\n",
      "📊 x.mean: 0.0593, std: 13.5777, max: 1000.0000, min: -268.7392\n",
      "📈 out.mean: -0.0000, std: 0.8230\n",
      "📊 x.mean: -0.0908, std: 6.0506, max: 1000.0000, min: -230.7204\n",
      "📈 out.mean: 0.0000, std: 0.8094\n",
      "📊 x.mean: -0.0962, std: 33.6064, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8089\n",
      "📊 x.mean: -0.3228, std: 45.7054, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7795\n",
      "📊 x.mean: -0.2959, std: 31.3613, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7790\n",
      "📊 x.mean: -0.4411, std: 67.7860, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8003\n",
      "📊 x.mean: -0.0586, std: 45.3556, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7409\n",
      "📊 x.mean: -0.0753, std: 32.7296, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7948\n",
      "📊 x.mean: -0.1564, std: 8.8914, max: 1000.0000, min: -244.4408\n",
      "📈 out.mean: 0.0000, std: 0.7799\n",
      "📊 x.mean: -0.2866, std: 11.6969, max: 1000.0000, min: -827.2722\n",
      "📈 out.mean: 0.0000, std: 0.7920\n",
      "📊 x.mean: -0.1757, std: 49.6083, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7933\n",
      "📊 x.mean: -0.1965, std: 63.2539, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8019\n",
      "📊 x.mean: -0.2235, std: 43.2777, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7848\n",
      "📊 x.mean: -0.2317, std: 44.2783, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8230\n",
      "📊 x.mean: 0.0904, std: 15.4254, max: 1000.0000, min: -279.1282\n",
      "📈 out.mean: 0.0000, std: 0.8099\n",
      "📊 x.mean: -0.3737, std: 49.9354, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8117\n",
      "📊 x.mean: -0.1996, std: 32.6460, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7857\n",
      "📊 x.mean: -0.1998, std: 32.0245, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7759\n",
      "📊 x.mean: -0.2331, std: 43.7167, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7742\n",
      "📊 x.mean: -0.2211, std: 53.6199, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7799\n",
      "📊 x.mean: -0.0348, std: 13.0135, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7908\n",
      "📊 x.mean: -0.2269, std: 9.2998, max: 1000.0000, min: -445.5513\n",
      "📈 out.mean: -0.0000, std: 0.7820\n",
      "📊 x.mean: -0.0189, std: 33.7131, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7718\n",
      "📊 x.mean: -0.0933, std: 32.0371, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.8277\n",
      "📊 x.mean: -0.1164, std: 8.3571, max: 1000.0000, min: -436.2136\n",
      "📈 out.mean: 0.0000, std: 0.7904\n",
      "\n",
      "🧪 Epoch 1/50\n",
      "Loss entrenamiento: 2.0574\n",
      "Loss validación  : 2.2334\n",
      "\n",
      "📊 Clasificación (val):\n",
      "Clase        0: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        1: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        2: Precisión=0.79  Recall=0.19  F1=0.31\n",
      "Clase        3: Precisión=0.00  Recall=0.40  F1=0.00\n",
      "Clase        4: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        5: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        6: Precisión=0.53  Recall=0.14  F1=0.22\n",
      "Clase        7: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        8: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        9: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase macro avg: Precisión=0.13  Recall=0.07  F1=0.05\n",
      "Clase weighted avg: Precisión=0.46  Recall=0.12  F1=0.18\n",
      "💾 Modelo mejorado guardado (mejor_modelo.pt).\n",
      "📊 x.mean: -0.1746, std: 69.6318, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7306\n",
      "📊 x.mean: -0.1819, std: 7.9035, max: 1000.0000, min: -384.4211\n",
      "📈 out.mean: 0.0000, std: 0.6659\n",
      "📊 x.mean: -0.2677, std: 9.4277, max: 1000.0000, min: -200.4157\n",
      "📈 out.mean: -0.0000, std: 0.6763\n",
      "📊 x.mean: -0.0969, std: 7.6610, max: 1000.0000, min: -458.8007\n",
      "📈 out.mean: -0.0000, std: 0.6733\n",
      "📊 x.mean: -0.1204, std: 31.5142, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6949\n",
      "📊 x.mean: -0.1810, std: 10.2964, max: 1000.0000, min: -757.5992\n",
      "📈 out.mean: 0.0000, std: 0.6880\n",
      "📊 x.mean: 0.0073, std: 45.1933, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6974\n",
      "📊 x.mean: -0.3824, std: 32.2466, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6839\n",
      "📊 x.mean: -0.2569, std: 70.3789, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7200\n",
      "📊 x.mean: -0.1888, std: 42.8478, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6977\n",
      "📊 x.mean: -0.1851, std: 31.1524, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7182\n",
      "📊 x.mean: -0.2642, std: 44.5124, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7012\n",
      "📊 x.mean: -0.1185, std: 7.9704, max: 1000.0000, min: -512.7254\n",
      "📈 out.mean: 0.0000, std: 0.6905\n",
      "📊 x.mean: -0.1629, std: 9.2039, max: 1000.0000, min: -587.0281\n",
      "📈 out.mean: 0.0000, std: 0.7246\n",
      "📊 x.mean: -0.0405, std: 70.1961, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7090\n",
      "📊 x.mean: -0.1483, std: 47.3618, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6783\n",
      "📊 x.mean: 0.0573, std: 44.7667, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6972\n",
      "📊 x.mean: -0.2311, std: 12.5414, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6988\n",
      "📊 x.mean: -0.2568, std: 10.3951, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6870\n",
      "📊 x.mean: 0.0065, std: 32.7207, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6977\n",
      "📊 x.mean: -0.1173, std: 9.3016, max: 1000.0000, min: -174.8472\n",
      "📈 out.mean: 0.0000, std: 0.6711\n",
      "📊 x.mean: -0.1541, std: 54.8963, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7117\n",
      "📊 x.mean: 0.0257, std: 15.0864, max: 1000.0000, min: -232.2841\n",
      "📈 out.mean: 0.0000, std: 0.6875\n",
      "📊 x.mean: -0.3736, std: 32.2827, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6992\n",
      "📊 x.mean: -0.1609, std: 31.8871, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6614\n",
      "📊 x.mean: -0.3554, std: 33.0235, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7031\n",
      "📊 x.mean: -0.1843, std: 31.8386, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6873\n",
      "📊 x.mean: -0.0587, std: 12.4800, max: 1000.0000, min: -274.0083\n",
      "📈 out.mean: 0.0000, std: 0.6883\n",
      "📊 x.mean: -0.3114, std: 33.0209, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6883\n",
      "📊 x.mean: -0.1469, std: 61.9084, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7367\n",
      "📊 x.mean: 0.0947, std: 34.1573, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6781\n",
      "📊 x.mean: -0.0596, std: 63.2728, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7451\n",
      "📊 x.mean: -0.0099, std: 40.6767, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6985\n",
      "📊 x.mean: -0.1852, std: 40.8406, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7044\n",
      "📊 x.mean: -0.2117, std: 31.5182, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6809\n",
      "📊 x.mean: -0.0860, std: 32.3950, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7149\n",
      "📊 x.mean: -0.2111, std: 33.7934, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6868\n",
      "📊 x.mean: -0.0493, std: 32.8922, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7061\n",
      "📊 x.mean: -0.3654, std: 31.6511, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6686\n",
      "📊 x.mean: -0.1486, std: 9.8285, max: 1000.0000, min: -338.3105\n",
      "📈 out.mean: 0.0000, std: 0.6692\n",
      "📊 x.mean: 0.0709, std: 18.9777, max: 1000.0000, min: -871.1334\n",
      "📈 out.mean: 0.0000, std: 0.6994\n",
      "📊 x.mean: -0.4163, std: 38.1322, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6630\n",
      "📊 x.mean: 0.0040, std: 25.5400, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6779\n",
      "📊 x.mean: -0.3640, std: 44.4220, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7028\n",
      "📊 x.mean: -0.2490, std: 7.6264, max: 1000.0000, min: -200.4314\n",
      "📈 out.mean: 0.0000, std: 0.6480\n",
      "📊 x.mean: -0.1714, std: 44.7070, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6764\n",
      "📊 x.mean: -0.3622, std: 13.2399, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7196\n",
      "📊 x.mean: -0.3887, std: 8.0098, max: 1000.0000, min: -680.9859\n",
      "📈 out.mean: 0.0000, std: 0.6216\n",
      "📊 x.mean: -0.1647, std: 9.0275, max: 1000.0000, min: -264.0749\n",
      "📈 out.mean: 0.0000, std: 0.6744\n",
      "📊 x.mean: -0.2503, std: 6.1858, max: 1000.0000, min: -305.3245\n",
      "📈 out.mean: 0.0000, std: 0.6739\n",
      "📊 x.mean: -0.2586, std: 36.8511, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6925\n",
      "📊 x.mean: -0.3493, std: 44.4699, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7008\n",
      "📊 x.mean: -0.2052, std: 11.1341, max: 1000.0000, min: -852.2794\n",
      "📈 out.mean: 0.0000, std: 0.7085\n",
      "📊 x.mean: -0.4228, std: 31.4436, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6798\n",
      "📊 x.mean: -0.2037, std: 6.4972, max: 1000.0000, min: -361.7283\n",
      "📈 out.mean: 0.0000, std: 0.6732\n",
      "📊 x.mean: -0.2056, std: 31.7997, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7067\n",
      "📊 x.mean: -0.1147, std: 11.5613, max: 1000.0000, min: -298.6991\n",
      "📈 out.mean: 0.0000, std: 0.6812\n",
      "📊 x.mean: -0.1926, std: 13.9989, max: 1000.0000, min: -349.8352\n",
      "📈 out.mean: 0.0000, std: 0.6879\n",
      "📊 x.mean: -0.1908, std: 32.4349, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6693\n",
      "📊 x.mean: -0.1839, std: 6.4862, max: 1000.0000, min: -356.8628\n",
      "📈 out.mean: 0.0000, std: 0.6583\n",
      "📊 x.mean: 0.0111, std: 10.6803, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7088\n",
      "📊 x.mean: -0.0812, std: 7.8704, max: 1000.0000, min: -280.6011\n",
      "📈 out.mean: 0.0000, std: 0.6801\n",
      "📊 x.mean: -0.1288, std: 32.2141, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6402\n",
      "📊 x.mean: 0.0171, std: 45.5081, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6768\n",
      "📊 x.mean: -0.2246, std: 21.7448, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6785\n",
      "📊 x.mean: -0.1412, std: 8.6745, max: 1000.0000, min: -662.3247\n",
      "📈 out.mean: 0.0000, std: 0.6836\n",
      "📊 x.mean: -0.2149, std: 45.4085, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7088\n",
      "📊 x.mean: -0.1176, std: 32.0495, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6931\n",
      "📊 x.mean: -0.1135, std: 31.6201, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6969\n",
      "📊 x.mean: -0.1032, std: 45.6234, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6989\n",
      "📊 x.mean: -0.0211, std: 32.2930, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7179\n",
      "📊 x.mean: -0.0068, std: 32.3056, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6850\n",
      "📊 x.mean: -0.0548, std: 45.3080, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7061\n",
      "📊 x.mean: 0.0177, std: 58.8687, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7197\n",
      "📊 x.mean: 0.0146, std: 46.2721, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7353\n",
      "📊 x.mean: -0.1907, std: 44.5995, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6873\n",
      "📊 x.mean: -0.0110, std: 44.2696, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6883\n",
      "📊 x.mean: -0.2301, std: 43.3743, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6927\n",
      "📊 x.mean: -0.1861, std: 32.3170, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7144\n",
      "📊 x.mean: 0.0840, std: 45.4787, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7023\n",
      "📊 x.mean: -0.0890, std: 8.4781, max: 1000.0000, min: -457.2573\n",
      "📈 out.mean: 0.0000, std: 0.6526\n",
      "📊 x.mean: -0.1540, std: 33.2003, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7136\n",
      "📊 x.mean: -0.3016, std: 8.5229, max: 1000.0000, min: -874.2003\n",
      "📈 out.mean: -0.0000, std: 0.6412\n",
      "📊 x.mean: -0.0280, std: 45.1806, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7076\n",
      "📊 x.mean: -0.1817, std: 43.6940, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6801\n",
      "📊 x.mean: -0.3514, std: 4.8155, max: 1000.0000, min: -252.9120\n",
      "📈 out.mean: -0.0000, std: 0.6560\n",
      "📊 x.mean: -0.1373, std: 36.0355, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6790\n",
      "📊 x.mean: -0.0832, std: 12.4113, max: 1000.0000, min: -205.1570\n",
      "📈 out.mean: -0.0000, std: 0.6662\n",
      "📊 x.mean: -0.1911, std: 24.0546, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6945\n",
      "📊 x.mean: -0.4961, std: 7.5218, max: 1000.0000, min: -252.8529\n",
      "📈 out.mean: 0.0000, std: 0.6889\n",
      "📊 x.mean: -0.2530, std: 8.9373, max: 1000.0000, min: -197.0653\n",
      "📈 out.mean: 0.0000, std: 0.6499\n",
      "📊 x.mean: -0.1717, std: 10.4872, max: 1000.0000, min: -492.2906\n",
      "📈 out.mean: 0.0000, std: 0.6701\n",
      "📊 x.mean: -0.1199, std: 7.5215, max: 1000.0000, min: -134.2098\n",
      "📈 out.mean: 0.0000, std: 0.6609\n",
      "📊 x.mean: -0.2516, std: 8.4271, max: 1000.0000, min: -782.4775\n",
      "📈 out.mean: -0.0000, std: 0.6820\n",
      "📊 x.mean: -0.4466, std: 44.0006, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7011\n",
      "📊 x.mean: -0.3207, std: 31.9544, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6916\n",
      "📊 x.mean: -0.3274, std: 31.6835, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6811\n",
      "📊 x.mean: 0.1766, std: 23.3690, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7078\n",
      "📊 x.mean: -0.1468, std: 32.5609, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6391\n",
      "📊 x.mean: -0.4450, std: 31.7024, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7148\n",
      "📊 x.mean: 0.4346, std: 44.8860, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6943\n",
      "📊 x.mean: -0.1382, std: 44.3186, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7117\n",
      "📊 x.mean: -0.1039, std: 32.1808, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6994\n",
      "📊 x.mean: -0.1549, std: 33.8737, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6847\n",
      "📊 x.mean: -0.0967, std: 10.3867, max: 1000.0000, min: -343.9219\n",
      "📈 out.mean: 0.0000, std: 0.6740\n",
      "📊 x.mean: -0.1107, std: 9.5599, max: 1000.0000, min: -195.3443\n",
      "📈 out.mean: 0.0000, std: 0.6661\n",
      "📊 x.mean: -0.3142, std: 32.3027, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6778\n",
      "📊 x.mean: -0.1320, std: 44.9302, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6738\n",
      "📊 x.mean: 0.0249, std: 21.5893, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6922\n",
      "📊 x.mean: -0.2583, std: 31.4687, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7111\n",
      "📊 x.mean: -0.1960, std: 45.6752, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7160\n",
      "📊 x.mean: -0.2234, std: 8.3552, max: 1000.0000, min: -403.8243\n",
      "📈 out.mean: 0.0000, std: 0.6982\n",
      "📊 x.mean: -0.1997, std: 57.0065, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7130\n",
      "📊 x.mean: -0.3666, std: 7.0616, max: 1000.0000, min: -499.0820\n",
      "📈 out.mean: 0.0000, std: 0.6401\n",
      "📊 x.mean: -0.3618, std: 5.6542, max: 1000.0000, min: -240.5550\n",
      "📈 out.mean: -0.0000, std: 0.6852\n",
      "📊 x.mean: -0.2003, std: 53.1526, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6983\n",
      "📊 x.mean: -0.2561, std: 6.0952, max: 1000.0000, min: -388.9590\n",
      "📈 out.mean: 0.0000, std: 0.6655\n",
      "📊 x.mean: -0.2457, std: 4.3682, max: 1000.0000, min: -313.7356\n",
      "📈 out.mean: 0.0000, std: 0.6717\n",
      "📊 x.mean: -0.1923, std: 31.4887, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6457\n",
      "📊 x.mean: -0.1577, std: 37.3747, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7150\n",
      "📊 x.mean: -0.1366, std: 32.6575, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6950\n",
      "📊 x.mean: 0.0135, std: 14.0258, max: 1000.0000, min: -562.2206\n",
      "📈 out.mean: 0.0000, std: 0.7026\n",
      "📊 x.mean: 0.0643, std: 32.0697, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7113\n",
      "📊 x.mean: -0.0633, std: 9.3661, max: 1000.0000, min: -570.3259\n",
      "📈 out.mean: 0.0000, std: 0.6392\n",
      "📊 x.mean: -0.0481, std: 5.3022, max: 1000.0000, min: -229.0974\n",
      "📈 out.mean: -0.0000, std: 0.6595\n",
      "📊 x.mean: -0.1483, std: 8.1070, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6630\n",
      "📊 x.mean: 0.1414, std: 33.4593, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6992\n",
      "📊 x.mean: -0.2953, std: 9.2562, max: 1000.0000, min: -557.7393\n",
      "📈 out.mean: 0.0000, std: 0.6824\n",
      "📊 x.mean: -0.1283, std: 31.2481, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6836\n",
      "📊 x.mean: -0.4038, std: 40.5557, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6599\n",
      "📊 x.mean: -0.1303, std: 8.0128, max: 1000.0000, min: -323.7151\n",
      "📈 out.mean: 0.0000, std: 0.6924\n",
      "📊 x.mean: -0.0431, std: 45.8093, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6946\n",
      "📊 x.mean: -0.2884, std: 32.4918, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7142\n",
      "📊 x.mean: -0.2154, std: 42.8279, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6874\n",
      "📊 x.mean: -0.0773, std: 39.6237, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6641\n",
      "📊 x.mean: -0.2449, std: 32.5548, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6723\n",
      "📊 x.mean: -0.2601, std: 6.8286, max: 1000.0000, min: -256.7592\n",
      "📈 out.mean: 0.0000, std: 0.6740\n",
      "📊 x.mean: -0.3403, std: 13.9256, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6622\n",
      "📊 x.mean: -0.0668, std: 43.0412, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6814\n",
      "📊 x.mean: -0.3451, std: 32.2157, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6652\n",
      "📊 x.mean: -0.6038, std: 12.7295, max: 1000.0000, min: -530.1432\n",
      "📈 out.mean: 0.0000, std: 0.6387\n",
      "📊 x.mean: -0.4824, std: 10.2860, max: 1000.0000, min: -304.1247\n",
      "📈 out.mean: -0.0000, std: 0.6837\n",
      "📊 x.mean: -0.2696, std: 53.4353, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6801\n",
      "📊 x.mean: -0.0848, std: 7.1559, max: 1000.0000, min: -444.1552\n",
      "📈 out.mean: 0.0000, std: 0.6586\n",
      "📊 x.mean: -0.2617, std: 31.7847, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6364\n",
      "📊 x.mean: -0.1667, std: 31.7859, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6903\n",
      "📊 x.mean: -0.1889, std: 3.4852, max: 297.1121, min: -323.4193\n",
      "📈 out.mean: 0.0000, std: 0.6479\n",
      "📊 x.mean: -0.1642, std: 31.8969, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6840\n",
      "📊 x.mean: -0.1729, std: 44.4440, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6925\n",
      "📊 x.mean: -0.3318, std: 31.9634, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7100\n",
      "📊 x.mean: -0.1704, std: 45.9860, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6761\n",
      "📊 x.mean: -0.1557, std: 52.6360, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7304\n",
      "📊 x.mean: -0.1829, std: 9.3494, max: 1000.0000, min: -262.3409\n",
      "📈 out.mean: 0.0000, std: 0.7012\n",
      "📊 x.mean: -0.2364, std: 13.9497, max: 1000.0000, min: -667.8555\n",
      "📈 out.mean: 0.0000, std: 0.6607\n",
      "📊 x.mean: -0.0886, std: 44.1262, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6881\n",
      "📊 x.mean: -0.0904, std: 45.3282, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7213\n",
      "📊 x.mean: 0.0108, std: 11.4134, max: 1000.0000, min: -240.6472\n",
      "📈 out.mean: 0.0000, std: 0.6794\n",
      "📊 x.mean: -0.0234, std: 55.1247, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6826\n",
      "📊 x.mean: -0.2416, std: 32.0728, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6914\n",
      "📊 x.mean: -0.2641, std: 44.2995, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7113\n",
      "📊 x.mean: 0.0470, std: 11.6182, max: 1000.0000, min: -216.2894\n",
      "📈 out.mean: 0.0000, std: 0.6758\n",
      "📊 x.mean: -0.1481, std: 6.6756, max: 1000.0000, min: -282.3818\n",
      "📈 out.mean: 0.0000, std: 0.6760\n",
      "📊 x.mean: -0.1559, std: 45.4378, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6863\n",
      "📊 x.mean: -0.1174, std: 60.0783, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7063\n",
      "📊 x.mean: -0.2672, std: 10.3626, max: 1000.0000, min: -755.6356\n",
      "📈 out.mean: 0.0000, std: 0.7001\n",
      "📊 x.mean: -0.2637, std: 7.2699, max: 1000.0000, min: -201.2643\n",
      "📈 out.mean: 0.0000, std: 0.6535\n",
      "📊 x.mean: -0.1453, std: 31.5805, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6653\n",
      "📊 x.mean: -0.7824, std: 6.6377, max: 94.5122, min: -94.2187\n",
      "📈 out.mean: 0.0000, std: 0.6031\n",
      "📊 x.mean: -0.0996, std: 9.4184, max: 1000.0000, min: -905.9398\n",
      "📈 out.mean: 0.0000, std: 0.7686\n",
      "📊 x.mean: -0.1865, std: 10.1813, max: 1000.0000, min: -503.8100\n",
      "📈 out.mean: 0.0000, std: 0.7682\n",
      "📊 x.mean: -0.1867, std: 32.2871, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7958\n",
      "📊 x.mean: -0.1921, std: 11.6156, max: 1000.0000, min: -370.2814\n",
      "📈 out.mean: 0.0000, std: 0.7837\n",
      "📊 x.mean: -0.1150, std: 32.0145, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8079\n",
      "📊 x.mean: -0.2310, std: 6.9207, max: 1000.0000, min: -728.9986\n",
      "📈 out.mean: 0.0000, std: 0.7987\n",
      "📊 x.mean: -0.1160, std: 5.7803, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7906\n",
      "📊 x.mean: -0.2061, std: 53.9520, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7697\n",
      "📊 x.mean: -0.3497, std: 44.4879, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7814\n",
      "📊 x.mean: -0.1838, std: 12.9944, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7639\n",
      "📊 x.mean: -0.4383, std: 44.8396, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7859\n",
      "📊 x.mean: -0.1365, std: 9.8898, max: 1000.0000, min: -296.7798\n",
      "📈 out.mean: 0.0000, std: 0.7730\n",
      "📊 x.mean: -0.1040, std: 31.9115, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7970\n",
      "📊 x.mean: -0.3741, std: 12.5224, max: 1000.0000, min: -785.3012\n",
      "📈 out.mean: 0.0000, std: 0.7939\n",
      "📊 x.mean: -0.0343, std: 33.2179, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7928\n",
      "📊 x.mean: -0.4159, std: 45.0037, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7656\n",
      "📊 x.mean: -0.2497, std: 19.3441, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7836\n",
      "📊 x.mean: 0.0593, std: 13.5777, max: 1000.0000, min: -268.7392\n",
      "📈 out.mean: 0.0000, std: 0.8096\n",
      "📊 x.mean: -0.0908, std: 6.0506, max: 1000.0000, min: -230.7204\n",
      "📈 out.mean: -0.0000, std: 0.7952\n",
      "📊 x.mean: -0.0962, std: 33.6064, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7950\n",
      "📊 x.mean: -0.3228, std: 45.7054, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7633\n",
      "📊 x.mean: -0.2959, std: 31.3613, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7617\n",
      "📊 x.mean: -0.4411, std: 67.7860, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7856\n",
      "📊 x.mean: -0.0586, std: 45.3556, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7238\n",
      "📊 x.mean: -0.0753, std: 32.7296, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7800\n",
      "📊 x.mean: -0.1564, std: 8.8914, max: 1000.0000, min: -244.4408\n",
      "📈 out.mean: -0.0000, std: 0.7645\n",
      "📊 x.mean: -0.2866, std: 11.6969, max: 1000.0000, min: -827.2722\n",
      "📈 out.mean: 0.0000, std: 0.7767\n",
      "📊 x.mean: -0.1757, std: 49.6083, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7792\n",
      "📊 x.mean: -0.1965, std: 63.2539, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7874\n",
      "📊 x.mean: -0.2235, std: 43.2777, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7702\n",
      "📊 x.mean: -0.2317, std: 44.2783, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8096\n",
      "📊 x.mean: 0.0904, std: 15.4254, max: 1000.0000, min: -279.1282\n",
      "📈 out.mean: 0.0000, std: 0.7960\n",
      "📊 x.mean: -0.3737, std: 49.9354, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7978\n",
      "📊 x.mean: -0.1996, std: 32.6460, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7708\n",
      "📊 x.mean: -0.1998, std: 32.0245, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7619\n",
      "📊 x.mean: -0.2331, std: 43.7167, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7576\n",
      "📊 x.mean: -0.2211, std: 53.6199, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7655\n",
      "📊 x.mean: -0.0348, std: 13.0135, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.7780\n",
      "📊 x.mean: -0.2269, std: 9.2998, max: 1000.0000, min: -445.5513\n",
      "📈 out.mean: 0.0000, std: 0.7663\n",
      "📊 x.mean: -0.0189, std: 33.7131, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7551\n",
      "📊 x.mean: -0.0933, std: 32.0371, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.8156\n",
      "📊 x.mean: -0.1164, std: 8.3571, max: 1000.0000, min: -436.2136\n",
      "📈 out.mean: -0.0000, std: 0.7750\n",
      "\n",
      "🧪 Epoch 2/50\n",
      "Loss entrenamiento: 1.9331\n",
      "Loss validación  : 2.1465\n",
      "\n",
      "📊 Clasificación (val):\n",
      "Clase        0: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        1: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        2: Precisión=0.82  Recall=0.31  F1=0.45\n",
      "Clase        3: Precisión=0.00  Recall=0.40  F1=0.00\n",
      "Clase        4: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        5: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        6: Precisión=0.28  Recall=0.18  F1=0.22\n",
      "Clase        7: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        8: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase        9: Precisión=0.00  Recall=0.00  F1=0.00\n",
      "Clase macro avg: Precisión=0.11  Recall=0.09  F1=0.07\n",
      "Clase weighted avg: Precisión=0.46  Recall=0.18  F1=0.26\n",
      "💾 Modelo mejorado guardado (mejor_modelo.pt).\n",
      "📊 x.mean: 0.0750, std: 33.9646, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7021\n",
      "📊 x.mean: -0.2496, std: 44.7925, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7059\n",
      "📊 x.mean: -0.3243, std: 8.3921, max: 1000.0000, min: -352.5999\n",
      "📈 out.mean: 0.0000, std: 0.6949\n",
      "📊 x.mean: -0.1124, std: 7.0042, max: 1000.0000, min: -449.7614\n",
      "📈 out.mean: 0.0000, std: 0.6776\n",
      "📊 x.mean: 0.1209, std: 33.3561, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7042\n",
      "📊 x.mean: -0.4235, std: 11.9321, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7189\n",
      "📊 x.mean: -0.3496, std: 7.4653, max: 1000.0000, min: -662.3247\n",
      "📈 out.mean: 0.0000, std: 0.6847\n",
      "📊 x.mean: -0.3285, std: 10.9063, max: 1000.0000, min: -277.5518\n",
      "📈 out.mean: 0.0000, std: 0.6853\n",
      "📊 x.mean: -0.1252, std: 13.0071, max: 1000.0000, min: -136.7940\n",
      "📈 out.mean: 0.0000, std: 0.6891\n",
      "📊 x.mean: -0.1160, std: 9.6967, max: 1000.0000, min: -164.6382\n",
      "📈 out.mean: 0.0000, std: 0.6652\n",
      "📊 x.mean: -0.1251, std: 31.4913, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6760\n",
      "📊 x.mean: -0.1693, std: 53.9819, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7283\n",
      "📊 x.mean: -0.2453, std: 32.9842, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7088\n",
      "📊 x.mean: 0.2595, std: 64.9855, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7351\n",
      "📊 x.mean: -0.1366, std: 31.7880, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7130\n",
      "📊 x.mean: -0.0029, std: 8.7487, max: 1000.0000, min: -125.5476\n",
      "📈 out.mean: 0.0000, std: 0.6608\n",
      "📊 x.mean: 0.0925, std: 11.8921, max: 1000.0000, min: -575.1288\n",
      "📈 out.mean: 0.0000, std: 0.6759\n",
      "📊 x.mean: -0.1697, std: 2.8276, max: 58.7684, min: -284.8349\n",
      "📈 out.mean: -0.0000, std: 0.6863\n",
      "📊 x.mean: -0.5147, std: 9.8801, max: 1000.0000, min: -667.8555\n",
      "📈 out.mean: 0.0000, std: 0.6521\n",
      "📊 x.mean: -0.1686, std: 44.3310, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7022\n",
      "📊 x.mean: -0.3061, std: 43.9624, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6856\n",
      "📊 x.mean: -0.1387, std: 8.6391, max: 1000.0000, min: -265.4407\n",
      "📈 out.mean: 0.0000, std: 0.6326\n",
      "📊 x.mean: -0.1459, std: 8.2260, max: 1000.0000, min: -852.2794\n",
      "📈 out.mean: 0.0000, std: 0.6928\n",
      "📊 x.mean: -0.1453, std: 32.1969, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7080\n",
      "📊 x.mean: -0.0914, std: 46.1904, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: -0.0000, std: 0.6853\n",
      "📊 x.mean: -0.2570, std: 31.7185, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.6735\n",
      "📊 x.mean: 0.0411, std: 53.4602, max: 1000.0000, min: -1000.0000\n",
      "📈 out.mean: 0.0000, std: 0.7165\n",
      "📊 x.mean: -0.2113, std: 53.9617, max: 1000.0000, min: -1000.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Ejecutar entrenamiento con early stopping y gráfica\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Entrenando modelo...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# transfer learning clásico\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# early stopping\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# True para depuración\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_2_transformer_training.py:152\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(train_loader, val_loader, num_classes, device, epochs, lr, freeze_encoder, patience, debug)\u001b[39m\n\u001b[32m    149\u001b[39m epochs_no_improve = \u001b[32m0\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     loss_train = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     loss_val, report = evaluate(model, val_loader, criterion, device, save_errors=\u001b[38;5;28;01mTrue\u001b[39;00m, label_encoder=label_encoder)\n\u001b[32m    155\u001b[39m     train_losses.append(loss_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_2_transformer_training.py:59\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, debug)\u001b[39m\n\u001b[32m     57\u001b[39m optimizer.zero_grad()\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     loss = criterion(outputs, y)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(loss):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_2_transformer_training.py:44\u001b[39m, in \u001b[36mAstroConformerClassifier.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     42\u001b[39m out = out.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     43\u001b[39m RoPE = \u001b[38;5;28mself\u001b[39m.encoder.pe(out, out.shape[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRoPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m out = out.mean(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📈 out.mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.mean().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.std().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\Astroconformer\\Astroconformer\\Model\\Modules\\conformer.py:360\u001b[39m, in \u001b[36mConformerEncoder.forward\u001b[39m\u001b[34m(self, x, RoPE, key_padding_mask)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03mForward propagate a `inputs` for  encoder training.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m \u001b[33;03m    * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRoPE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRoPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\Astroconformer\\Astroconformer\\Model\\Modules\\conformer.py:308\u001b[39m, in \u001b[36mConformerBlock.forward\u001b[39m\u001b[34m(self, x, RoPE, key_padding_mask)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.modlist:\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m.module, MHA_rotary):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m         x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRoPE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRoPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    310\u001b[39m         x = m(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\Astroconformer\\Astroconformer\\Model\\Modules\\conformer.py:40\u001b[39m, in \u001b[36mPostNorm.forward\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, **kwargs) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m + (inputs * \u001b[38;5;28mself\u001b[39m.input_factor))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\Astroconformer\\Astroconformer\\Model\\Modules\\mhsa_pro.py:117\u001b[39m, in \u001b[36mMHA_rotary.forward\u001b[39m\u001b[34m(self, x, RoPE, key_padding_mask)\u001b[39m\n\u001b[32m    115\u001b[39m     key_padding_mask = key_padding_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]           \u001b[38;5;66;03m# (B, T) -> (B, 1, 1, T)\u001b[39;00m\n\u001b[32m    116\u001b[39m     att = att.masked_fill(key_padding_mask == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m att = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m                                                  \u001b[38;5;66;03m# softmax\u001b[39;00m\n\u001b[32m    119\u001b[39m x = att @ v                                                                     \u001b[38;5;66;03m# (B, nh, T, T) * (B, nh, T, hs) -> (B, nh, T, hs)\u001b[39;00m\n\u001b[32m    120\u001b[39m x = x.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, -\u001b[32m1\u001b[39m)                               \u001b[38;5;66;03m# (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\site-packages\\torch\\nn\\functional.py:2140\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2138\u001b[39m     dim = _get_softmax_dim(\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m.dim(), _stacklevel)\n\u001b[32m   2139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2140\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2142\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.softmax(dim, dtype=dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from src.fase2.script_2_transformer_training import main as train_model\n",
    "import torch\n",
    "\n",
    "# Restaurar datasets y reconstruir DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Detectar dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"🔄 Restaurando datasets...\")\n",
    "train_dataset = torch.load(\"data/train/train_dataset.pt\", weights_only=False)\n",
    "val_dataset = torch.load(\"data/train/val_dataset.pt\", weights_only=False)\n",
    "\n",
    "# Cargar datasets completos\n",
    "print(\"🔄 Cargando datasets completos...\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    \n",
    "# Calcular número de clases\n",
    "num_classes = len(set([y.item() for _, y, _ in train_loader.dataset]))\n",
    "\n",
    "# Ejecutar entrenamiento con early stopping y gráfica\n",
    "print(\"🚀 Entrenando modelo...\")\n",
    "model = train_model(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    epochs=50,\n",
    "    lr=1e-5,\n",
    "    freeze_encoder=True,# transfer learning clásico\n",
    "    patience=5,         # early stopping\n",
    "    debug=False         # True para depuración\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
