{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f6f4e81f",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "_Máster Universitario en Inteligencia Artificial_\n",
    "\n",
    "_Trabajo Fin de Máster_\n",
    "\n",
    "- Gustavo Mateos Santos\n",
    "- gustavo.mateos830@comunidadunir.net\n",
    "\n",
    "# Clasificación automática de estrellas variables con modelos Transformer aplicados a series temporales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62128cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting lightkurve\n",
      "  Downloading lightkurve-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Collecting astropy>=5.0 (from lightkurve)\n",
      "  Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting astroquery>=0.3.10 (from lightkurve)\n",
      "  Downloading astroquery-0.4.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.13.4)\n",
      "Requirement already satisfied: bokeh>=2.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.7.2)\n",
      "Collecting fbpca>=1.0 (from lightkurve)\n",
      "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.10.1)\n",
      "Collecting memoization>=0.3.1 (from lightkurve)\n",
      "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.26.4)\n",
      "Collecting oktopus>=0.1.2 (from lightkurve)\n",
      "  Downloading oktopus-0.1.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.32.3)\n",
      "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.67.1)\n",
      "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
      "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.23 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.4.0)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy>=5.0->lightkurve)\n",
      "  Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting astropy-iers-data>=0.2024.10.28.0.34.7 (from astropy>=5.0->lightkurve)\n",
      "  Downloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (6.0.2)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (24.2)\n",
      "Collecting html5lib>=0.999 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting keyring>=15.0 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading pyvo-1.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.5)\n",
      "Requirement already satisfied: contourpy>=1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.3.2)\n",
      "Requirement already satisfied: narwhals>=1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.35.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (11.2.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (2025.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
      "Collecting autograd (from oktopus>=0.1.2->lightkurve)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (2025.1.31)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.0.1)\n",
      "Collecting multidict<7.0.0,>=6.0.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
      "Collecting SecretStorage>=3.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading SecretStorage-3.3.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jeepney>=0.4.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jeepney-0.9.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: importlib_metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.11.0)\n",
      "Collecting jaraco.classes (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib_metadata>=4.11.4->keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.21.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (44.0.2)\n",
      "Collecting more-itertools (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.22)\n",
      "Downloading lightkurve-2.5.0-py3-none-any.whl (270 kB)\n",
      "Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astroquery-0.4.10-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
      "Downloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
      "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.7/738.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyvo-1.6.2-py3-none-any.whl (999 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.4/999.4 kB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading jeepney-0.9.0-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Building wheels for collected packages: fbpca, memoization, oktopus\n",
      "  Building wheel for fbpca (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11428 sha256=7a1c0c198fcd5b6d325211ca045f7e490a1f57d14f4fd405ac819d3c8488574e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3c/ea/60/8d1c9fbbc99492a1775b36a5e29c8c1ef309cc5821bd5a219d\n",
      "  Building wheel for memoization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50537 sha256=db994a6d6c135ea1fa23778fd6df8bb1ae62f672728f823114592e7ad00d4162\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3e/b8/c5/b553d5e8b0249bd2859b3b6d7bb2a1849e7b01c6e8b64f6e87\n",
      "  Building wheel for oktopus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for oktopus: filename=oktopus-0.1.2-py3-none-any.whl size=12817 sha256=4397a316aadeda57340417b4534cf34779c5dd2f8c9e0519ca0c4b36debe9cbb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b6/55/53/8d7c151deab7a11f182e5a69feb906cab95123da7a2438ed9d\n",
      "Successfully built fbpca memoization oktopus\n",
      "Installing collected packages: fbpca, wrapt, uncertainties, pyerfa, propcache, multidict, more-itertools, memoization, jeepney, html5lib, frozenlist, backports.tarfile, autograd, async-timeout, astropy-iers-data, aioitertools, aiohappyeyeballs, yarl, oktopus, jaraco.functools, jaraco.context, jaraco.classes, botocore, astropy, aiosignal, SecretStorage, pyvo, aiohttp, keyring, aiobotocore, s3fs, astroquery, lightkurve\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.38.1\n",
      "    Uninstalling botocore-1.38.1:\n",
      "      Successfully uninstalled botocore-1.38.1\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 0.4.2\n",
      "    Uninstalling s3fs-0.4.2:\n",
      "      Successfully uninstalled s3fs-0.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.40.0 requires botocore==1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "boto3 1.38.1 requires botocore<1.39.0,>=1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "s3transfer 0.12.0 requires botocore<2.0a.0,>=1.37.4, but you have botocore 1.37.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SecretStorage-3.3.3 aiobotocore-2.22.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aioitertools-0.12.0 aiosignal-1.3.2 astropy-6.1.7 astropy-iers-data-0.2025.5.12.0.38.29 astroquery-0.4.10 async-timeout-5.0.1 autograd-1.8.0 backports.tarfile-1.2.0 botocore-1.37.3 fbpca-1.0 frozenlist-1.6.0 html5lib-1.1 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jeepney-0.9.0 keyring-25.6.0 lightkurve-2.5.0 memoization-0.4.0 more-itertools-10.7.0 multidict-6.4.3 oktopus-0.1.2 propcache-0.3.1 pyerfa-2.0.1.5 pyvo-1.6.2 s3fs-2025.3.2 uncertainties-3.2.3 wrapt-1.17.2 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages\n",
    "%pip install torch lightkurve\n",
    "%pip install -q pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455e0895-292c-4535-9e86-1e21e24e186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.2\n",
      "Lightkurve: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch, lightkurve as lk\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Lightkurve:\", lk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb5af2",
   "metadata": {},
   "source": [
    "## **Fase 2: Diseño e Implementación del Modelo Transformer**\n",
    "\n",
    "Tras haber finalizado la **Fase 1 (Recopilación y preparación de datos)**, donde hemos generado un conjunto consolidado y etiquetado a partir de diversas misiones espaciales (Kepler, K2, TESS), el siguiente paso definido claramente en la memoria es la implementación de la **Fase 2**:\n",
    "\n",
    "Aquí se menciona específicamente:\n",
    "\n",
    "* **Arquitectura Transformer especializada**, adaptada a series temporales astronómicas.\n",
    "* Implementación técnica con PyTorch en entorno local (VSCode).\n",
    "* Exploración e incorporación de técnicas avanzadas:\n",
    "  * Codificación posicional rotatoria (*Rotary Positional Encoding*).\n",
    "  * Mecanismos de atención jerárquica (*Multi-head Self-Attention*).\n",
    "  * Técnicas de *transfer learning* usando modelos preentrenados como **ASTROMER** para mejorar la generalización en casos de datos limitados o escasos.\n",
    "\n",
    "### ⚙️ Pasos concretos ajustados al estado actual del trabajo:\n",
    "\n",
    "1. **Preparación final del dataset consolidado** (*ya realizada con éxito*):\n",
    "   * `all_missions_labeled.parquet` generado, validado y listo.\n",
    "\n",
    "2. **Preprocesamiento para Modelo Transformer** (*inmediato*):\n",
    "   * Normalización individual por curva (por ejemplo, estándar Z-Score o min-max).\n",
    "   * Segmentación o padding de las curvas para homogeneizar longitud.\n",
    "   * Opcional: generación de embeddings iniciales (si decides usar ASTROMER).\n",
    "\n",
    "3. **Diseño Arquitectónico Inicial del Modelo**:\n",
    "   * Entrada: secuencia temporal de magnitudes (+ potencialmente metadatos adicionales).\n",
    "   * Embedding lineal inicial.\n",
    "   * Codificación posicional rotatoria.\n",
    "   * Bloques de atención jerárquica multi-cabezal.\n",
    "   * Capas finales para clasificación multiclase usando salida \"softmax\".\n",
    "\n",
    "4. **Implementación técnica**:\n",
    "   * Desarrollo del modelo en PyTorch (en tu entorno local VSCode ya configurado).\n",
    "   * Uso de GPU en AWS SageMaker para entrenamientos largos o con muchos datos.\n",
    "\n",
    "5. **Entrenamiento y validación inicial del modelo**:\n",
    "   * División del dataset en entrenamiento, validación y prueba.\n",
    "   * Evaluación sistemática usando métricas definidas (Accuracy, Recall, F1-score).\n",
    "\n",
    "El paso más lógico y urgente, dado el avance actual, es comenzar inmediatamente la implementación técnica del modelo Transformer descrito anteriormente. De hecho, sería idóneo empezar con un script base (`script_8_train_transformer.py`) en PyTorch que:\n",
    "\n",
    "* Cargue datos desde el fichero final consolidado (`all_missions_labeled.parquet`).\n",
    "* Realice la normalización y segmentación (preprocesamiento).\n",
    "* Construya la arquitectura Transformer básica propuesta.\n",
    "* Ejecute entrenamiento inicial y validación para comprobar su funcionamiento.\n",
    "\n",
    "#### 🚨 Consideración importante sobre transfer learning:\n",
    "\n",
    "En la memoria se destaca claramente la relevancia del uso de modelos preentrenados, particularmente **ASTROMER**, para aprovechar embeddings preaprendidos y mejorar la generalización del modelo. Se recomendaría evaluar claramente incorporar este paso desde ahora, ya que podría acelerar la convergencia del entrenamiento y mejorar resultados iniciales.\n",
    "\n",
    "#### 📋 Plan posterior (según tu TFM):\n",
    "\n",
    "Tras la implementación y validación básica del Transformer, seguirían:\n",
    "\n",
    "* **Fase 3**: Evaluación experimental comparativa contra CNN y LSTM.\n",
    "* **Fase 4**: Evaluación de robustez ante incertidumbres (curvas incompletas, ruido, muestreo irregular).\n",
    "* **Fase 5**: Análisis crítico, síntesis y propuestas futuras de mejora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c69b60",
   "metadata": {},
   "source": [
    "\n",
    "#### 📚 **1. Citación del uso de ASTROMER en la Memoria (Capítulo 6)**\n",
    "\n",
    "Correcto. El uso de ASTROMER debe ser claramente citado en la memoria del TFM. En el capítulo 6, donde describas el diseño e implementación del modelo, deberías incluir explícitamente algo similar a:\n",
    "\n",
    "> Para aprovechar las ventajas del aprendizaje por transferencia en series temporales astronómicas, se ha utilizado el modelo preentrenado **ASTROMER** desarrollado por Fang et al. (2022), disponible públicamente en el repositorio de GitHub ([https://github.com/IShengFang/ASTROMER](https://github.com/IShengFang/ASTROMER)).\n",
    "\n",
    "Luego, en la bibliografía añadirás la referencia completa, por ejemplo en formato APA:\n",
    "\n",
    "> Fang, I. S., Teyssier, D., & Longo, G. (2022). **ASTROMER**: A Transformer-based Embedding for Astronomical Time Series. Recuperado de [https://github.com/IShengFang/ASTROMER](https://github.com/IShengFang/ASTROMER)\n",
    "\n",
    "Esto garantiza la transparencia y buenas prácticas en tu trabajo académico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     26437.000000\n",
      "mean       9514.005523\n",
      "std        7929.330408\n",
      "min           3.000000\n",
      "50%        8242.000000\n",
      "75%       13771.000000\n",
      "90%       17275.000000\n",
      "95%       19273.400000\n",
      "99%       36005.720000\n",
      "max      131072.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración\n",
    "DATASET_PATHS = [\n",
    "    \"data/processed/all_missions_labeled.parquet\",\n",
    "    \"data/processed/dataset_gaia_complemented_normalized.parquet\"\n",
    "]\n",
    "dataset = ds.dataset(DATASET_PATHS, format=\"parquet\")\n",
    "scanner = dataset.scanner(columns=[\"id_objeto\"])\n",
    "\n",
    "lengths = []\n",
    "for batch in scanner.to_batches():\n",
    "    df_batch = batch.to_pandas()\n",
    "    lengths.append(df_batch.groupby('id_objeto').size())\n",
    "\n",
    "all_lengths = pd.concat(lengths)\n",
    "print(all_lengths.describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cf0ba",
   "metadata": {},
   "source": [
    "\n",
    "📌 **Adaptaciones necesarias para la memoria del TFM (capítulo 6):**\n",
    "\n",
    "En la próxima redacción de tu capítulo 6 debes mencionar explícitamente:\n",
    "\n",
    "-   **Cambio de ASTROMER a AstroConformer** (motivado por disponibilidad).\n",
    "-   **Justificación técnica** del uso de máscaras de atención (gestión eficiente de curvas de longitud variable).\n",
    "-   **Detalles técnicos de implementación** de AstroConformer y la gestión eficiente de memoria usando PyArrow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c33b",
   "metadata": {},
   "source": [
    "#### 🎓 Objetivo del TFM:\n",
    "\n",
    "**Entrenar tu propio Transformer sobre curvas de luz completas**, usando **transfer learning con AstroConformer**.\n",
    "\n",
    "#### ¿Qué significa entonces aplicar *transfer learning* desde AstroConformer?\n",
    "\n",
    "1. **Usar AstroConformer preentrenado como bloque de codificación de secuencias**:\n",
    "\n",
    "   * No lo usas solo como extractor de un vector resumen por curva.\n",
    "   * Lo usas **para procesar secuencias completas**: pasa la curva entera, y extrae **embeddings intermedios** (por ejemplo, las salidas de cada paso de tiempo).\n",
    "\n",
    "2. **Congelar o afinar (fine-tune) sus pesos** durante el entrenamiento de tu modelo.\n",
    "\n",
    "   * Si congelas: aprovechas lo aprendido sin tocarlo.\n",
    "   * Si haces fine-tuning: permites que se adapte a tu nuevo conjunto de clases.\n",
    "\n",
    "3. **Añadir una cabeza de clasificación propia** sobre el output del encoder de AstroConformer.\n",
    "\n",
    "   * Es decir, el modelo final sería algo como:\n",
    "\n",
    "     ```\n",
    "     curva de luz --> AstroConformer --> [CLS] embedding o secuencia completa --> capa lineal --> clases\n",
    "     ```\n",
    "\n",
    "La **opción correcta** en lugar de usar *nuestro propio Transformer desde cero*, el **bloque encoder será AstroConformer**, cargado con pesos preentrenados.\n",
    "\n",
    "En `script_2_transformer_training.py`:\n",
    "\n",
    "* Cargas AstroConformer como `encoder`.\n",
    "* Le pasas `x` y `mask` como entrada.\n",
    "* Añades tu cabeza de clasificación (`nn.Linear(...)`).\n",
    "* Entrenas usando `train_loader` y `val_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a289da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n",
      "🔍 Limitando procesamiento a los primeros 200 objetos\n",
      "🚀 Procesando 200 curvas en paralelo usando 8 CPUs...\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing import main as transformer_preprocessing\n",
    "\n",
    "# Ejecutar una prueba limitada a 200 objetos\n",
    "train_loader, val_loader = transformer_preprocessing(\n",
    "    seq_length=20000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    limit_objects=200,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8c7e4",
   "metadata": {},
   "source": [
    "**Validación del primer batch de entrenamiento**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d3527d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extraer el primer batch del loader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[32m      3\u001b[39m     x, y, mask = batch\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔍 Batch de entrenamiento:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Extraer el primer batch del loader\n",
    "for batch in train_loader:\n",
    "    x, y, mask = batch\n",
    "    print(\"🔍 Batch de entrenamiento:\")\n",
    "    print(f\"- x.shape       : {x.shape}\")\n",
    "    print(f\"- y.shape       : {y.shape}\")\n",
    "    print(f\"- mask.shape    : {mask.shape}\")\n",
    "    print(f\"- x[0, :10]     : {x[0, :10]}\")  # primeros 10 pasos de la curva 0\n",
    "    print(f\"- mask[0, :20]  : {mask[0, :20]}\")  # primeros 20 elementos de la máscara\n",
    "    print(f\"- y[0] (clase)  : {y[0]}\")\n",
    "    break  # solo inspeccionar el primer batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeaeba",
   "metadata": {},
   "source": [
    "| Variable    | Forma esperada             | Resultado     | ✅ Estado |\n",
    "| ----------- | -------------------------- | ------------- | -------- |\n",
    "| `x`         | `(batch_size, seq_length)` | `(64, 20000)` | ✅ Ok     |\n",
    "| `mask`      | `(batch_size, seq_length)` | `(64, 20000)` | ✅ Ok     |\n",
    "| `y`         | `(batch_size,)`            | `(64,)`       | ✅ Ok     |\n",
    "| `x[0][:10]` | valores normalizados       | valores ∈ ℝ   | ✅ Ok     |\n",
    "| `mask[0]`   | 1s en datos reales         | todo `1.0`    | ✅ Ok     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ffdec",
   "metadata": {},
   "source": [
    "**Comprobacion de número de objetos únicos entre los dos datasets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7bba571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Objetos únicos detectados: 15080\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Configuración\n",
    "DATASET_PATHS = [\n",
    "    \"data/processed/all_missions_labeled.parquet\",\n",
    "    \"data/processed/dataset_gaia_complemented_normalized.parquet\"\n",
    "]\n",
    "# Dataset streaming por lotes\n",
    "dataset = ds.dataset(DATASET_PATHS, format=\"parquet\")\n",
    "\n",
    "scanner = dataset.scanner(columns=[\"id_objeto\"])\n",
    "unique_ids = set()\n",
    "\n",
    "for batch in scanner.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    unique_ids.update(df[\"id_objeto\"].dropna().unique())\n",
    "\n",
    "print(f\"🔎 Objetos únicos detectados: {len(unique_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6505e",
   "metadata": {},
   "source": [
    "**Ejecución para todos los objetos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98875370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agrupando curvas por objeto: 1127351batch [42:05, 446.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Procesando 15080 curvas en paralelo usando 8 CPUs...\n",
      "📊 Recuento por clase codificada:\n",
      " 4 (Other): 4417\n",
      " 2 (Eclipsing Binary): 6317\n",
      " 6 (Rotational): 2117\n",
      " 1 (Delta Scuti): 197\n",
      " 3 (Irregular): 105\n",
      " 7 (Variable): 113\n",
      " 5 (RR Lyrae): 32\n",
      " 8 (White Dwarf): 7\n",
      " 9 (Young Stellar Object): 16\n",
      " 0 (Cataclysmic): 9\n",
      "\n",
      "📉 Resumen de curvas descartadas:\n",
      "🔸 All nan                       : 0\n",
      "🔸 Low std                       : 0\n",
      "🔸 Short curve                   : 0\n",
      "🔸 Nan or inf after norm         : 0\n",
      "🔸 Ok                            : 0\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing_optimizado_2 import main as transformer_preprocessing_optimizado2\n",
    "\n",
    "# Ejecutar una prueba con todos los objetos y SEQ_LENGTH=25000\n",
    "train_loader, val_loader = transformer_preprocessing_optimizado2(\n",
    "    seq_length=25000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    device=\"cpu\",\n",
    "    limit_objects=None,\n",
    "    max_per_class=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardado de los datasets de la celda anterior, serializados para poder recuperarlos si se reinicia el Kernel**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2452f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Guardar datasets serializados para no perderlos al reiniciar el kernel\n",
    "torch.save(train_loader.dataset, \"data/train/train_dataset.pt\")\n",
    "torch.save(val_loader.dataset, \"data/train/val_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0169d",
   "metadata": {},
   "source": [
    "🧠 `script_2_transformer_training.py`:\n",
    "\n",
    "#### ✅ ¿Qué hace este script?\n",
    "\n",
    "1. Carga **AstroConformer** como encoder con la configuración oficial (`default_config.yaml`).\n",
    "2. Añade una **capa lineal de clasificación**.\n",
    "3. Utiliza los **DataLoaders preparados** (`train_loader`, `val_loader`).\n",
    "4. Soporta entrenamiento en **CPU o GPU (`device`)**.\n",
    "5. Muestra **métricas detalladas** por clase en validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Clases codificadas presentes en train_loader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "🔢 Número de clases distintas detectadas: 10\n",
      "🎯 Clases codificadas presentes en val_loader: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "🔢 Número de clases distintas detectadas: 9\n"
     ]
    }
   ],
   "source": [
    "# Mostrar clases codificadas presentes en el dataset\n",
    "clase_ids_unicos = sorted(set([y.item() for _, y, _ in train_loader.dataset]))\n",
    "print(f\"🎯 Clases codificadas presentes en train_loader: {clase_ids_unicos}\")\n",
    "print(f\"🔢 Número de clases distintas detectadas: {len(clase_ids_unicos)}\")\n",
    "\n",
    "clase_ids_unicos = sorted(set([y.item() for _, y, _ in val_loader.dataset]))\n",
    "print(f\"🎯 Clases codificadas presentes en val_loader: {clase_ids_unicos}\")\n",
    "print(f\"🔢 Número de clases distintas detectadas: {len(clase_ids_unicos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a62cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comprobando valores infinitos en el dataset de entrenamiento...\n",
      "🔍 Comprobando valores infinitos en el dataset de validación...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = torch.load(\"data/train/train_dataset.pt\", weights_only=False)\n",
    "val_dataset = torch.load(\"data/train/val_dataset.pt\", weights_only=False)\n",
    "\n",
    "# Comprobar si hay valores infinitos en el dataset de entrenamiento\n",
    "# y mostrar estadísticas de los tensores\n",
    "print(\"🔍 Comprobando valores infinitos en el dataset de entrenamiento...\")\n",
    "for i in range(len(train_dataset)):\n",
    "    x, y, mask = train_dataset[i]\n",
    "    if torch.isinf(x).any():\n",
    "        print(f\"⚠️ Inf detectado en muestra {i} (clase {y})\")\n",
    "        print(f\"x.mean: {x.mean()}, std: {x.std()}, max: {x.max()}, min: {x.min()}\")\n",
    "\n",
    "# Comprobar si hay valores infinitos en el dataset de validación\n",
    "print(\"🔍 Comprobando valores infinitos en el dataset de validación...\")\n",
    "for i in range(len(val_dataset)):\n",
    "    x, y, mask = val_dataset[i]\n",
    "    if torch.isinf(x).any():\n",
    "        print(f\"⚠️ Inf detectado en muestra {i} (clase {y})\")\n",
    "        print(f\"x.mean: {x.mean()}, std: {x.std()}, max: {x.max()}, min: {x.min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325d40b",
   "metadata": {},
   "source": [
    "#### ✅ SITUACIÓN ACTUAL (consolidada)\n",
    "\n",
    "🧱 Dataset unificado: `all_missions_labeled.parquet`\n",
    "\n",
    "Generado con el script `script_6_unify_all_missions.py`, a partir de:\n",
    "\n",
    "| Dataset                            | Incluido | Motivo                                       |\n",
    "| ---------------------------------- | -------- | -------------------------------------------- |\n",
    "| `dataset_eb_kepler_labeled_fixed`  | ✅        | Curvas reales Kepler + etiquetas EB          |\n",
    "| `dataset_eb_tess_labeled_fixed`    | ✅        | Curvas reales TESS + etiquetas EB            |\n",
    "| `dataset_k2varcat_labeled_fixed`   | ✅        | Curvas reales K2 + clases variadas           |\n",
    "| `dataset_vsx_tess_labeled_fixed`   | ✅        | Curvas reales TESS cruzadas con VSX          |\n",
    "| `dataset_gaia_dr3_vsx_tic_labeled` | ❌        | ⚠️ No contiene curvas (`tiempo`, `magnitud`) |\n",
    "\n",
    "🔍 **Resultado:**\n",
    "📊 `all_missions_labeled.parquet` incluye **todas las curvas reales necesarias**, y tiene la columna `clase_variable_normalizada`.\n",
    "\n",
    "✅ El CSV de resumen muestra una buena base para RR Lyrae (362.000), WD (508.000), DSCT (2.7M), etc.\n",
    "\n",
    "🔄 ¿Es necesario volver a ampliar o reetiquetar?\n",
    "\n",
    "**No.** Dado que:\n",
    "\n",
    "* Ya hicimos el paso de normalización de clases en el script 6\n",
    "* El CSV muestra que sí tienes miles de curvas etiquetadas para las clases minoritarias que antes faltaban\n",
    "* El `.parquet` final unificado los contiene\n",
    "\n",
    "❗ ¿Entonces por qué no aparecen esas clases?\n",
    "\n",
    "Porque el problema está en la **fase de preprocesado**, donde solo se incluyeron:\n",
    "\n",
    "```text\n",
    " 7033 → Eclipsing Binary  \n",
    " 4970 → Other  \n",
    " 1139 → Rotational  \n",
    "   98 → Delta Scuti  \n",
    "   19 → RR Lyrae  \n",
    "    4 → White Dwarf  \n",
    "```\n",
    "Es decir, aunque el `.parquet` contiene decenas de miles de ejemplos de RR Lyrae, WD, DSCT, etc., **no se incluyeron durante la generación de secuencias** (`script_1_transformer_preprocessing.py`).\n",
    "\n",
    "✅ Próximo paso: regenerar dataset con clases balanceadas\n",
    "\n",
    "1. Adaptar `script_1_transformer_preprocessing.py` -> `script_1_transformer_preprocessing_optimizado.py`\n",
    "2. Añadir un filtro que **igualice o limite el número de curvas por clase**\n",
    "   * Por ejemplo: `max_per_class = 100.000`\n",
    "3. Tomar **una muestra estratificada por clase\\_variable\\_normalizada**\n",
    "4. Proceder como antes, y guardar el nuevo `.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28434aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No se ha aplicado balanceo por clase (max_per_class=None). Algunas clases pueden estar sobrerrepresentadas.\n",
      "🚀 Procesando 13415 curvas en paralelo usando 4 CPUs...\n",
      "📊 Recuento por clase codificada:\n",
      " 2 (Eclipsing Binary): 7020\n",
      " 4 (Other): 4969\n",
      " 6 (Rotational): 1143\n",
      " 1 (Delta Scuti): 100\n",
      " 7 (Variable): 44\n",
      " 3 (Irregular): 51\n",
      " 9 (Young Stellar Object): 11\n",
      " 5 (RR Lyrae): 19\n",
      " 0 (Cataclysmic): 3\n",
      " 8 (White Dwarf): 4\n",
      "\n",
      "📉 Resumen de curvas descartadas:\n",
      "🔸 All nan                       : 0\n",
      "🔸 Low std                       : 0\n",
      "🔸 Short curve                   : 0\n",
      "🔸 Nan or inf after norm         : 0\n",
      "🔸 Ok                            : 0\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing_optimizado_2 import main as transformer_preprocessing_optimizado2\n",
    "\n",
    "# Ejecutar una prueba con todos los objetos y SEQ_LENGTH=25000\n",
    "train_loader, val_loader = transformer_preprocessing_optimizado2(\n",
    "    seq_length=25000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    device=\"cpu\",\n",
    "    limit_objects=None,\n",
    "    max_per_class=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Guardar datasets serializados para no perderlos al reiniciar el kernel\n",
    "torch.save(train_loader.dataset, \"data/train/train_dataset.pt\")\n",
    "torch.save(val_loader.dataset, \"data/train/val_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95225302",
   "metadata": {},
   "source": [
    "**Refuerzo del fine-tuning (en SageMaker) a partir de los errores detectados en la primera ronda**:\n",
    "\n",
    "Estos errores estan en `data\\outputs\\errores_mal_clasificados_con_id.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9c56d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Leyendo errores desde outputs\\errores_mal_clasificados_con_id.csv\n",
      "🔍 IDs a extraer: 1,226\n",
      "📦 Cargando datasets originales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agrupando curvas por objeto: 1127351batch [44:18, 424.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Extrayendo solo curvas asociadas a errores...\n",
      "✅ Dataset de refuerzo guardado en: data\\processed\\dataset_refuerzo_desde_errores.parquet (160,275 filas)\n"
     ]
    }
   ],
   "source": [
    "%run src/fase1/script_6d_refuerzo_desde_errores.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8e237",
   "metadata": {},
   "source": [
    "#### El siguiente bloque (entrenamiento y fine-tuning) lo hacemos en el notebook de la fase 2 para SageMaker, no aqui en local, para aprovechar procesamiento GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc45a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 IDs únicos en errores: 1226\n",
      "📦 IDs únicos en refuerzo: 1226\n",
      "✅ IDs comunes entre ambos: 1226\n",
      "\n",
      "Ejemplos:\n",
      "['KIC_10004546', 'KIC_10006096', 'KIC_10007533', 'KIC_10014536', 'KIC_10015516', 'KIC_10019399', 'KIC_10019763', 'KIC_10024144', 'KIC_10026457', 'KIC_10028352']\n",
      "📈 IDs candidatos a agregar (errores no presentes en train): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar CSV con errores y dataset de refuerzo\n",
    "errores = pd.read_csv(\"outputs/errores_mal_clasificados_con_id.csv\")\n",
    "df_refuerzo = pd.read_parquet(\"data/processed/dataset_refuerzo_desde_errores.parquet\")\n",
    "\n",
    "# Normalizar id_objeto\n",
    "errores[\"id_objeto\"] = errores[\"id_objeto\"].astype(str)\n",
    "df_refuerzo[\"id_objeto\"] = df_refuerzo[\"id_objeto\"].astype(str)\n",
    "\n",
    "# Comparar\n",
    "ids_errores = set(errores[\"id_objeto\"])\n",
    "ids_refuerzo = set(df_refuerzo[\"id_objeto\"])\n",
    "ids_comunes = ids_errores.intersection(ids_refuerzo)\n",
    "\n",
    "print(f\"🔎 IDs únicos en errores: {len(ids_errores)}\")\n",
    "print(f\"📦 IDs únicos en refuerzo: {len(ids_refuerzo)}\")\n",
    "print(f\"✅ IDs comunes entre ambos: {len(ids_comunes)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "print(\"\\nEjemplos:\")\n",
    "print(sorted(list(ids_comunes))[:10])\n",
    "\n",
    "df_debug = pd.read_csv(\"data/train/debug_clases_codificadas.csv\")\n",
    "ids_train = set(df_debug[\"id_objeto\"].astype(str))\n",
    "\n",
    "ids_finales_para_agregar = ids_comunes - ids_train\n",
    "print(f\"📈 IDs candidatos a agregar (errores no presentes en train): {len(ids_finales_para_agregar)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d98e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ Cargando datasets existentes...\n",
      "🔍 Cargando dataset de refuerzo...\n",
      "📂 Cargando IDs de errores conocidos y entrenamiento...\n",
      "📊 Total errores: 1226, en train: 0, candidatos: 1226\n",
      "🔹 1226 curvas candidatas desde refuerzo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1226/1226 [00:01<00:00, 1063.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 0 nuevas curvas se agregarán al entrenamiento\n",
      "⚠️ No se han añadido nuevas curvas. Se mantienen los datasets originales.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run src/fase1/script_6e_combina_refuerzo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f06b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset original cargado (4.2GB). Longitud: 10664\n",
      "✅ Dataset nuevo cargado (2.3GB). Longitud: 11731\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset = torch.load(\"data/train/train_dataset_orig.pt\", weights_only=False)\n",
    "print(f\"✅ Dataset original cargado (4.2GB). Longitud: {len(dataset)}\")\n",
    "\n",
    "dataset = torch.load(\"data/train/train_dataset.pt\", weights_only=False)\n",
    "print(f\"✅ Dataset nuevo cargado (2.3GB). Longitud: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedcf0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curva 0: shape=torch.Size([25000]), dtype=torch.float32, label=2, mask_mean=0.002\n",
      "Curva 1: shape=torch.Size([25000]), dtype=torch.float32, label=2, mask_mean=0.007\n",
      "Curva 2: shape=torch.Size([25000]), dtype=torch.float32, label=2, mask_mean=0.009\n",
      "Curva 3: shape=torch.Size([25000]), dtype=torch.float32, label=6, mask_mean=0.002\n",
      "Curva 4: shape=torch.Size([25000]), dtype=torch.float32, label=4, mask_mean=0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_1_transformer_preprocessing_optimizado.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.sequences[idx], dtype=torch.float32),\n",
      "c:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_1_transformer_preprocessing_optimizado.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.labels[idx], dtype=torch.long),\n",
      "c:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\fase2\\script_1_transformer_preprocessing_optimizado.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.masks[idx], dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset = torch.load(\"data/train/train_dataset.pt\", weights_only=False)\n",
    "\n",
    "# Inspecciona las 5 primeras curvas\n",
    "for i in range(5):\n",
    "    x, y, m = dataset[i]\n",
    "    print(f\"Curva {i}: shape={x.shape}, dtype={x.dtype}, label={y}, mask_mean={m.float().mean().item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512fa25",
   "metadata": {},
   "source": [
    "### **NUEVO PREPROCESADO CON BALANCEADO DE CLASES**:\n",
    "\n",
    "`script_1_transformer_preprocessing_optimizado_balanceado.py`\n",
    "\n",
    "✅ MEJORAS INCLUIDAS\n",
    "\n",
    "1. Activación clara de balanceo por clase:\n",
    "    - Usando --max_per_class 1000 puedes limitar clases dominantes.\n",
    "    - Las clases minoritarias quedan incluidas hasta su máximo natural.\n",
    "\n",
    "2. Reporte claro de clases limitadas y no limitadas:\n",
    "    - Se imprime cuántas curvas por clase se han incluido y si fueron truncadas.\n",
    "\n",
    "3. Mensajes explícitos para trazabilidad de la selección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffef630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos en lotes con PyArrow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agrupando curvas por objeto: 1127351batch [31:04, 604.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Procesando 15080 curvas en paralelo usando 8 CPUs...\n",
      "📊 Recuento por clase codificada:\n",
      " 4 (Other): 4417\n",
      " 2 (Eclipsing Binary): 6317\n",
      " 7 (Variable): 113\n",
      " 6 (Rotational): 2117\n",
      " 3 (Irregular): 105\n",
      " 1 (Delta Scuti): 197\n",
      " 9 (Young Stellar Object): 16\n",
      " 5 (RR Lyrae): 32\n",
      " 0 (Cataclysmic): 9\n",
      " 8 (White Dwarf): 7\n",
      "\n",
      "📉 Resumen de curvas descartadas:\n",
      "🔸 All nan                       : 0\n",
      "🔸 Low std                       : 0\n",
      "🔸 Short curve                   : 0\n",
      "🔸 Nan or inf after norm         : 0\n",
      "🔸 Ok                            : 0\n",
      "✅ Datos preparados como secuencias normalizadas y máscaras.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio src al path\n",
    "script_dir = os.path.abspath(\"src\")\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing_optimizado_2 import main as transformer_preprocessing_optimizado2\n",
    "\n",
    "# Ejecutar una prueba con todos los objetos y SEQ_LENGTH=25000\n",
    "train_loader, val_loader = transformer_preprocessing_optimizado2(\n",
    "    seq_length=25000,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    device=\"cpu\",\n",
    "    limit_objects=None,\n",
    "    max_per_class=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Guardar datasets serializados para no perderlos al reiniciar el kernel y poder subirlos a SageMaker\n",
    "torch.save(train_loader.dataset, \"data/train/train_dataset.pt\")\n",
    "torch.save(val_loader.dataset, \"data/train/val_dataset.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
