{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADBCAYAAAC+C2ljAAAgAElEQVR4nO2de3xcVdX3v+vMTC5tSu83KCB3KSJIK8qlTVJEQAEBBX0BHx9RKZK2XF55URSbguDDg0JJ0/qgKAiKPqDc5CJamrTFcrGIIhQQKJeChd7bNE0yM2ev9489k2QykzSZpk2TWd/P53wKk3P2XvvMnN9Z+7YWGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIZhGAWE9LUBhrGTKUodQer/Q6AFSPaZRcZugwmgMRCJAROBE4GjgT3xIqjANmAlsARYBLyb+twwDKPfMwK4HngPcHhxy3UkgH8AXwaifWKpYRhGLxIFfojv3nYmfB2P9cCZfWGsYRhGb3Ik3vPrrviljz8Be/SBvUYfE2z/FMPoNxwDjM/juiOBg3vZFqMfYAJoDCQOIr+JvaHA3r1si9EPMAE0BgoCDMnz2gheBI0CwwTQGCgIXsjyvdZmggsQE0BjILEj6/lcr1lh9BtMAI2BggOa8rw2BBp70Rajn2ACaAwk3s3zuibg/d40xOgfmAAaA4m/AlvyuG4l8K9etsUwDGOXUgbcS88WQceB/9sXxhqGYfQ2hwKP4CO+bE/8NgA3YEtgChaLBmMMREYCJ+OjwRyIF7gYXvRagHXA88DDwNOpz4wCxATQGMhEgcFAKV4AwYtdIz4sloXBMgzDMAzDMAzDMAzDMAzDMAzDMAzDMIwBQ77hgwyjKwTbZmn0Awp9HaDQt2vB+qr+3qhX8FvPxuCjKe+DTz+5Bz4waToNZSM+8dCbwKv4fbebe6H+fOnr79zYjRhoAhgBivEP5jD8g7gHMCr1/2XAoNS/JfiFsg5oANYArwH/BP5NfvHhglS5g9vVPyRV//AO9Ze2q38rsBZ4HXgBH9WkJ/VHUvUOwu96SB8jU/UOztHusF27XwdepC2VZFftGwd8HDgeOArYP9W+QXTt9SWBjcArwOPA/an/3tE4fFH8d16K/67TApxu+5B2R2nKxia8KK/Cf9+vAc3drG8QcGqq7N4UUqXtXqT/2+HvWzx1NOMXcDfj29CAD/6wDZ/m04S9hww0AbwAOB//QA7FP/gl+F0AMbpur+J/WCuB3wK34kWpJ3wOmIH3iobiBacY7w1F2X63sBl4G7+hfwGwuht1HgT8P+AAfLv3oE3o0vV2p91vAf+Lb3fH0FBF+MRBnwdOSdVZ0g3buqrzbeAXwE+BD/Is5zTgXPz9Hkl222NsX5TXAouBW/Db4rbHnvgscofR+4LTsbz0nmXX7gjxdjfjveuN+Bfmm8BLtAl6X3rZRh8xn56nRMx1hPiHs6c5Jq7spfodXoRHdKPOCrwX0Fvt/j3eywMvHkfhRer9Xqqj/ZHEi8kR3WhnLq7tRVv+BRzXjTr3xHvLvX0veut+rgWWAt8HJuFfwEaBUEvv/Zia8N5FT/h/vVh/HLiwG3WW03sCqHjxvQYvvlfgPbWd/eA+h39Ye8qcXrbjfnyvoSt2ZwHs+D2+D/wKOAETwpzYTF3nlABn0nc/nBi+S729B7K3EeArwN3AD/CTGzubo/Bd0P12QV1dMQU4vI9t6C0EGAuchx9SmQdM7FOLdkNMALvmcGB0H9Z/KG3d0V3JPsBJ+HG0XcVxeI+zLz2VEeTfHd+dGQ58A/gdcDaWAa8VE8CuGUnfCuBQ/MRGoXAuMK0P6xfgQ31Y/87mUPwk1yVYlxgwAdwexez6LmjH+sv6sP5dzVDga/ilJn3FMAbe6oj2DAeqgSrMEzQB3A4BfXuPCjFhdwV92w3d3nKpgUAZcBV+jLmgMQHcvREG/sPYkZHAp7t57s64N90ps7frTS947uoI8bO7vcVI4Gp8yoCCZaB5F7/HL2Ruv7vA4cd1prPzu1Z1wOVk/lAdsBfwTXq+rrA7vAF8m+wJC4dfsvFN/ALh/sRxeC9l63bO+yN+IXBHYYgA/wF8tPdNoxG/tGRcu3oVv2rgDPKbtLob/9vtzCFJ9wTKgPH4hegfSf27I7+pI/BDDlfRu+Jq7GYcjd/61NO1VFuBqb1Q/0fwuzp6Wv82fHKffDkMv61vR9aTJfDZ01YCz+JF53f4h/Y3+Af3T8Df8FvLtu1gfZqqa0c8k6KUXfnU/XPy6xkNx+8kyafOK3tYVwQ/Ofdp4E52bB3oG8AhPW/uwGCgeYC7K33Vjc23Xgcsw28RW4EXpPfx26ta8KLoaIv6EsN7QCPxXslZ+G1z+aabHAVMwO9Rzof+NnTQU8EN8Ts+/gQsARYC1+HvWU/ZD7/k6dU8ru33mAAauYgDNwIP9eD8dNSXfwF/Bp4B/pv8RLAU3303tk8z3gtU/FbQnnaJBT/x9FO6HxBiwGCTIEZnJHfg2jhwB3BPntdH6dv1l/2R3wGP5nntRHxAiYLDBNDYWcTxD2Vjntf35VrA/kgTfi9zPI9rx+In6goOE0BjZ/IKfhImH2LbP8XowEv4YYiekp5dLjhMAI2dSQN+mYqxa9iUOnpKgJ/FLjhMAI2dSTqAp7FrSEeN7inpSOYFhwmgYQwcivAz6D0lHT+w4DABNIyBQzo1QE9R8p+s6teYABrGwOET5CeASfKbPOn3mAAaxsBgJD7YaT7P9Ga6l4BrwGECaBj9H8FnQ+xOUqdcrCb/5Ur9GtsKZxj9mwC/7zpXRKDu8gIF2gU2ATSM/ssofAKrK/C7OfIhCTyBD3BRcJgAGsbuQ1dDUoIPg1WGj/oyFTgHP/GxI8mrXgUW7cD1/RoTQMPoe87Cxz/sKoRXBB/pZS9gX7z3F9nBeh3wa+CdHSyn32ICaBh9z+TUsatZBvyyD+rdbbBZYMMoTN4FZlOgs79pTAANo/BYDXwLn8OmoDEBNIzC4mXgIuBeLBGSjQEaRoGwGXgQn+rgxT62ZbfBBLBr0kl/DKM/EgJrgHp83pDF+MjRRgoTwK5Jr7syjP6GA/4HuBV4jQJMeNQdCsW7yXesowi/5sow+hsBPkDCG5j4dUqheIAh+QV8FOAY4Gfkl2zGMLrD68Bb5F4IXQx8DBicR7mnA2fiFzsbOSgUAWwmfwH7NPAp8k85CDbbZnTNncCPyN0jKwZqgPPyKHcQcAk+efqqvK0bwBRKF3gLfhYsH0YD84Aq/Dak7m4/ar9v8ygKNOeC0S0S+MmJxhzHBnzC8/fzLHsy8HUK51nvEYXiAW7Gz4Ydmuf1+wM349dPPY1PP7gKn/UsiRe7YmAEXjBH4YUvfYwj/8kU8x4HPl3tAQZ4FvgVfvFyPmVfADySKsdoR6EIYCN+nKV8B8qIAR9JHYp/ayfxY4tB6oix4xvUOzIEGEr2d9XMwM7jUIpP1djRc4kDWymsF0MI3AacBhySx/UT8F3hbwDbetGufk+hCKAD/oZ/E27vbdsdBD9DvCNhiLpDMXA9PthleyLAPcAPd3L9fcl/ACd2+CyC3771bQpvUupVvAj+F/m9ZD8HPAT8b28a1d8pFAEEeAZYi8+c1V8IgIM6+dtTu9KQPmDP1NGRd+idl1h/5Nf40FnH5HHtYOBSYCkFHgChPYU0MPoK8Ne+NqIX6Q9dQKX37ewP7d5ZrAYWkP+6vqOBr1K4L5AsCkkAG/Huf0tfG1JAOGwRbm/zEPDnPK8N8OOAH+s9c/o3hSSA4GfCCj4E0C6kBfigr40YYGwBaoGNeV6/LzALP8lU8BSaAG7ATxzs6hDgW/E/3EIjgd+HavQu9cDvd+D6s4CTe8mWfk2hCSD4QeDLgbd3QV0K/AOYDvxiF9S3O/IkhSn+O5M4PtBBvrs7hgCX4denFjSFKIAK3IdfZvEk+e0R7g7vAzfh37Z3Az/HT8T0Fv1lIPsp4PG+NmIA8jw7ls/jWODLvWRLv6UQBRC8CC4BvgRch+8S98bsouKXGNwKnIFfr7Yy9bcX8W/df+5gXSHeA+jOglYhf6HsLYHdClTjB+6TO1COw3epGynsmeA0Dt+r+Gee10fwO5uO6DWL+iGFtA4wF+8Bc4Df4XOsngocTM8HiBuAFcBj+Fm6F8mdaPqP+DGxs/C7UvbH7/Iowb+MkqnrmvEC14gf7F4LrMcvg3g/9f8rumHX5pQ9w+mZaCRSdfUWK/Dexmn4xc2HpGwqxe+ecbQJe7rtW/AvkzWpf1fj2/0vtp/EW1NlpXfrdJdoN8ruipZUvWEPrhHyfzG8CfwUv00zn2d5f2BG6ijI1RH9pRu1KwiAscAk4DjgcPyM2TB8VI0i2h6sRrwgvYHvijwFvICfZOmu0KS3enUmgE2pf1tSR08eqjTpZNr5fM9Jdo6nVYRv8zA6F8Am2tqfyMOOAO/ZjOvhtYIfV3spjzpj+MADw/K49l+09RR6ymj8rqCKPK/fjH85/SHP6/s1JoC5EbwoDcM/rEPx29LAP5ibUsdmCm9LlrH78QXgDvKLGQiwCD8ctLbXLDIMw9hFDMZ7gZrnkcBvkzMMw+iXlOMXnecrgq8Ch+1yq/uY3g7dZBhG3/AesA9+v28+jMQP/fyZ/Mab+yU2BmgYA4eP4HP/7p/n9Rvx61XXklsbksAD+Mk/wzCM3QoBvof34PLtCnd1NOOXig0YCnUhtGEMRBS/O+T5vjakv2ACaBgDi1X4fcK2PKsbmAAaxsDj9/iIMcZ2MAE0jIHHRnwq13xTwRYMJoCGMTBZiF8cbXSBCaBhDEya8Rnk/tLXhuzOmAAaxsBlJVCFHw/cWXEv+zWFHg7L6A9Ua8AeTxWTcFHGJBK8XRGnWuyB7h7/AM4HvgKcCRwAlOGf/fRi5/Q6vxC/LziOj0DUhI/nuBUfAWkzvRsmrc8pnJ0gtXVfQqUSJI46AWkhGpnPxVO6F4Zofl0ZTi5GZTzIz5g1tTvx+FJ1L/ocGnymrW5AuIOZlctbz5lXdwbIKaj6eHQiggQ/x+koAncWKiHIFiLBz3LaXFt/JuhJaJBAnSAkiURrCJMjQL6KqiKBAlGU+4EyAvfptvPFgfyMmeVtATZrlowmcJfiGI5IiLgYsBzHW0jwedSFSKCoRhBp+y2pKrCWYp1HS2mcoPlSf99IggYogmgcZD3Ci8R0KdMr12W16dblg0g0TANOxQUHghYjNCG8B7Iccb+nqvJ9ahYdgUS+7tsRKOJiKH9kRsWDzF/yDXBHtbUziLTaGLAVZRXCcppaXuCKkxq3+10uWDqcMJyB6FCC6IKc38X8ujJUZqKyT2ubkVRvSx2wGdGVBJFnWTPyFaoP2xVLVgS/3e1AYO/Ufw9K/a0FH9OyAS90Dfh4jFvxXek4bfEV8wlPtttSOB6gowRhAmgFIhuBP5Fw3d8L7YKjQa9CdChoA/D9bl+rkRLQcam6W0AXIRLLPEdKEUYhMgVQlHokGUMiJTgZjjAJ9ADCcE9ueHIGVx7fkN0+GY3qVESaUOpJuAgRiaE6EpGpoEPxoY+K/SEHg05FZD3wRJZNLh4g0SEIx4AegQbPoroCXDEwGqQctAikDrQJVEAU4aNAGS3uToob1hEvGoLqJITJKMsRXkeDQYiWo3yLuDzJgqUzM8TkxscHE2+sRmQ6yruIexaRzSgjUD4J+kU0WAU8hEgMccNBpoKOQqUOJBW+zA1CZX/QckTeAV0OKogUoYwCvogymJLiRdQs/iGzyrteRBy6KcC3URmEC/+NT3uQydaYMDhZBnwM9GjgedBXvfgHJYgehcp/EjoYseY+ap+4kRkn7OwcNQqsSx1P7+S6+g2F4wECzK87ECf1CE8RKzuX6ZO7F/33nnsifDC2FtFK771IA05OYdbU7sdP+8lfxpBM1CO8j+jpVFVuzWFfGS54GDQk0M9RVbkVVeHeewM+GPMTfE7XZuAqZpTPRSTzTXzDk0MoTT6KsIqisq+0tu+mZaVE4/chHIyE01oftrlL9yESPgYMRiKfZsaUf2XZVF0XZYT8BOE0VE5pFYiap/dAWh4FHUEyOIFLp7zPnDlCdbWjpn4OwlcJdBpVla8DMK/+q8BPQS5iZvnPuXV5DNcynGRiBnAVyM3MLL+itd6aui8gchfwR5J6GZdWvN3a3prF/4HoAoQvM6Pifm/nS0WMWHsXwnEEWtFaL8AtiyYRBAsR7qSq/FLmIFAfUBorpSycgOr5KDOAt1H5SqciWP1SESPX3g581N8z3sKVnMGsT+ZO+uR7HXeiejmzKmup1gBWRBm6ZhBRDgS5GDgPZRGx8Ot884T3cpZj7DQKaxIkdP4BUpThK7s/hrR67P6gJ6L6W+B/UZ1IEH4iDwsERYkO6aJuFVTazhFRVqzwXVcfqfgN4Erm1Z+QdWnQmC43s33DW9JC6ZBo2+eXHL8K0ceAfcDljig8MtgboRJhKTS2dfubkm1llkRCRJTqal924P6Gci+JsJ2XmuoCSir8+/TJCb553BrCyG34DH0fZ35dWVtbZAoQRbiTyyrfyhR7eQXRX0HQ5jWNb1KC1O+5fRvbo6TsFEd1ZZIrj2+gqvxl1ulshDnAh0G/zY2P5w4sOnrNRGAqwu0If0CZRNByZM5zATTIbHO1OKoPi3NZ5SZmVi4n0FnAbQgnEUYvQrWwHJLdgMISwEjgkwRpD8cwRE8BBqHBgwiPAXFUTuOee7rfhW5pEgQBHI3xHo6hzAY0QFiJyDVADJFrubnuQxmnucEBQgTR7PIl9V2nXwLgxdXJg0ADqp/lpmXZuVDUlQPjcdzHrM+05Y0Y1hSARgAlEWbWV1X5EBvKr+DST2UnRXcd7n2g6dD37epUQSkFIqhmi9HMKX9lXcXFVE3J9NRUBNCMNrbRubhUVyaJF90BPIVwIsVFh+c8z8lpXswiD6PuUSAG2nlwAE3VqUHu77uqciuhqwHeQvXzLFgyodOyjJ1CYQmgRgNAsrqOXXFz3TCELwDPQOMKGqMvIvwN+BTrxu+bhxWunUeWydaYoASIZorkxHsFIYpKQFPzI6jeDEwmyvdyeisqISvObrt+Y7EAAUJIrIM4RPR5kKeAY4gmPpzxt5pHixE5DViJRBdnGyyC4gjirsPHmjVLK3ixlA4JgCR5FDAB0WdbhwVEFJHlgEPlCuYt+jy1C0f6LmS78tt/j6tLxU825ECiPi+KaOee9+XHbkB4AhhOIB/L+vvchWOBM1FdzJjVb1AkzwEvoZzMvCf3zFlmoP73Rhf1lu6xEngG2B8ND+70PGOnUDiTIBn0YAVFLDgWp4cS6ExmfsYnKKqtewjkv0m6CrqbzEaLAtQFKI6Gps4F2M+mdmKghjROSLDHv+cRj30U5HyKi/6O6nxElGFNAfFYyiud0+FSEXLpflXlVmrrH0Q5EXEn0T6SiJYehPAJ4NfMPP7fGddtiwpRVYSxaPQS5tVtS9UTEkQezDmeCAJuH2qWTCTiBuPko6CXAisgclvGmfHY7yiKfwI4D4K70OCfjFiyjNrFT+HcMmZWvJchgCPeFnSwf3kkc3qAnXtibea9DQqqe2X9KRqpxLE3QfA9zjknBNYxb/HDoN+B+DH4/bcdywtA8TPsnTB9coJ59W8CRbhIwScq39UUmAeYDIAApHsRb6vroqg7C6EI5HDmLb6EmvpZflYRED2V2+tKulVWNOhBjt4OY0ErRgsqbd3t6SduJpDZwL8Q+Q619eWAFyUA6SCgg4sE0QiKozmW/TCGshDhHVQ+y811w1o/D4ITUIrBPZR1TXFUQAQYinIyyOkgpyOcCi77QVYXAFGQSxH3EI4/gv4Y1cVoeH6WYF5+7AZcyWXAl1F+B4xAtArVu0EeoXbxf1L9UlHGNaK+C9zRyyVMeY45hgba49LeaZA5tHF7XQnI5xEE5eOtvwN0LyAGchrVdZ07E9sdctH0MhiL0L6LKSwBDII2YfETC10zSg8B+RSwDpUTQM9FOA/kaHxazGNpCA7tVt3eK9GM9XIdGdaUFgnN7iar0D5UeVX5yyDfA0pQuZaauvT4UUDnLq5SFGa3e/wHK1H+hHBka/dvfl0ZymnAcpoTf++kuAjwGi7yBbZFT0JLTmZb9FTWuWXZ50rg7df/Qd3XEFkAhIgMJVq0IWfxsz65hZkV91Jc9jUCmQbyBYRfInwImMuoNWe1njukVEAigCNZ1GGcUfz4q+uiKwqAG+5NZVPGx9vkcJTj8WlPT2n7HXAEygaQCobpAdnFpQQt6CLEvKpAMBZwWfUaO53CEsBQ0l6Y8xML20GDzwKDEHchSXcyiaJTSBSdQlH8JJQfAqMJ9ORu1S1xh+BQoiSH5b7vCYn6dXUks7rJ6QH+8e0+H/vBIyA3+W6qXEWJG9wmsO3at2V9Oj9wbtH3XboHgIAIp6TqOxzRj4A8kHOBsGsJUmWGFNHAlcc3MOuTW7jy+AaqK7MTfUvKfglWMGvaYmKDr0H0duBLhMmv5bQrzfTJCarKVzGz/CFiZRcB1cBgNPhClheYywMMu3jppKmuixLIJCAB4SuZbeVMvEB9JeN3kNSTEJkLTCAaTMtuczc8/p88OQyfg3oDEXlru+cbvUphCWAQtBPADsxdOJa5iw7h1uV+MXDtwpEgZwF/pSnxNJdVbuLyYzdw+bEbmH7iZpx7HHgPlc+yYOnw7dbtBiVQ4qB74BOCZ6MlxcBg0AbGbW0TkfFDBBEvNqsb2h7uc84JKYrXAg/4hzNyXmoGNWR2B7HTVLuDotxeUKLoGeAfKCczd+FYVD4NNEBy4fZaRksys66ap/egZslEbnhySNtZ2rYDA7yoaTAfeBnkImqWTGyzVYV5dR9mfv3HqXm0OKPs6ZMThO5RYA3oaEav8QK4sVhQzd2FDEJJrd/s3AMcERyOciLwKhL5W1tb6iYApwOLWTf6uYzfwWWVmxB5DNiAclrWhJSmJmXUde4BurAC9CiEp2lsslwbu5jCEsAwkXoj5xgLisQuIhL5Jdu2jfKnRKaAHgpyX04PyM/eLQb9GMnkpO3WPbxlG8g7CPsSTeSePXbJQ4AxqL6U8sraobmX77SNB74GXAZ8KOdsZ1ddb/Bjbt4LPJBo5DREP43oE2yY9mbO82OR1MyyKIM6CCDbpiDufkqSk7Ou03bjrzPL30S0BtgbcVWtL5859RGQ7+D4JZHBY7LbEuyJUAa8R3RI29Ic38aQIJlb6KQTD3ju0n0QrQbGoPyMqvJVbX8MpgF7g/t9zi1rsUGv4Gdxj+l0+QxBbntqlxyFcjXQjHO3dmsrntGrFM4ssN9lMcaPW0kxYxeN5+Yl/ocZDQRNHoAwlCKifllD8lxgExK+yE3LSrn82KaM8pINxRAsBz0P5CzmLvwnpcM2dLq75KuVzdTWPYDKp4DZ1C65hkS4kkFD4iQ2leKCjwDXAOuR4MHW66pfKiK5bixQitDCCMZx4+MbMx6WqvKXqam/GvgFwghcO0/n9roSGtw4hBKUgLBlNDc+3pj7YZPHQGeiMgsYgQZzcgYduHX5IJobxhEQQxUSJWO5eckgooEQlSTJxL7A6Nbzb1o2AokPTf3fMBYsHc6a4zdTLY4EvyPKWcD5JLY+za11j9HS1AClxSCjCDmUm5Y1MriombCpGE0ehuP7/mXgfs/0yQm/XCcxDigBIoSxscyv20xV5Va/Y6V5DEqAk0HMXTiWSEmEUKPEkiNwfBLCrwGHIsxlW/R2wC8BCorH4fT/IKxG5A1uryvhq5WZaxZbklGEZ4FTETmTmiVvsGHkZsZsGEwy9BNKokP9TiCNIi0xkHGoVKLu68AwYDYb5PGcvxtjp1I4AqgyE/Tr+EgYJ5KMPEbrhgEHwt7A64QcioTXAIcjhGhwG0Ut1wG/zSgvlFmIfh0IEc4jEvsozQ0XAS92akNT/DcUF++N6EWoPkJEVhHf2gjRPfw+ZdaCfouZFW0BCUasPxKn84BDfF3yGCVFNwC/yih73JpHWTPmJpRrfeCC1BKRBj0BiVyH6gGAoMG9lBTfCNyeZd/6Ua8yas1iVM4D6okGf83ZjnjjGQTyXeBDgEPD+4jiwEECRRgOOCJuGzctG0Es/nPgE/i1eLMJwxMZWv9NYBOXVW5ift31OPk5Sg0JWYrscREarkZ0EKJ3Eou/RzzRgOogRPYBGhG5inVj/+C/20HHIvpjhANRouDuJpQFVFfXQvNs4CygCNEvEUSnQVKIEMNJKb4X9DIiFxCP/YEr0y+6kk+iwVyED6M0o3IHDVwN/KH1PqgKtUu+A5yLn6D6BugkRq75NqFcjlDu7zlXkkhc5McEI0UogxBCHM8SuJ+yThbnHDc1djqFI4BQj8oHrTNymmPRrLo1BPoqGtS2Rg1BAyT4R9a5AXU4eR9IggtAEkgke+dDe644qZHql65j9NqHCTkGZD+817IJ5WWEpcwof4uZ7a4JWUWM2tatZI4Igcveq3rOOSG3/nkB8djrqGvXhdNXUJ1LkFoDpxoQCZ7LaV/1YXHmLb4BdQuBV7h4ysac56n8HdEftY6ptb+XQjrSSjO414EmhDtRHkIlRDQCsp5h2uZJXVzxF2rqP0/AkbhgGy2Nmykr+yGJ+CMgE4F9EQYhbEHlVVz4FzZVvtbqnWrkDdTNpW2vTQRxLzB7tlK75BEcL5H+ntJDASoh6EZU3yTKW1RVZO7NdrE3CdzNBKmJDOeECC9n34vwjxB5rV35TQS8S8hvEP7s62lXrz9vDWFkJdL4DjPa7a4xDMMwDMMwDMMwDMMwDMMwDMMwDMMwDMMwDMMwDMMwuk3h5CC4554IG/fveu/z6galujJJtQaMfy6S8VlnZa44LJIRoeXCSclOI05X10UZPyTznl84KckcpNv1dWzDhZOSzKmPZJXbFcNXOs4+2/HT57a/EH71pJBqcTnrXt2gzK4Ic7ZXVTLKT5ezvfPytS+9j7i7dGaPUVAUjgDWLr4Q1dPweU0hV/BJkRdYN+paRq75DCIX4MQh2kCgP6aqMqrE63sAAAzSSURBVDsm3rzF5yL6JZQkfstTnGhwHRdPfSHr3JpHi5HS7yByFEoSlQDRNURj3yORPJ5A/9Pv4dUtRIMf5S5j8XkE+sXW+qCZQG/AcTLIMR3OdoCm6knvfkgL1b1E3DM4mY3KEERdxnnpc0XjCDczo+Ipausu9TERSaTq9vH9lK2IvAruccaufb41iMOCpfuTDKsRhgIO1QfZUHFnlujULJmIuKt9WlB1PvQ/9xMmHyWI/gBhjK8np30OkdtIuGVEg2tB9+nWPQg0gaOGWRVLs+6xUVAUUDQYXYuwDjgROBLhLXyGtTdA3wQOw2kFg9+N4WQzTt4A3Q84HyfXUbNkdFaRgduAsgo4GvgowlsktCnrPIAN+yoq76EkgNMR3QvVd2iSJMImVFciuj/ClwndtT4cVwdEN+D0nVR9h6H6NkmSqJQDR+Kzq72B8jrIaOBziCqir6G8jugHiFYQ6NEQtCDyFgETUueFiL7Wdr22AKeg+ECrTj4ANgEnAQen7t27CINBLwZ5kPfHnNNqazIZB95Mie7piFzHyCXZmeei0uyTPTEMOBllM8o6oiQReQtkaMq+WAf7tgAnoXoAJW4w6MmI7OX/lvpOhUOBkwh0I8rr/h64ZpTPghzU6U/FKBgKxwME75WE4RLgOdaPPrs1vFF1XZSRcjfCXjRGT+bK4xtSG90XgE4HksCPKCqbnRXtZX5dGU4eATawXs/e7qb22kUnoMEfUG5kVkVb1FJVYX79T1C5EEgicgPr3Jys8nx9j4L+m7Frz+PtoSWUFD8KrKOo7Eut9tUuvhLVa4AzmFnxGOBjHkaii4BnmFH+NUSUmrr/QuRSJPgsM6Y+0VpPTf0U4D4CqWJG+T2+7sWH4rQO4RGqyr+OiPqcyaOngNwJrCPQz1BV+X67ck5D+C1QgvA0hOfmTAJeW385yhWEroJLp73a7vpZCD8CzmNmxb2tn89bfDjon1GuJwgfRCOLEX7BjIpr2n2ndwHlSKSiNeT+/PqP43gM1WpmVdZ2+V0ZA54C8gCBZDwt+C5j3I4KB/oQyN2UbmkX800D4HmQZ4Eq4o1n0ZGtsXQo9u5lmnOpTfG58lM4ifj6eA7VmYzgjJz1KUI6w1FkiENkCVDP6kltsfbUZb/cWkq2ITyCalvAz9agDx10W+RdROcTSFt05HREbdXMoKxj1y4F6oAPEerYzHI0gvccH0CZhEau4tblg3K03UeM7kjQSaa3wK1FWIC455CgBeQxVNqGKfyYaCoTXbv4gBquQbmfAIu+bBRUNBifFjOdOKc9flzqV1nn+/GoDxC9GeXniM6hZtErzJqWHR1GCJm4thsiGASg2dGJ5yCMStUXaA1ObkPkGm6pe4VLKjNDbIm0pVq8/NgmquvmMHGtMrNdmUrUBwB1bZ9deXwD1XVXMXGttk5cqAYIStghaOfM8jfxoefb0DAV1aSD7SvOVkYtTuIICToqKQAh4hakslb+B4mtfwd+ktmmVDTnaCxXKoDMQKpAysu8JtUG4afPzcp4AbQrIfO6ae8wp/6b/qVnFDqF5QGmE6MLIcNXdu8BUIR40ZOI/ABlXyS4hpuWjWj9e2lUUsmBummD87liOz7QbWkslUHUA9cD+xHInIyQ+76+IMODrK5MZkeQ7oSO5/oAUkqwnYxpmWSeO7L+YJRjEf6OG7Qq88zAt9exDnHfB95E+W6qi91Ga86QDtGcu2OfiDJ9ciJjgmV1qeBf8C4rGXx1ZdJmgA0oNA/Q41CO5IMxNzOvzqU8mi0kZAGXTV2dcaamEiFGNzniw+4iGj8S4RtE45dQXXct1ZVJihuEeAxcT5INd2pZBAEampTiMXcQ33oEcAFhOIN77rk+Q7icuIzk5x2RIAKqSCfh2NsaGYAUI1JFzeLPIU5AAoQ6ZlTcn3FqEEhK+/Zkft005i2K+piGwfmpcn7IrE9uybQjFEjFIpwx7SVq6r6PyG0I11FTdy6zKt/17VHf9rBDQiMngmgU5QJqFk/19gURlKfZMPU33RKyaFFPxN0oIArLA/R5gQUYBLInBBNQJuB0HCKZ68jmIKkxQP/wXH5sE+j1wDKEWYwITm93dpCVi7dTG1IeER0T5cxOezu+nOmTtxFGrgN5BriMNaNOBaC4ISVQnaw1bK0nZbvrlmcnqIxGdC8IJgATcDo06ywN0/djChosgOAukHmga4gEZzNjal120YH/jUWj3o4NPIByC3AM8G1uWuYTRKUDhnYUK3HpXMcj2uzTvXwKyw7J39sz4m1BxdvbUVQNI0VhCaDPChdBWIY2nsd69yXW6xcZt+YiLjl+Ve6L2gnNrMp3CfS7QAOi11CzZCItQzSVcU27lWu4tdgg89yJ96aSDLXrXl465R2Eq4BtaHAt8+o+zKZS53MEb+ehTqdk3G7XNogATeCqmVlxTuqenMOGijs7uSAC+kfEnYXoBcALwGEk3dDcopyKZJ0IvbBXVyZJFt0CPIzIBRS1nOfP09xi5Sdp/Cx8pn0LqK7u+qUjGkFwqZzMhpFFYQlg6ywmyritST8WlBoTy7mbQYKsDHJVlX48EPYjcHOQlpEpseneGBwuFdo+Z5LuACVkw75tdVZNXYpwHXAQUM3gFj/+KJKd+rI9kpoB7Ti50RE/W6ytkxfpe5KzaxkJvI2ygRnTXmJG5cPgrgJGI1ydMTbaoRYi7e7j5cduwOnV+HWC36NmydGtVmdfmk4F0A37ctacnSfYMFIUlgCm8wLnSi85d+k+zK87vi2365y2Lml7QQKIx+5C9XaU00EvovU+diPZeiudPsCZyc9FlFjZL0HuADkDIhfSm99bevKhY1e55tFiauuPoeaJA1o/C9JLeNrdv/XyBOidQAWxlnPoSDpfiItmln9J5Yvgvg8MQ9y1fkiCZJa35jrxZKvrotQsOZraRYehmi2cQ0rTy2AcSRsDNHJTWAKYzguca7wuCL+Ik19QVLKn/2A2iEq77WNtXH5sEzH3Q5CngOnAwUjO5R+d0zF374rR6e55tic5ffI2Aq5H+StKFcIhoLnTb6bxiciVSMfZ5ixDgtQ/mfYExeNQfoZEzu14QYZ4+4Xa/wO8CTKTWxZn7rBIC2wkR1d8vTwIOheYAnoWgsv21tLjsB3s26N4D59TWGb68dpO0U7zBBsFT+HMAtcuHImT/YAYyBA+GH0QtywOiQSCi4eI7A9aTEDS54R9cl9UhiJazIj1B3Jz3b+5rHJTa3nfPOE95i36LgS/BvZF1XdJqzupX1VYUD8WxwS8bzmKuUv3obR0NY3xKEHzPhDsgVMl2XggN9etzqivqnwVNfVXAb9G2Rskd7e9+qUiRq7bC9WRgOB0L2rqJjBu7eqMWWSfL3gv0GF4n24CNU8c4NdKJh0qB+KTtPs6apaMRpP7IUEEp0OZX3cgLcXvcfmxTcysfIWa+vkIPyJw36V20Y00Fr1DkStDdTxCDNy+zHtyM+uPe7+1+1pdmWTB0lsIw8PxqSvbFiffujxGYvOeKKNS9o1nwdL9SbgIUackE2NAR+PEZd33Gx8fzBb2QxiCECMI9mfuwgSXfqrrrH1GwVFAHmC0CmE+UAx6DMiDBPowGv4BiTwKejbQQFKaoewIXHgvcBwwGXEPEAs+l1XkzGl/QfQHwNbteoB31Bejch3I1fggAl8jGt6B2zyGaMsRBME9oJ9A+AROH8xZ36yKpb4MttLZmOOYDRNAfwmcgZ9U+W9E5rNxeFnGeVuDg5DI3fi90RHgRiTyKIE+7O9H8BNgCIFsA0DctyCY6/PucgpO7iWaPLzt9kbuAn4DciYa3EdpfCqx8EZEvwUMxcmtaPK/GP9cSYYdF0/ZiNPZwD9R4kgqWXOycRxEbgc5F3CgswnDxwj0YZw8QqB3AeNBt2S9CIqKjyaQ3yIcDoxE9U6i0auo7mRXiVGwFI4HGEYeQcLXWndgSI6tYiqbGRZupLkoJOl+0No9VA2QTnLpxobcTaLx3whru1yaMnhtgq2j70JlIUEqV6yTRpqSmygrS5BMdK++luZfURx7F+X9nH9PxtYRJH6MSCkEDqcRIrqR1XtmBmlIxN4jmriBwBX57qXLkSc5UAieT9l0P8jfEXEETlKTEm+2nnvxlI0sWHopLvwVyjBc7J+Ia0L1EUScb5N8wOqGeFY9l1S+yLy6CxDGES1b7z90G31icjeoU/sACF7K/ij6KhJek5qdB4ig4dtdeuiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRiGYRgDlP8PPiGHCYe6IgwAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "f6f4e81f",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "_Máster Universitario en Inteligencia Artificial_\n",
    "\n",
    "_Trabajo Fin de Máster_\n",
    "\n",
    "- Gustavo Mateos Santos\n",
    "- gustavo.mateos830@comunidadunir.net\n",
    "\n",
    "# Clasificación automática de estrellas variables con modelos Transformer aplicados a series temporales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62128cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting lightkurve\n",
      "  Downloading lightkurve-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Collecting astropy>=5.0 (from lightkurve)\n",
      "  Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting astroquery>=0.3.10 (from lightkurve)\n",
      "  Downloading astroquery-0.4.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.13.4)\n",
      "Requirement already satisfied: bokeh>=2.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.7.2)\n",
      "Collecting fbpca>=1.0 (from lightkurve)\n",
      "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (3.10.1)\n",
      "Collecting memoization>=0.3.1 (from lightkurve)\n",
      "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.26.4)\n",
      "Collecting oktopus>=0.1.2 (from lightkurve)\n",
      "  Downloading oktopus-0.1.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.32.3)\n",
      "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (4.67.1)\n",
      "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
      "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.23 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightkurve) (2.4.0)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy>=5.0->lightkurve)\n",
      "  Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting astropy-iers-data>=0.2024.10.28.0.34.7 (from astropy>=5.0->lightkurve)\n",
      "  Downloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (6.0.2)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astropy>=5.0->lightkurve) (24.2)\n",
      "Collecting html5lib>=0.999 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting keyring>=15.0 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
      "  Downloading pyvo-1.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.5)\n",
      "Requirement already satisfied: contourpy>=1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.3.2)\n",
      "Requirement already satisfied: narwhals>=1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (1.35.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (11.2.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bokeh>=2.3.2->lightkurve) (2025.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
      "Collecting autograd (from oktopus>=0.1.2->lightkurve)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->lightkurve) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.22.0->lightkurve) (2025.1.31)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.0.1)\n",
      "Collecting multidict<7.0.0,>=6.0.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
      "Collecting SecretStorage>=3.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading SecretStorage-3.3.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jeepney>=0.4.2 (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jeepney-0.9.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: importlib_metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.11.0)\n",
      "Collecting jaraco.classes (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib_metadata>=4.11.4->keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.21.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (44.0.2)\n",
      "Collecting more-itertools (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring>=15.0->astroquery>=0.3.10->lightkurve)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.22)\n",
      "Downloading lightkurve-2.5.0-py3-none-any.whl (270 kB)\n",
      "Downloading astropy-6.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astroquery-0.4.10-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
      "Downloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
      "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astropy_iers_data-0.2025.5.12.0.38.29-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Downloading pyerfa-2.0.1.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.7/738.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyvo-1.6.2-py3-none-any.whl (999 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.4/999.4 kB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading jeepney-0.9.0-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Building wheels for collected packages: fbpca, memoization, oktopus\n",
      "  Building wheel for fbpca (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11428 sha256=7a1c0c198fcd5b6d325211ca045f7e490a1f57d14f4fd405ac819d3c8488574e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3c/ea/60/8d1c9fbbc99492a1775b36a5e29c8c1ef309cc5821bd5a219d\n",
      "  Building wheel for memoization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50537 sha256=db994a6d6c135ea1fa23778fd6df8bb1ae62f672728f823114592e7ad00d4162\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3e/b8/c5/b553d5e8b0249bd2859b3b6d7bb2a1849e7b01c6e8b64f6e87\n",
      "  Building wheel for oktopus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for oktopus: filename=oktopus-0.1.2-py3-none-any.whl size=12817 sha256=4397a316aadeda57340417b4534cf34779c5dd2f8c9e0519ca0c4b36debe9cbb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b6/55/53/8d7c151deab7a11f182e5a69feb906cab95123da7a2438ed9d\n",
      "Successfully built fbpca memoization oktopus\n",
      "Installing collected packages: fbpca, wrapt, uncertainties, pyerfa, propcache, multidict, more-itertools, memoization, jeepney, html5lib, frozenlist, backports.tarfile, autograd, async-timeout, astropy-iers-data, aioitertools, aiohappyeyeballs, yarl, oktopus, jaraco.functools, jaraco.context, jaraco.classes, botocore, astropy, aiosignal, SecretStorage, pyvo, aiohttp, keyring, aiobotocore, s3fs, astroquery, lightkurve\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.38.1\n",
      "    Uninstalling botocore-1.38.1:\n",
      "      Successfully uninstalled botocore-1.38.1\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 0.4.2\n",
      "    Uninstalling s3fs-0.4.2:\n",
      "      Successfully uninstalled s3fs-0.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.40.0 requires botocore==1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "boto3 1.38.1 requires botocore<1.39.0,>=1.38.1, but you have botocore 1.37.3 which is incompatible.\n",
      "s3transfer 0.12.0 requires botocore<2.0a.0,>=1.37.4, but you have botocore 1.37.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SecretStorage-3.3.3 aiobotocore-2.22.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aioitertools-0.12.0 aiosignal-1.3.2 astropy-6.1.7 astropy-iers-data-0.2025.5.12.0.38.29 astroquery-0.4.10 async-timeout-5.0.1 autograd-1.8.0 backports.tarfile-1.2.0 botocore-1.37.3 fbpca-1.0 frozenlist-1.6.0 html5lib-1.1 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jeepney-0.9.0 keyring-25.6.0 lightkurve-2.5.0 memoization-0.4.0 more-itertools-10.7.0 multidict-6.4.3 oktopus-0.1.2 propcache-0.3.1 pyerfa-2.0.1.5 pyvo-1.6.2 s3fs-2025.3.2 uncertainties-3.2.3 wrapt-1.17.2 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages\n",
    "%pip install torch lightkurve\n",
    "%pip install -q pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455e0895-292c-4535-9e86-1e21e24e186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.2\n",
      "Lightkurve: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch, lightkurve as lk\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Lightkurve:\", lk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547a524",
   "metadata": {},
   "source": [
    "| Fuente       | Misión | Etiquetado | Tipo         | Método de acceso                   |\n",
    "| ------------ | ------ | ---------- | ------------ | ---------------------------------- |\n",
    "| Kepler EB    | Kepler | Sí         | Binaries     | NASA Exoplanet Archive / Villanova |\n",
    "| K2VARCAT     | K2     | Sí         | Multiclase   | MAST HLSP                          |\n",
    "| TESS-SVC     | TESS   | Semi       | Variabilidad | MAST HLSP                          |\n",
    "| TESS-EB      | TESS   | Sí         | EB continuas | MAST HLSP                          |\n",
    "| VSX (AAVSO)  | Todos  | Sí         | Multiclase   | AAVSO + MAST                       |\n",
    "| Gaia DR3 Var | Todos  | Sí         | Multiclase   | ESA Gaia Archive                   |\n",
    "\n",
    "Diseño del esquema del dataset. Vamos a unificar las curvas descargadas en un dataset estructurado tipo CSV/Parquet con este esquema mínimo:\n",
    "\n",
    "| id\\_mision | id\\_objeto | mision | clase\\_variable | fecha\\_inicio | fecha\\_fin | tiempo | magnitud | error | origen\\_etiqueta |\n",
    "| ---------- | ---------- | ------ | --------------- | ------------- | ---------- | ------ | -------- | ----- | ---------------- |\n",
    "\n",
    "Estrategia de implementación técnica\n",
    "Podemos hacerlo en varias etapas:\n",
    "\n",
    "- Script 1: descarga de curvas etiquetadas de Kepler EB y TESS-EB (usando ID + clase desde CSV original)\n",
    "- Script 2: procesamiento de K2VARCAT (CSV disponible con clase + ID)\n",
    "- Script 3: combinación cruzada entre TIC (TESS Input Catalog) y VSX (requiere posiciones celestes + nombre)\n",
    "- Script 4: cruce de Gaia DR3 variables con coordenadas de Kepler/K2/TESS\n",
    "- Script 5: exportación consolidada en formato CSV/Parquet final con clases normalizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd0a98",
   "metadata": {},
   "source": [
    "### 📥 Script 1: descarga y consolidación de curvas de Kepler y TESS de la clase EB (entorno local o SageMaker)\n",
    "Esta sección permite ejecutar el script 1, ya sea con el catálogo completo (en SageMaker) o con una muestra reducida (en local), y verificar el resultado consolidado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1325ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔇 Warnings silenciados: UserWarning, FutureWarning\n",
      "📁 Usando RAW_DIR: /home/ec2-user/backup/data/raw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/25 11:50:07] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/25 11:50:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=329048;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=354539;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "🗃️ Curvas ya existentes en disco: 7707\n",
      "🔁 Ejecutando en SageMaker → catálogo completo\n",
      "[⬇] Descargando catálogos completos de Kepler y TESS...\n",
      "[⬇] Descargando catálogo Kepler EB...\n",
      "[📂] Cargando catálogo Kepler EB desde copia local...\n",
      "[⬇] Descargando catálogo TESS EB...\n",
      "[⬇] Generando CSV de entrada para descarga de curvas...\n",
      "📝 CSV generado con 0 estrellas → data/lists/eb_ids.csv\n",
      "[⬇] Descargando curvas de luz...\n",
      "\n",
      "🚀 Procesando misión: Kepler (only_pending=True)\n",
      "📝 CSV generado con 0 estrellas → data/lists/eb_ids_kepler_pendientes.csv\n",
      "[⬇] Descargando 0 curvas en paralelo con 8 hilos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Descargando curvas: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t📦 0 objetos procesados hasta ahora...\n",
      "[✓] Descarga finalizada: 0 objetos procesados.\n",
      "[⭢] Leyendo y fusionando curvas descargadas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[📁] Usando 37 lotes ya existentes en /home/ec2-user/temp_batches\n",
      "[⏳] Uniendo todos los lotes en /home/ec2-user/backup/processed/dataset_eb_kepler.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 Uniendo lotes: 100%|██████████| 37/37 [01:46<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Dataset parquet construido → /home/ec2-user/backup/processed/dataset_eb_kepler.parquet\n",
      "🧹 Carpeta temporal eliminada: /home/ec2-user/temp_batches\n",
      "[⭢] Fusionando metadatos (etiquetas)...\n",
      "[⏱] Tiempo total: 108.39 segundos\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys, platform\n",
    "import glob\n",
    "from astropy.units import UnitsWarning\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignorar warnings comunes de librerías externas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UnitsWarning)\n",
    "print(\"🔇 Warnings silenciados: UserWarning, FutureWarning\")\n",
    "# Desactivar Rich para evitar problemas de compatibilidad\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "\n",
    "# Lanzando el script de ejemplo directamente\n",
    "# !python src/script_2_k2varcat_multiclase.py.py\n",
    "\n",
    "# Asegurarse de que el directorio 'src' esté en el path de Python\n",
    "src_path = Path(\"src\").resolve()\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "from src.fase1.script_1_kepler_tess_eb import main as run_script_1\n",
    "\n",
    "# Detectar entorno local/AWS y ejecutar el script\n",
    "try:\n",
    "    import sagemaker\n",
    "    is_sagemaker = True\n",
    "except ImportError:\n",
    "    is_sagemaker = False\n",
    "\n",
    "existing = len(glob.glob(\"/home/ec2-user/backup/data/raw/kepler/*.csv\")) + len(glob.glob(\"/home/ec2-user/backup/data/raw/tess/*.csv\"))\n",
    "print(f\"🗃️ Curvas ya existentes en disco: {existing}\", flush=True)\n",
    "\n",
    "# Ejecutar con la opción adecuada\n",
    "if is_sagemaker:\n",
    "    print(\"🔁 Ejecutando en SageMaker → catálogo completo\")\n",
    "    # Solo Kepler, solo pendientes\n",
    "    run_script_1(mission=\"Kepler\", only_pending=True)\n",
    "    # Ambos, solo pendientes\n",
    "    #run_script_1(mission=\"ALL\", only_pending=True)\n",
    "else:\n",
    "    print(\"💻 Ejecutando en entorno local → catálogo de prueba\")\n",
    "    run_script_1(use_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb92467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cleanup_raw import cleanup_raw_data\n",
    "\n",
    "# Limpiar los datos de prueba\n",
    "#cleanup_raw_data('/home/ec2-user/backup/data/raw', confirm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fc53a",
   "metadata": {},
   "source": [
    "##### 📥 Comprobación de los fichero de curvas Kepler y TESS (EB) mergeado y consolidado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ae821",
   "metadata": {},
   "source": [
    "##### ⚠️ Comprobación de la columna \"clase_variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 KEPLER contiene 31 columnas:\n",
      "['tiempo', 'magnitud', 'error', 'quality', 'timecorr', 'centroid_col', 'centroid_row', 'cadenceno', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'sap_quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable']\n",
      "✅ 'clase_variable' está presente en KEPLER\n",
      "\n",
      "📦 TESS contiene 30 columnas:\n",
      "['tiempo', 'magnitud', 'error', 'timecorr', 'cadenceno', 'centroid_col', 'centroid_row', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable']\n",
      "✅ 'clase_variable' está presente en TESS\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "for name in [\"kepler\", \"tess\"]:\n",
    "    path = Path(f\"data/processed/dataset_eb_{name}_labeled.parquet\")\n",
    "    if not path.exists():\n",
    "        print(f\"❌ Archivo no encontrado: {path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        schema = parquet_file.schema_arrow\n",
    "        columns = schema.names\n",
    "\n",
    "        print(f\"\\n📦 {name.upper()} contiene {len(columns)} columnas:\")\n",
    "        print(columns)\n",
    "\n",
    "        if \"clase_variable\" in columns:\n",
    "            print(f\"✅ 'clase_variable' está presente en {name.upper()}\")\n",
    "        else:\n",
    "            print(f\"⚠️  'clase_variable' NO está presente en {name.upper()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92b04a",
   "metadata": {},
   "source": [
    "### ✅ FIX de la columna \"clase_variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[🔧] Reparando parquet: dataset_eb_tess.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🛠️ Reparando por lotes: 2426it [10:19,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Reparación completada → data\\processed\\dataset_eb_tess_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reparar_parquet_streaming(parquet_path: Path, output_path: Path, clase_default: str = \"EB\"):\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\")\n",
    "\n",
    "    # Preparamos el escritor con el esquema extendido\n",
    "    sample_batch = next(dataset.to_batches(batch_size=100))\n",
    "    schema_original = sample_batch.schema\n",
    "    schema_nueva = schema_original.append(pa.field(\"clase_variable\", pa.string()))\n",
    "\n",
    "    writer = pq.ParquetWriter(output_path, schema=schema_nueva, compression=\"snappy\")\n",
    "\n",
    "    print(f\"[🔧] Reparando parquet: {parquet_path.name}\")\n",
    "    fragmentos = dataset.to_batches(batch_size=50000)\n",
    "\n",
    "    for batch in tqdm(fragmentos, desc=\"🛠️ Reparando por lotes\"):\n",
    "        n = batch.num_rows\n",
    "        columna_clase = pa.array([clase_default] * n, type=pa.string())\n",
    "        batch_corregido = batch.append_column(\"clase_variable\", columna_clase)\n",
    "        writer.write_table(pa.Table.from_batches([batch_corregido], schema=schema_nueva))\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"[✅] Reparación completada → {output_path}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "reparar_parquet_streaming(Path(\"data/processed/dataset_eb_tess.parquet\"),\n",
    "                          Path(\"data/processed/dataset_eb_tess_labeled.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb18ab",
   "metadata": {},
   "source": [
    "### 📥 Script 2: descarga y consolidación de curvas de K2 (multiclase) (entorno local o SageMaker)\n",
    "Esta sección permite ejecutar el script 2, ya sea con el catálogo completo (en SageMaker) o con una muestra reducida (en local), y verificar el resultado consolidado.\n",
    "\n",
    "| Posición | Nombre          | Descripción                                                                   |\n",
    "| -------- | --------------- | ----------------------------------------------------------------------------- |\n",
    "| 0        | `EPIC_ID`       | Identificador del catálogo EPIC                                               |\n",
    "| 1        | `K2_TYPE`       | Tipo de curva: EB, AP, QP, P                                                  |\n",
    "| 2        | `RANGE_PCT`     | Rango de la curva de luz (en %)                                               |\n",
    "| 3        | `PERIOD_DAYS`   | Periodo estimado (días). 0 para sistemas aperiódicos                          |\n",
    "| 4        | `AMPLITUDE_PCT` | Amplitud (en %), interpretada según el tipo                                   |\n",
    "| 5        | `PROPOSAL_INFO` | Texto libre con número de propuesta y/o tipo de variable                      |\n",
    "| 6        | `AMP_ERR1`      | Sin descripción clara — puede ser un valor derivado técnico (puede ignorarse) |\n",
    "| 7        | `AMP_ERR2`      | Ídem                                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7f111",
   "metadata": {},
   "source": [
    "Aunque search_lightcurve() encuentra resultados, estos están en un formato no compatible con las versiones actuales de Lightkurve (que está diseñado para los productos SPOC y algunos QLP).\n",
    "En resumen:\n",
    "Los datos de K2VARCAT en MAST existen.\n",
    "Pero no se pueden abrir ni procesar directamente con Lightkurve, ni con .download() ni con .download_all().\n",
    "\n",
    "- Usar EPIC_ID + clase del CSV de K2VARCAT.\n",
    "- Buscar curvas reales usando author=\"EVEREST\" (sí compatibles con Lightkurve).\n",
    "- Descargar y procesar curvas para esos objetos.\n",
    "- Guardar todo en formato .parquet con esquema estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc982c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔇 Warnings silenciados: UserWarning, FutureWarning\n",
      "💻 Ejecutando en entorno local → catálogo de prueba\n",
      "✅ Catálogo ya descargado: data/raw/k2varcat_official.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📦 Procesando curvas EVEREST: 100%|██████████| 8395/8395 [43:51<00:00,  3.19it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen de estado: {'❌': 3478, '🟡': 3024, '✅': 1893}\n",
      "✅ Dataset final guardado en: data/processed/dataset_k2varcat.parquet\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys\n",
    "import glob\n",
    "from astropy.units import UnitsWarning\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignorar warnings comunes de librerías externas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UnitsWarning)\n",
    "print(\"🔇 Warnings silenciados: UserWarning, FutureWarning\")\n",
    "# Desactivar Rich para evitar problemas de compatibilidad\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "\n",
    "# Lanzando el script de ejemplo directamente\n",
    "# !python src/script_2_k2varcat_multiclase.py.py\n",
    "\n",
    "# Asegurarse de que el directorio 'src' esté en el path de Python\n",
    "src_path = Path(\"src\").resolve()\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "from src.fase1.script_2_k2varcat_everest import main as run_script_2\n",
    "\n",
    "# Detectar entorno local/AWS y ejecutar el script\n",
    "try:\n",
    "    import sagemaker\n",
    "    is_sagemaker = True\n",
    "except ImportError:\n",
    "    is_sagemaker = False\n",
    "\n",
    "if is_sagemaker:\n",
    "    print(\"🔁 Ejecutando en SageMaker → catálogo completo\")\n",
    "    run_script_2(limit=None, workers=10)\n",
    "else:\n",
    "    print(\"💻 Ejecutando en entorno local → catálogo de prueba\")\n",
    "    run_script_2(limit=None, workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2d608",
   "metadata": {},
   "source": [
    "#### Borrado caché Lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa966cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ clear_download_cache no está disponible en esta versión de Lightkurve. Se eliminará manualmente.\n",
      "✅ Caché eliminada: C:\\Users\\hav3f/.lightkurve/cache\n",
      "💾 Espacio libre actual: 96 GB\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Ruta por defecto en sistemas Windows / Unix\n",
    "cache_dir = os.path.expanduser(\"~/.lightkurve/cache\")\n",
    "\n",
    "# Intentar vaciar con método oficial de Lightkurve (si existe)\n",
    "try:\n",
    "    from lightkurve import clear_download_cache\n",
    "    print(\"🧹 Limpiando caché con Lightkurve...\")\n",
    "    clear_download_cache()\n",
    "except ImportError:\n",
    "    print(\"⚠️ clear_download_cache no está disponible en esta versión de Lightkurve. Se eliminará manualmente.\")\n",
    "\n",
    "# Borrar carpeta cache si todavía quedan ficheros residuales\n",
    "if os.path.exists(cache_dir):\n",
    "    try:\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(f\"✅ Caché eliminada: {cache_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ No se pudo eliminar completamente la carpeta: {e}\")\n",
    "else:\n",
    "    print(\"ℹ️ No se encontró la carpeta de caché\")\n",
    "\n",
    "# Confirmación de espacio liberado (solo si quieres usarlo)\n",
    "import psutil\n",
    "total, used, free = shutil.disk_usage(\".\")\n",
    "print(f\"💾 Espacio libre actual: {free // (1024 ** 3)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ee007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "chunks_dir = \"data/processed/k2varcat_chunks\"\n",
    "shutil.rmtree(chunks_dir)\n",
    "print(f\"✅ Directorio eliminado: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907e244",
   "metadata": {},
   "source": [
    "\n",
    "### 📥 Script 3: descarga y consolidación de curvas de K2 (multiclase) (entorno local o SageMaker)\n",
    "\n",
    "#### 🎯 Objetivo:\n",
    "\n",
    "Cruzar objetos del catálogo TIC (TESS Input Catalog) con el catálogo **VSX (AAVSO Variable Star Index)** para:\n",
    "\n",
    "-   Obtener variables etiquetadas con clases del VSX.\n",
    "    \n",
    "-   Localizar sus posiciones celestes (RA, DEC).\n",
    "    \n",
    "-   Buscar sus curvas reales mediante ID TIC o coordenadas.\n",
    "    \n",
    "#### ✅ Fuentes necesarias:\n",
    "\n",
    "**TIC**\n",
    "\n",
    "ID TIC, RA/DEC, magnitudes, TESS target info\n",
    "\n",
    "MAST via Vizier o CDS\n",
    "\n",
    "**VSX**\n",
    "\n",
    "Nombre variable, RA/DEC, tipo variable (EA, DSCT, RRab, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fe5ed",
   "metadata": {},
   "source": [
    "tic_dec00_00N__02_00N.csv.gz - 4.51GB\n",
    "\n",
    "tic_dec02_00N__04_00N.csv.gz - 4.77GB\n",
    "\n",
    "tic_dec02_00S__00_00N.csv.gz - 5.04GB\n",
    "\n",
    "tic_dec04_00N__06_00N.csv.gz - 4.57GB\n",
    "\n",
    "tic_dec04_00S__02_00S.csv.gz - 4.40GB\n",
    "\n",
    "tic_dec06_00N__08_00N.csv.gz - 4.65GB\n",
    "\n",
    "tic_dec06_00S__04_00S.csv.gz - 5.01GB\n",
    "\n",
    "tic_dec08_00N__10_00N.csv.gz - 4.87GB\n",
    "\n",
    "tic_dec08_00S__06_00S.csv.gz - 5.51GB\n",
    "\n",
    "tic_dec10_00N__12_00N.csv.gz - 5.09GB\n",
    "\n",
    "tic_dec10_00S__08_00S.csv.gz - 5.40GB\n",
    "\n",
    "tic_dec12_00N__14_00N.csv.gz - 5.03GB\n",
    "\n",
    "tic_dec12_00S__10_00S.csv.gz - 5.40GB\n",
    "\n",
    "tic_dec14_00N__16_00N.csv.gz - 4.94GB\n",
    "\n",
    "tic_dec14_00S__12_00S.csv.gz - 5.90GB\n",
    "\n",
    "tic_dec16_00N__18_00N.csv.gz - 4.76GB\n",
    "\n",
    "tic_dec16_00S__14_00S.csv.gz - 6.93GB\n",
    "\n",
    "tic_dec18_00N__20_00N.csv.gz - 4.77GB\n",
    "\n",
    "tic_dec18_00S__16_00S.csv.gz - 7.79GB\n",
    "\n",
    "tic_dec20_00S__18_00S.csv.gz - 7.99GB\n",
    "\n",
    "**TOTAL: 107 GB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔇 Warnings silenciados: UserWarning, FutureWarning\n",
      "💻 Ejecutando en entorno local → catálogo completo\n",
      "📉 Catálogo VSX reducido a franja DEC -20 → 20 (3900 objetos)\n",
      "🎯 Limitando a 30 regiones DEC (bins únicos)\n",
      "📦 Archivos DEC necesarios: ['tic_dec00_00N__02_00N.csv.gz', 'tic_dec02_00N__04_00N.csv.gz', 'tic_dec02_00S__00_00N.csv.gz', 'tic_dec04_00N__06_00N.csv.gz', 'tic_dec04_00S__02_00S.csv.gz', 'tic_dec06_00N__08_00N.csv.gz', 'tic_dec06_00S__04_00S.csv.gz', 'tic_dec08_00N__10_00N.csv.gz', 'tic_dec08_00S__06_00S.csv.gz', 'tic_dec10_00N__12_00N.csv.gz', 'tic_dec10_00S__08_00S.csv.gz', 'tic_dec12_00N__14_00N.csv.gz', 'tic_dec12_00S__10_00S.csv.gz', 'tic_dec14_00N__16_00N.csv.gz', 'tic_dec14_00S__12_00S.csv.gz', 'tic_dec16_00N__18_00N.csv.gz', 'tic_dec16_00S__14_00S.csv.gz', 'tic_dec18_00N__20_00N.csv.gz', 'tic_dec18_00S__16_00S.csv.gz', 'tic_dec20_00S__18_00S.csv.gz']\n",
      "🟡 Ya existe: tic_dec00_00N__02_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec02_00S__00_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec04_00N__06_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec02_00N__04_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec06_00N__08_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec04_00S__02_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec06_00S__04_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec08_00N__10_00N.csv.gz. No se vuelve a descargar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⬇️ Descargando TIC: 100%|██████████| 20/20 [00:00<00:00, 661.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟡 Ya existe: tic_dec10_00N__12_00N.csv.gz. No se vuelve a descargar.🟡 Ya existe: tic_dec08_00S__06_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec12_00N__14_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec10_00S__08_00S.csv.gz. No se vuelve a descargar.\n",
      "\n",
      "🟡 Ya existe: tic_dec12_00S__10_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec14_00S__12_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec14_00N__16_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec16_00S__14_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec16_00N__18_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec18_00N__20_00N.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec18_00S__16_00S.csv.gz. No se vuelve a descargar.\n",
      "🟡 Ya existe: tic_dec20_00S__18_00S.csv.gz. No se vuelve a descargar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Procesando en chunks tic_dec06_00N__08_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 389.62 s\n",
      "📂 Procesando en chunks tic_dec02_00N__04_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 422.36 s\n",
      "📂 Procesando en chunks tic_dec08_00N__10_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 407.71 s\n",
      "📂 Procesando en chunks tic_dec02_00S__00_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 348.85 s\n",
      "📂 Procesando en chunks tic_dec04_00S__02_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 304.94 s\n",
      "📂 Procesando en chunks tic_dec00_00N__02_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 284.42 s\n",
      "📂 Procesando en chunks tic_dec06_00S__04_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 306.23 s\n",
      "📂 Procesando en chunks tic_dec04_00N__06_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 266.55 s\n",
      "📂 Procesando en chunks tic_dec08_00S__06_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 336.47 s\n",
      "📂 Procesando en chunks tic_dec12_00N__14_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 274.89 s\n",
      "📂 Procesando en chunks tic_dec10_00S__08_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 303.38 s\n",
      "📂 Procesando en chunks tic_dec10_00N__12_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 265.61 s\n",
      "📂 Procesando en chunks tic_dec12_00S__10_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 318.66 s\n",
      "📂 Procesando en chunks tic_dec14_00S__12_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 320.75 s\n",
      "📂 Procesando en chunks tic_dec14_00N__16_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 255.19 s\n",
      "📂 Procesando en chunks tic_dec16_00S__14_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 374.08 s\n",
      "📂 Procesando en chunks tic_dec16_00N__18_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 242.62 s\n",
      "📂 Procesando en chunks tic_dec18_00N__20_00N.csv.gz...\n",
      "⏱️ Chunks procesados en 248.63 s\n",
      "📂 Procesando en chunks tic_dec18_00S__16_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 424.08 s\n",
      "📂 Procesando en chunks tic_dec20_00S__18_00S.csv.gz...\n",
      "⏱️ Chunks procesados en 443.89 s\n",
      "✅ Guardado en data/processed/dataset_vsx_tic_labeled.parquet (3989 coincidencias)\n",
      "\n",
      "📁 Inspeccionando: data/processed/dataset_vsx_tic_labeled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧮 Procesando por lotes: 1it [00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resumen exportado a: data/processed/summary\\dataset_vsx_tic_labeled_summary.csv\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Acceso denegado: 'data\\\\processed\\\\temp_vsx_tic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💻 Ejecutando en entorno local → catálogo completo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mrun_script_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradio_arcsec\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_bins\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_download_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_3_vsx_tic_match.py:256\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(limit, radio_arcsec, limit_bins, max_download_workers)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parquet_paths:\n\u001b[32m    255\u001b[39m     p.unlink()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[43mTEMP_DIR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🧹 Archivos temporales eliminados.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\anaconda3\\envs\\astro_transformer\\Lib\\pathlib.py:1156\u001b[39m, in \u001b[36mPath.rmdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrmdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1153\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1154\u001b[39m \u001b[33;03m    Remove this directory.  The directory must be empty.\u001b[39;00m\n\u001b[32m   1155\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 5] Acceso denegado: 'data\\\\processed\\\\temp_vsx_tic'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys\n",
    "import glob\n",
    "from astropy.units import UnitsWarning\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignorar warnings comunes de librerías externas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UnitsWarning)\n",
    "print(\"🔇 Warnings silenciados: UserWarning, FutureWarning\")\n",
    "# Desactivar Rich para evitar problemas de compatibilidad\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "\n",
    "# Lanzando el script de ejemplo directamente\n",
    "# !python src/script_2_k2varcat_multiclase.py.py\n",
    "\n",
    "# Asegurarse de que el directorio 'src' esté en el path de Python\n",
    "src_path = Path(\"src\").resolve()\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "from src.fase1.script_3_vsx_tic_match import main as run_script_3\n",
    "\n",
    "# Detectar entorno local/AWS y ejecutar el script\n",
    "try:\n",
    "    import sagemaker\n",
    "    is_sagemaker = True\n",
    "except ImportError:\n",
    "    is_sagemaker = False\n",
    "\n",
    "if is_sagemaker:\n",
    "    print(\"🔁 Ejecutando en SageMaker → catálogo completo\")\n",
    "    run_script_3(limit=None, radio_arcsec=3.0, limit_bins=5)\n",
    "else:\n",
    "    print(\"💻 Ejecutando en entorno local → catálogo completo\")\n",
    "    run_script_3(limit=5000, radio_arcsec=3.0, limit_bins=30, max_download_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29741a",
   "metadata": {},
   "source": [
    "### ℹ️ Verificación de estructura y metada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fa5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Inspeccionando: data/processed/dataset_vsx_tess_labeled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧮 Procesando por lotes: 314it [03:04,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resumen exportado a: data/processed/summary\\dataset_vsx_tess_labeled_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def inspect_and_export_summary(parquet_path, output_format=\"json\"):\n",
    "    print(f\"\\n📁 Inspeccionando: {parquet_path}\")\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\")\n",
    "    schema = dataset.schema\n",
    "\n",
    "    summary = {\n",
    "        \"file\": parquet_path,\n",
    "        \"columns\": {field.name: str(field.type) for field in schema},\n",
    "        \"class_distribution\": {},\n",
    "        \"total_rows\": 0,\n",
    "        \"total_objects\": 0,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    class_counter = Counter()\n",
    "    objetos = set()\n",
    "\n",
    "    for batch in tqdm(dataset.to_batches(columns=[\"clase_variable\", \"id_objeto\"]), desc=\"🧮 Procesando por lotes\"):\n",
    "        summary[\"total_rows\"] += batch.num_rows\n",
    "        if \"clase_variable\" in batch.schema.names:\n",
    "            clases = batch.column(\"clase_variable\").to_pylist()\n",
    "            class_counter.update(clases)\n",
    "        if \"id_objeto\" in batch.schema.names:\n",
    "            objetos.update(batch.column(\"id_objeto\").to_pylist())\n",
    "\n",
    "    summary[\"class_distribution\"] = dict(class_counter)\n",
    "    summary[\"total_objects\"] = len(objetos)\n",
    "\n",
    "    # Salida\n",
    "    output_dir = \"data/processed/summary\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    basename = os.path.splitext(os.path.basename(parquet_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{basename}_summary.{output_format}\")\n",
    "\n",
    "    if output_format == \"json\":\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    elif output_format == \"csv\":\n",
    "        with open(output_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Clase\", \"Recuento\"])\n",
    "            for clase, count in class_counter.items():\n",
    "                writer.writerow([clase, count])\n",
    "        # Añadir resumen general en info.txt\n",
    "        with open(output_path.replace(\".csv\", \"_info.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Fichero: {summary['file']}\\n\")\n",
    "            f.write(f\"Filas totales: {summary['total_rows']}\\n\")\n",
    "            f.write(f\"Curvas únicas (id_objeto): {summary['total_objects']}\\n\")\n",
    "            f.write(f\"Columnas: {list(summary['columns'].keys())}\\n\")\n",
    "            f.write(f\"Fecha: {summary['timestamp']}\\n\")\n",
    "    else:\n",
    "        raise ValueError(\"❌ Formato no soportado. Usa 'json' o 'csv'.\")\n",
    "\n",
    "    print(f\"✅ Resumen exportado a: {output_path}\")\n",
    "\n",
    "# 🔧 Uso de ejemplo\n",
    "#inspect_and_export_summary(\"data/processed/dataset_eb_kepler_labeled.parquet\", output_format=\"csv\")\n",
    "#inspect_and_export_summary(\"data/processed/dataset_eb_tess_labeled.parquet\", output_format=\"csv\")\n",
    "# Inspeccionar el parquet de K2\n",
    "#inspect_and_export_summary(\"data/processed/dataset_k2varcat_labeled.parquet\", output_format=\"csv\")\n",
    "# Inspeccionar el parquet de VSX-TIC-TESS\n",
    "inspect_and_export_summary(\"data/processed/dataset_vsx_tess_labeled.parquet\", output_format=\"csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda7bc4",
   "metadata": {},
   "source": [
    "📊 ¿Qué indica el recuento?\n",
    "No representa el número de estrellas, sino la suma de todas las observaciones individuales que componen las curvas.\n",
    "\n",
    "#### **Kepler**\n",
    "\n",
    "| Métrica                          | Valor         |\n",
    "| -------------------------------- | ------------- |\n",
    "| 🔢 Filas totales (observaciones) | 71,735,011    |\n",
    "| 🌟 Curvas únicas (`id_objeto`)   | 2762         |\n",
    "| 🧬 Clases presentes              | EB |\n",
    "\n",
    "71,7 millones de puntos de luz (observaciones) etiquetados como \"EB\" (eclipsing binary), pertenecientes a 2762 curvas de luz.\n",
    "\n",
    "#### **TESS**\n",
    "\n",
    "| Métrica                          | Valor         |\n",
    "| -------------------------------- | ------------- |\n",
    "| 🔢 Filas totales (observaciones) | 120,747,997    |\n",
    "| 🌟 Curvas únicas (`id_objeto`)   | 4142         |\n",
    "| 🧬 Clases presentes              | EB |\n",
    " \n",
    "120,7 millones de observaciones etiquetadas como \"EB\", pertenecientes a 4142 curvas de luz.\n",
    "\n",
    "#### **K2**\n",
    "\n",
    "| Métrica                          | Valor         |\n",
    "| -------------------------------- | ------------- |\n",
    "| 🔢 Filas totales (observaciones) | 17,895,159    |\n",
    "| 🌟 Curvas únicas (`id_objeto`)   | 4917         |\n",
    "| 🧬 Clases presentes              | AP, QP, P, EB |\n",
    "\n",
    "17,8 millones de observaciones etiquetadas multiclase, pertenecientes a 4917 curvas de luz.\n",
    "\n",
    "Distribución de clases: \n",
    "\n",
    "| Clase | Observaciones |\n",
    "| ----- | ------------- |\n",
    "| AP    | 9,884,704     |\n",
    "| QP    | 7,344,096     |\n",
    "| P     | 433,803       |\n",
    "| EB    | 232,556       |\n",
    "\n",
    "NOTA: la clase `EB`, que no estaba en el subconjunto reducido pero sí aparece en el total, lo cual valida que el catálogo completo contiene ejemplos de todas las clases previstas.\n",
    "\n",
    "#### **VSX <> TIC**\n",
    "\n",
    "\n",
    "-   **Total de coincidencias**: 3989\n",
    "-   **Objetos únicos**: 3879 → solo hay unos pocos objetos duplicados (esperable si hay múltiples matches por posición con el mismo `id_objeto` VSX)\n",
    "-   **Columnas correctas**: identificadores, posición cruzada, tipo de variable y fuente.\n",
    "\n",
    "📊 Distribución de clases\n",
    "\n",
    "-   El sistema ha capturado una gran variedad de clases reales como:  \n",
    "    -   `ROT`, `RRAB`, `DSCT`, `RS`, `EA`, `EW`, `EB`, `GDOR`, `ACV`, etc.\n",
    "-   Hay algunas clases **ambiguas o sin limpiar**, como:\n",
    "    -   Compuestos (`DSCT|GDOR|SXPHE`, `ESD|EC`)\n",
    "    -   Vacíos: `''` → 16 filas\n",
    "    -   Etiquetas complejas (más de 3 clases unidas por `|`)\n",
    "\n",
    "🔁 Esto es habitual en VSX, y podrás **normalizar esas clases** para agrupar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0fa31",
   "metadata": {},
   "source": [
    "✅ `dataset_vsx_tic_labeled.parquet` es un **catálogo cruzado** que contiene:\n",
    "\n",
    "* Estrellas variables identificadas en VSX\n",
    "* Posiciones (RA/DEC) y tipos de variabilidad (`clase_variable`)\n",
    "* Su correspondiente `tic_id` en el catálogo TIC\n",
    "\n",
    "🚫 Pero **no contiene curvas de luz**, a diferencia de tus datasets previos de Kepler, K2 y TESS.\n",
    "\n",
    "🧭 Por tanto, los **pasos siguientes correctos** son:\n",
    "\n",
    "#### ✅ 1. Limpieza y consolidación de clases (`clase_variable`)\n",
    "\n",
    "* VSX usa nomenclaturas complejas: combinaciones (`DSCT|GDOR|SXPHE`), ambigüedades (`VAR`, `MISC`) y valores vacíos.\n",
    "* Necesitas definir una taxonomía unificada (por ejemplo, agrupar todos los `RR*` en `RR Lyrae`, o filtrar `UNKNOWN`).\n",
    "* Resultado: una columna `clase_variable_normalizada` útil para clasificación supervisada.\n",
    "\n",
    "👉 Esto se puede hacer ya sobre el `.parquet` actual sin descargar nada más.\n",
    "\n",
    "#### ✅ 2. Usar `tic_id` para descargar curvas reales de TESS\n",
    "\n",
    "* Usas `Lightkurve` o MAST para buscar y descargar curvas de luz (`PDCSAP_FLUX`) por `tic_id`.\n",
    "* Construyes un nuevo `.parquet` que **sí contendrá curvas reales** (tiempo, magnitud, error...).\n",
    "* Este será tu dataset VSX-TESS etiquetado real.\n",
    "\n",
    "👉 Esto será el que puedes fusionar con Kepler, K2 y TESS ya procesados. **...Eso es lo que hemos hecho (ver celda abajo con llamada a `script_3a_clean_vsx_classes`) y tenemos**: \n",
    "\n",
    "##### 📌 `dataset_vsx_tess_labeled.parquet`\n",
    "\n",
    "#### **VSX <> TIC** => TESS\n",
    "\n",
    "| Métrica                          | Valor         |\n",
    "| -------------------------------- | ------------- |\n",
    "| 🔢 Filas totales (observaciones) | 41,1 millones    |\n",
    "| 🌟 Curvas únicas (`id_objeto`)   | 1594         |\n",
    "| 🧬 Clases presentes              | múltiples |\n",
    "| 🟢 Columnas              | tiempo, magnitud, error, clase_variable, etc. |\n",
    "\n",
    "\n",
    "📊 Distribución de clases\n",
    "✅ Cosas correctas:\n",
    "- Las clases originales del catálogo VSX han sido conservadas.\n",
    "- Muchas de ellas fueron etiquetadas por patrones múltiples (DSCT|GDOR|SXPHE, EC|ESD, etc.).\n",
    "- Existen clases ya normalizadas como ROT, EA, RS, EB, RRAB…\n",
    "- La clase vacía , se ha detectado como tal y separada (468006 filas): podemos limpiar eso después si hace falta.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b872d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Leyendo: data/processed/dataset_vsx_tic_labeled.parquet\n",
      "🔧 Normalizando clases...\n",
      "🔎 Clases únicas encontradas: 33\n",
      "💾 Guardando dataset limpio en: data/processed/dataset_vsx_tic_labeled_clean.parquet\n",
      "📊 Exportando resumen de clases...\n",
      "✅ Proceso completado\n"
     ]
    }
   ],
   "source": [
    "from src.fase1.script_3a_clean_vsx_classes import main as run_normalization\n",
    "run_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7041f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔇 Warnings silenciados: UserWarning, FutureWarning\n",
      "💻 Ejecutando en entorno local → catálogo completo\n",
      "📥 Cargando dataset: data/processed/dataset_vsx_tic_labeled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3987/3987 [12:00:11<00:00, 10.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardados 41143597 registros en parquet temporales.\n",
      "📦 Unificando resultados...\n",
      "✅ Dataset final guardado en: data/processed/dataset_vsx_tess_labeled.parquet (41143597 filas)\n",
      "\n",
      "📁 Inspeccionando: data/processed/dataset_vsx_tess_labeled.parquet\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💻 Ejecutando en entorno local → catálogo completo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43mrun_vsx_tess_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_3b_download_tess_curves_from_vsx.py:172\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(limit, workers)\u001b[39m\n\u001b[32m    169\u001b[39m     df = df.sample(n=\u001b[38;5;28mmin\u001b[39m(limit, \u001b[38;5;28mlen\u001b[39m(df)), random_state=\u001b[32m42\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔍 Ejecutando solo con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m estrellas (modo test aleatorio)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43mprocesar_todos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_3b_download_tess_curves_from_vsx.py:103\u001b[39m, in \u001b[36mprocesar_todos\u001b[39m\u001b[34m(df, num_workers)\u001b[39m\n\u001b[32m    100\u001b[39m df_final.to_parquet(OUTPUT_PARQUET, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Dataset final guardado en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_PARQUET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filas)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43minspect_and_export_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PARQUET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🧹 Eliminando temporales...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    106\u001b[39m shutil.rmtree(TEMP_DIR, ignore_errors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_3b_download_tess_curves_from_vsx.py:111\u001b[39m, in \u001b[36minspect_and_export_summary\u001b[39m\u001b[34m(parquet_path, output_format)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minspect_and_export_summary\u001b[39m(parquet_path, output_format=\u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📁 Inspeccionando: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     dataset = \u001b[43mds\u001b[49m.dataset(parquet_path, \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m     schema = dataset.schema\n\u001b[32m    114\u001b[39m     summary = {\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: parquet_path,\n\u001b[32m    116\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: {field.name: \u001b[38;5;28mstr\u001b[39m(field.type) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m schema},\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: datetime.now().isoformat()\n\u001b[32m    121\u001b[39m     }\n",
      "\u001b[31mNameError\u001b[39m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys\n",
    "import glob\n",
    "from astropy.units import UnitsWarning\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignorar warnings comunes de librerías externas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UnitsWarning)\n",
    "print(\"🔇 Warnings silenciados: UserWarning, FutureWarning\")\n",
    "# Desactivar Rich para evitar problemas de compatibilidad\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "\n",
    "# Asegurarse de que el directorio 'src' esté en el path de Python\n",
    "src_path = Path(\"src\").resolve()\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "from src.fase1.script_3b_download_tess_curves_from_vsx import main as run_vsx_tess_download\n",
    "\n",
    "# Detectar entorno local/AWS y ejecutar el script\n",
    "try:\n",
    "    import sagemaker\n",
    "    is_sagemaker = True\n",
    "except ImportError:\n",
    "    is_sagemaker = False\n",
    "\n",
    "if is_sagemaker:\n",
    "    print(\"🔁 Ejecutando en SageMaker → catálogo completo\")\n",
    "    run_vsx_tess_download(limit=None, workers=4)\n",
    "else:\n",
    "    print(\"💻 Ejecutando en entorno local → catálogo completo\")\n",
    "    run_vsx_tess_download(limit=None, workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f493cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Leídas 41143597 filas desde data/processed/dataset_vsx_tess_labeled.parquet\n",
      "✅ Guardado dataset limpio en: data/processed/dataset_vsx_tess_labeled_clean.parquet\n",
      "\n",
      "📁 Inspeccionando: data/processed/dataset_vsx_tess_labeled_clean.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧮 Procesando por lotes: 314it [03:52,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resumen exportado a: data/processed/summary\\dataset_vsx_tess_labeled_clean_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NORMALIZACION DE CLASES EN EL FICHERO FINAL DE ESTE SCRIPT\n",
    "\n",
    "from src.fase1.script_3c_clean_vsx_tess_classes import limpiar_dataset, inspect_and_export_summary\n",
    "\n",
    "path = limpiar_dataset()\n",
    "inspect_and_export_summary(path, output_format=\"csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388d6f1",
   "metadata": {},
   "source": [
    "| Clase        | Recuento        |\n",
    "| ------------ | --------------- |\n",
    "| Rotational   | 26.2 millones ✅ |\n",
    "| Eclipsing    | 3.1 millones ✅  |\n",
    "| Delta\\_Scuti | 2.9 millones ✅  |\n",
    "| RS\\_CVn      | 1.7 millones ✅  |\n",
    "| RR\\_Lyrae    | 427 mil ✅       |\n",
    "| Cataclysmic  | 535 mil ✅       |\n",
    "| White\\_Dwarf | 219 mil ✅       |\n",
    "| Irregular    | 3.6 millones ✅  |\n",
    "| BY\\_Dra      | 277 mil ✅       |\n",
    "| Beta\\_Cep    | 181 mil ✅       |\n",
    "| YSO          | 138 mil ✅       |\n",
    "| RARE         | 1.17 millones ✅ |\n",
    "| UNKNOWN      | 468 mil ⚠️      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4aa5c",
   "metadata": {},
   "source": [
    "#### ✅ 3. Script 4: cruce con Gaia DR3\n",
    "\n",
    "Este paso (cruce por coordenadas) tendrá más sentido cuando tengas todo lo anterior consolidado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analizando datasets para cruce con Gaia DR3:\n",
      "\n",
      "📁 dataset_eb_kepler_labeled.parquet\n",
      "🔑 Columnas detectadas: ['tiempo', 'magnitud', 'error', 'quality', 'timecorr', 'centroid_col', 'centroid_row', 'cadenceno', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'sap_quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable']\n",
      "\n",
      "⚠️ Coordenadas insuficientes para cruce con Gaia\n",
      "       tiempo  magnitud     error  quality  timecorr  centroid_col\n",
      "0  131.512362  1.065252  0.000161        0  0.001369     860.09303\n",
      "1  131.532797  0.974507  0.000158        0  0.001369     860.09343\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "📁 dataset_eb_tess_labeled.parquet\n",
      "🔑 Columnas detectadas: ['tiempo', 'magnitud', 'error', 'timecorr', 'cadenceno', 'centroid_col', 'centroid_row', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable']\n",
      "\n",
      "⚠️ Coordenadas insuficientes para cruce con Gaia\n",
      "        tiempo  magnitud     error  timecorr  cadenceno  centroid_col\n",
      "0  1544.113310  0.997588  0.001281  0.001009     227993    1644.27114\n",
      "1  1544.114699  0.997343  0.001280  0.001009     227994    1644.27761\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "📁 dataset_k2varcat_labeled.parquet\n",
      "🔑 Columnas detectadas: ['flux', 'flux_err', 'tiempo', 'magnitud', 'cadenceno', 'error', 'quality', 'fcor', 'cbv01', 'cbv02', 'cbv03', 'cbv04', 'cbv05', 'cbv06', 'bkg', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable', 'origen_etiqueta']\n",
      "\n",
      "⚠️ Coordenadas insuficientes para cruce con Gaia\n",
      "       flux  flux_err   tiempo      magnitud  cadenceno     error\n",
      "0  0.997226       NaN  91434.0  34713.106522    91434.0  9.058647\n",
      "1  0.997391       NaN  91435.0  34727.785645    91435.0  9.057479\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "📁 dataset_vsx_tess_labeled_clean.parquet\n",
      "🔑 Columnas detectadas: ['tiempo', 'magnitud', 'error', 'id_objeto', 'clase_variable', 'clase_variable_normalizada', 'id_mision', 'mision', 'origen_etiqueta']\n",
      "\n",
      "⚠️ Coordenadas insuficientes para cruce con Gaia\n",
      "        tiempo  magnitud     error        id_objeto clase_variable  \\\n",
      "0  2447.695610  1.006541  0.004634  TIC_611119043.0            ROT   \n",
      "1  2447.702555  1.004698  0.004634  TIC_611119043.0            ROT   \n",
      "\n",
      "  clase_variable_normalizada  \n",
      "0                 Rotational  \n",
      "1                 Rotational  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run src/inspect_parquets_for_gaia.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21be561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando dataset limpio: dataset_vsx_tic_labeled_clean.parquet\n",
      "📂 Cargando dataset original: dataset_vsx_tic_labeled.parquet\n",
      "🧾 Columnas del original: ['id_objeto', 'nombre_vsx', 'clase_variable', 'ra', 'dec', 'tic_id', 'tic_ra', 'tic_dec', 'origen_etiqueta']\n",
      "🧹 Eliminando columnas conflictivas del limpio: ['ra', 'dec', 'tic_ra', 'tic_dec']\n",
      "🧭 Usando coordenadas: tic_ra → ra, tic_dec → dec\n",
      "✅ Columnas finales en df_merged: ['id_objeto', 'nombre_vsx', 'clase_variable', 'tic_id', 'origen_etiqueta', 'clase_variable_normalizada', 'ra', 'dec']\n",
      "⚠️ Filas sin coordenadas tras merge: 0\n",
      "✅ Guardado con coordenadas: data\\processed\\dataset_vsx_tic_labeled_clean_with_coords.parquet (3989 filas)\n"
     ]
    }
   ],
   "source": [
    "%run src/script_3c_merge_coordinates_into_clean_vsx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf54573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔇 Warnings silenciados: UserWarning, FutureWarning\n",
      "💻 Ejecutando en entorno local → catalogo completo\n",
      "📄 Cargando: dataset_vsx_tic_labeled_clean_fixed.parquet\n",
      "📊 Total inicial: 3989\n",
      "📊 Tras normalizar id_objeto: 3989\n",
      "📊 Tras filtrar coordenadas válidas: 3989\n",
      "📊 Final tras drop_duplicates: 3987\n",
      "📦 Dataset listo: 3987 objetos únicos con coordenadas\n",
      "📦 Dataset listo: 3987 objetos únicos con coordenadas\n",
      "📊 Procesando dataset completo con 3987 objetos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3987/3987 [06:44<00:00,  9.85it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado en data\\processed\\dataset_gaia_dr3_vsx_tic_labeled.parquet (3987 filas)\n",
      "\n",
      "📁 Inspeccionando: data\\processed\\dataset_gaia_dr3_vsx_tic_labeled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧮 Procesando por lotes: 1it [00:00, 55.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resumen exportado a: data\\processed\\summary\\dataset_gaia_dr3_vsx_tic_labeled_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys\n",
    "import glob\n",
    "from astropy.units import UnitsWarning\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignorar warnings comunes de librerías externas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UnitsWarning)\n",
    "print(\"🔇 Warnings silenciados: UserWarning, FutureWarning\")\n",
    "# Desactivar Rich para evitar problemas de compatibilidad\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "\n",
    "# Asegurarse de que el directorio 'src' esté en el path de Python\n",
    "src_path = Path(\"src\").resolve()\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "from src.fase1.script_4_gaia_dr3_vsx_tic_crossmatch import main as run_gaiadr3\n",
    "\n",
    "# Detectar entorno local/AWS y ejecutar el script\n",
    "try:\n",
    "    import sagemaker\n",
    "    is_sagemaker = True\n",
    "except ImportError:\n",
    "    is_sagemaker = False\n",
    "\n",
    "if is_sagemaker:\n",
    "    print(\"🔁 Ejecutando en SageMaker → catálogo completo\")\n",
    "    run_gaiadr3(limit=None, workers=4)\n",
    "else:\n",
    "    print(\"💻 Ejecutando en entorno local → catalogo completo\")\n",
    "    run_gaiadr3(limit=None, workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335110d8",
   "metadata": {},
   "source": [
    "#### ✅ **NORMALIZACION DE LOS ID OBJETO EN LOS DISTINTOS DATASETS**\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "Corregir los valores de la columna id_objeto en estos ficheros:\n",
    "\n",
    "- dataset_eb_kepler_labeled.parquet\n",
    "- dataset_eb_tess_labeled.parquet\n",
    "- dataset_k2varcat_labeled.parquet\n",
    "- dataset_vsx_tess_labeled.parquet\n",
    "\n",
    "Para que cumplan esta regla:\n",
    "\n",
    "| Misión       | Formato correcto `id_objeto` |\n",
    "| ------------ | ---------------------------- |\n",
    "| Kepler       | `KIC_<id>`                   |\n",
    "| K2 (EVEREST) | `EPIC_<id>`                  |\n",
    "| TESS         | `TIC_<id>`                   |\n",
    "| VSX-TESS     | `TIC_<id>`                   |\n",
    "\n",
    "\n",
    "Y eliminar sufijos .0, espacios o símbolos indeseados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751b2d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Procesando KEPLER → dataset_eb_kepler_labeled.parquet\n",
      "📝 Procesando batch #1...\n",
      "✅ Primera tanda procesada. Esquema: ['tiempo', 'magnitud', 'error', 'quality', 'timecorr', 'centroid_col', 'centroid_row', 'cadenceno', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'sap_quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'id_objeto', 'id_mision', 'mision', 'fecha_inicio', 'fecha_fin', 'clase_variable']\n",
      "✅ Batch #1 escrito: 50000 filas\n",
      "📝 Procesando batch #2...\n",
      "✅ Batch #2 escrito: 50000 filas\n",
      "📝 Procesando batch #3...\n",
      "✅ Batch #3 escrito: 50000 filas\n",
      "📝 Procesando batch #4...\n",
      "✅ Batch #4 escrito: 50000 filas\n",
      "📝 Procesando batch #5...\n",
      "✅ Batch #5 escrito: 50000 filas\n",
      "📝 Procesando batch #6...\n",
      "✅ Batch #6 escrito: 50000 filas\n",
      "📝 Procesando batch #7...\n",
      "✅ Batch #7 escrito: 50000 filas\n",
      "📝 Procesando batch #8...\n",
      "✅ Batch #8 escrito: 50000 filas\n",
      "📝 Procesando batch #9...\n",
      "✅ Batch #9 escrito: 50000 filas\n",
      "📝 Procesando batch #10...\n",
      "✅ Batch #10 escrito: 50000 filas\n",
      "📝 Procesando batch #11...\n",
      "✅ Batch #11 escrito: 50000 filas\n",
      "📝 Procesando batch #12...\n",
      "✅ Batch #12 escrito: 50000 filas\n",
      "📝 Procesando batch #13...\n",
      "✅ Batch #13 escrito: 50000 filas\n",
      "📝 Procesando batch #14...\n",
      "✅ Batch #14 escrito: 50000 filas\n",
      "📝 Procesando batch #15...\n",
      "✅ Batch #15 escrito: 50000 filas\n",
      "📝 Procesando batch #16...\n",
      "✅ Batch #16 escrito: 50000 filas\n",
      "📝 Procesando batch #17...\n",
      "✅ Batch #17 escrito: 50000 filas\n",
      "📝 Procesando batch #18...\n",
      "✅ Batch #18 escrito: 50000 filas\n",
      "📝 Procesando batch #19...\n",
      "✅ Batch #19 escrito: 50000 filas\n",
      "📝 Procesando batch #20...\n",
      "✅ Batch #20 escrito: 50000 filas\n",
      "📝 Procesando batch #21...\n",
      "✅ Batch #21 escrito: 48576 filas\n",
      "📝 Procesando batch #22...\n",
      "✅ Batch #22 escrito: 50000 filas\n",
      "📝 Procesando batch #23...\n",
      "✅ Batch #23 escrito: 50000 filas\n",
      "📝 Procesando batch #24...\n",
      "✅ Batch #24 escrito: 50000 filas\n",
      "📝 Procesando batch #25...\n",
      "✅ Batch #25 escrito: 50000 filas\n",
      "📝 Procesando batch #26...\n",
      "✅ Batch #26 escrito: 50000 filas\n",
      "📝 Procesando batch #27...\n",
      "✅ Batch #27 escrito: 50000 filas\n",
      "📝 Procesando batch #28...\n",
      "✅ Batch #28 escrito: 50000 filas\n",
      "📝 Procesando batch #29...\n",
      "✅ Batch #29 escrito: 50000 filas\n",
      "📝 Procesando batch #30...\n",
      "✅ Batch #30 escrito: 50000 filas\n",
      "📝 Procesando batch #31...\n",
      "✅ Batch #31 escrito: 50000 filas\n",
      "📝 Procesando batch #32...\n",
      "✅ Batch #32 escrito: 50000 filas\n",
      "📝 Procesando batch #33...\n",
      "✅ Batch #33 escrito: 50000 filas\n",
      "📝 Procesando batch #34...\n",
      "✅ Batch #34 escrito: 50000 filas\n",
      "📝 Procesando batch #35...\n",
      "✅ Batch #35 escrito: 50000 filas\n",
      "📝 Procesando batch #36...\n",
      "✅ Batch #36 escrito: 50000 filas\n",
      "📝 Procesando batch #37...\n",
      "✅ Batch #37 escrito: 50000 filas\n",
      "📝 Procesando batch #38...\n",
      "✅ Batch #38 escrito: 50000 filas\n",
      "📝 Procesando batch #39...\n",
      "✅ Batch #39 escrito: 50000 filas\n",
      "📝 Procesando batch #40...\n",
      "✅ Batch #40 escrito: 23714 filas\n",
      "📝 Procesando batch #41...\n",
      "✅ Batch #41 escrito: 50000 filas\n",
      "📝 Procesando batch #42...\n",
      "✅ Batch #42 escrito: 50000 filas\n",
      "📝 Procesando batch #43...\n",
      "✅ Batch #43 escrito: 50000 filas\n",
      "📝 Procesando batch #44...\n",
      "✅ Batch #44 escrito: 50000 filas\n",
      "📝 Procesando batch #45...\n",
      "✅ Batch #45 escrito: 50000 filas\n",
      "📝 Procesando batch #46...\n",
      "✅ Batch #46 escrito: 50000 filas\n",
      "📝 Procesando batch #47...\n",
      "✅ Batch #47 escrito: 50000 filas\n",
      "📝 Procesando batch #48...\n",
      "✅ Batch #48 escrito: 50000 filas\n",
      "📝 Procesando batch #49...\n",
      "✅ Batch #49 escrito: 50000 filas\n",
      "📝 Procesando batch #50...\n",
      "✅ Batch #50 escrito: 50000 filas\n",
      "📝 Procesando batch #51...\n",
      "✅ Batch #51 escrito: 50000 filas\n",
      "📝 Procesando batch #52...\n",
      "✅ Batch #52 escrito: 50000 filas\n",
      "📝 Procesando batch #53...\n",
      "✅ Batch #53 escrito: 50000 filas\n",
      "📝 Procesando batch #54...\n",
      "✅ Batch #54 escrito: 50000 filas\n",
      "📝 Procesando batch #55...\n",
      "✅ Batch #55 escrito: 50000 filas\n",
      "📝 Procesando batch #56...\n",
      "✅ Batch #56 escrito: 50000 filas\n",
      "📝 Procesando batch #57...\n",
      "✅ Batch #57 escrito: 50000 filas\n",
      "📝 Procesando batch #58...\n",
      "✅ Batch #58 escrito: 50000 filas\n",
      "📝 Procesando batch #59...\n",
      "✅ Batch #59 escrito: 50000 filas\n",
      "📝 Procesando batch #60...\n",
      "✅ Batch #60 escrito: 50000 filas\n",
      "📝 Procesando batch #61...\n",
      "✅ Batch #61 escrito: 48576 filas\n",
      "📝 Procesando batch #62...\n",
      "✅ Batch #62 escrito: 50000 filas\n",
      "📝 Procesando batch #63...\n",
      "✅ Batch #63 escrito: 50000 filas\n",
      "📝 Procesando batch #64...\n",
      "✅ Batch #64 escrito: 50000 filas\n",
      "📝 Procesando batch #65...\n",
      "✅ Batch #65 escrito: 50000 filas\n",
      "📝 Procesando batch #66...\n",
      "✅ Batch #66 escrito: 50000 filas\n",
      "📝 Procesando batch #67...\n",
      "✅ Batch #67 escrito: 50000 filas\n",
      "📝 Procesando batch #68...\n",
      "✅ Batch #68 escrito: 50000 filas\n",
      "📝 Procesando batch #69...\n",
      "✅ Batch #69 escrito: 50000 filas\n",
      "📝 Procesando batch #70...\n",
      "✅ Batch #70 escrito: 50000 filas\n",
      "📝 Procesando batch #71...\n",
      "✅ Batch #71 escrito: 50000 filas\n",
      "📝 Procesando batch #72...\n",
      "✅ Batch #72 escrito: 50000 filas\n",
      "📝 Procesando batch #73...\n",
      "✅ Batch #73 escrito: 50000 filas\n",
      "📝 Procesando batch #74...\n",
      "✅ Batch #74 escrito: 50000 filas\n",
      "📝 Procesando batch #75...\n",
      "✅ Batch #75 escrito: 50000 filas\n",
      "📝 Procesando batch #76...\n",
      "✅ Batch #76 escrito: 50000 filas\n",
      "📝 Procesando batch #77...\n",
      "✅ Batch #77 escrito: 50000 filas\n",
      "📝 Procesando batch #78...\n",
      "✅ Batch #78 escrito: 50000 filas\n",
      "📝 Procesando batch #79...\n",
      "✅ Batch #79 escrito: 50000 filas\n",
      "📝 Procesando batch #80...\n",
      "✅ Batch #80 escrito: 50000 filas\n",
      "📝 Procesando batch #81...\n",
      "✅ Batch #81 escrito: 24476 filas\n",
      "📝 Procesando batch #82...\n",
      "✅ Batch #82 escrito: 50000 filas\n",
      "📝 Procesando batch #83...\n",
      "✅ Batch #83 escrito: 50000 filas\n",
      "📝 Procesando batch #84...\n",
      "✅ Batch #84 escrito: 50000 filas\n",
      "📝 Procesando batch #85...\n",
      "✅ Batch #85 escrito: 50000 filas\n",
      "📝 Procesando batch #86...\n",
      "✅ Batch #86 escrito: 50000 filas\n",
      "📝 Procesando batch #87...\n",
      "✅ Batch #87 escrito: 50000 filas\n",
      "📝 Procesando batch #88...\n",
      "✅ Batch #88 escrito: 50000 filas\n",
      "📝 Procesando batch #89...\n",
      "✅ Batch #89 escrito: 50000 filas\n",
      "📝 Procesando batch #90...\n",
      "✅ Batch #90 escrito: 50000 filas\n",
      "📝 Procesando batch #91...\n",
      "✅ Batch #91 escrito: 50000 filas\n",
      "📝 Procesando batch #92...\n",
      "✅ Batch #92 escrito: 50000 filas\n",
      "📝 Procesando batch #93...\n",
      "✅ Batch #93 escrito: 50000 filas\n",
      "📝 Procesando batch #94...\n",
      "✅ Batch #94 escrito: 50000 filas\n",
      "📝 Procesando batch #95...\n",
      "✅ Batch #95 escrito: 50000 filas\n",
      "📝 Procesando batch #96...\n",
      "✅ Batch #96 escrito: 50000 filas\n",
      "📝 Procesando batch #97...\n",
      "✅ Batch #97 escrito: 50000 filas\n",
      "📝 Procesando batch #98...\n",
      "✅ Batch #98 escrito: 50000 filas\n",
      "📝 Procesando batch #99...\n",
      "✅ Batch #99 escrito: 50000 filas\n",
      "📝 Procesando batch #100...\n",
      "✅ Batch #100 escrito: 50000 filas\n",
      "📝 Procesando batch #101...\n",
      "✅ Batch #101 escrito: 50000 filas\n",
      "📝 Procesando batch #102...\n",
      "✅ Batch #102 escrito: 48576 filas\n",
      "📝 Procesando batch #103...\n",
      "✅ Batch #103 escrito: 50000 filas\n",
      "📝 Procesando batch #104...\n",
      "✅ Batch #104 escrito: 50000 filas\n",
      "📝 Procesando batch #105...\n",
      "✅ Batch #105 escrito: 50000 filas\n",
      "📝 Procesando batch #106...\n",
      "✅ Batch #106 escrito: 50000 filas\n",
      "📝 Procesando batch #107...\n",
      "✅ Batch #107 escrito: 50000 filas\n",
      "📝 Procesando batch #108...\n",
      "✅ Batch #108 escrito: 50000 filas\n",
      "📝 Procesando batch #109...\n",
      "✅ Batch #109 escrito: 50000 filas\n",
      "📝 Procesando batch #110...\n",
      "✅ Batch #110 escrito: 50000 filas\n",
      "📝 Procesando batch #111...\n",
      "✅ Batch #111 escrito: 50000 filas\n",
      "📝 Procesando batch #112...\n",
      "✅ Batch #112 escrito: 50000 filas\n",
      "📝 Procesando batch #113...\n",
      "✅ Batch #113 escrito: 50000 filas\n",
      "📝 Procesando batch #114...\n",
      "✅ Batch #114 escrito: 50000 filas\n",
      "📝 Procesando batch #115...\n",
      "✅ Batch #115 escrito: 50000 filas\n",
      "📝 Procesando batch #116...\n",
      "✅ Batch #116 escrito: 50000 filas\n",
      "📝 Procesando batch #117...\n",
      "✅ Batch #117 escrito: 50000 filas\n",
      "📝 Procesando batch #118...\n",
      "✅ Batch #118 escrito: 50000 filas\n",
      "📝 Procesando batch #119...\n",
      "✅ Batch #119 escrito: 50000 filas\n",
      "📝 Procesando batch #120...\n",
      "✅ Batch #120 escrito: 46201 filas\n",
      "📝 Procesando batch #121...\n",
      "✅ Batch #121 escrito: 50000 filas\n",
      "📝 Procesando batch #122...\n",
      "✅ Batch #122 escrito: 50000 filas\n",
      "📝 Procesando batch #123...\n",
      "✅ Batch #123 escrito: 50000 filas\n",
      "📝 Procesando batch #124...\n",
      "✅ Batch #124 escrito: 50000 filas\n",
      "📝 Procesando batch #125...\n",
      "✅ Batch #125 escrito: 50000 filas\n",
      "📝 Procesando batch #126...\n",
      "✅ Batch #126 escrito: 50000 filas\n",
      "📝 Procesando batch #127...\n",
      "✅ Batch #127 escrito: 50000 filas\n",
      "📝 Procesando batch #128...\n",
      "✅ Batch #128 escrito: 50000 filas\n",
      "📝 Procesando batch #129...\n",
      "✅ Batch #129 escrito: 50000 filas\n",
      "📝 Procesando batch #130...\n",
      "✅ Batch #130 escrito: 50000 filas\n",
      "📝 Procesando batch #131...\n",
      "✅ Batch #131 escrito: 50000 filas\n",
      "📝 Procesando batch #132...\n",
      "✅ Batch #132 escrito: 50000 filas\n",
      "📝 Procesando batch #133...\n",
      "✅ Batch #133 escrito: 50000 filas\n",
      "📝 Procesando batch #134...\n",
      "✅ Batch #134 escrito: 50000 filas\n",
      "📝 Procesando batch #135...\n",
      "✅ Batch #135 escrito: 50000 filas\n",
      "📝 Procesando batch #136...\n",
      "✅ Batch #136 escrito: 50000 filas\n",
      "📝 Procesando batch #137...\n",
      "✅ Batch #137 escrito: 50000 filas\n",
      "📝 Procesando batch #138...\n",
      "✅ Batch #138 escrito: 50000 filas\n",
      "📝 Procesando batch #139...\n",
      "✅ Batch #139 escrito: 50000 filas\n",
      "📝 Procesando batch #140...\n",
      "✅ Batch #140 escrito: 50000 filas\n",
      "📝 Procesando batch #141...\n",
      "✅ Batch #141 escrito: 48576 filas\n",
      "📝 Procesando batch #142...\n",
      "✅ Batch #142 escrito: 50000 filas\n",
      "📝 Procesando batch #143...\n",
      "✅ Batch #143 escrito: 50000 filas\n",
      "📝 Procesando batch #144...\n",
      "✅ Batch #144 escrito: 50000 filas\n",
      "📝 Procesando batch #145...\n",
      "✅ Batch #145 escrito: 50000 filas\n",
      "📝 Procesando batch #146...\n",
      "✅ Batch #146 escrito: 50000 filas\n",
      "📝 Procesando batch #147...\n",
      "✅ Batch #147 escrito: 50000 filas\n",
      "📝 Procesando batch #148...\n",
      "✅ Batch #148 escrito: 50000 filas\n",
      "📝 Procesando batch #149...\n",
      "✅ Batch #149 escrito: 50000 filas\n",
      "📝 Procesando batch #150...\n",
      "✅ Batch #150 escrito: 50000 filas\n",
      "📝 Procesando batch #151...\n",
      "✅ Batch #151 escrito: 50000 filas\n",
      "📝 Procesando batch #152...\n",
      "✅ Batch #152 escrito: 50000 filas\n",
      "📝 Procesando batch #153...\n",
      "✅ Batch #153 escrito: 50000 filas\n",
      "📝 Procesando batch #154...\n",
      "✅ Batch #154 escrito: 50000 filas\n",
      "📝 Procesando batch #155...\n",
      "✅ Batch #155 escrito: 50000 filas\n",
      "📝 Procesando batch #156...\n",
      "✅ Batch #156 escrito: 50000 filas\n",
      "📝 Procesando batch #157...\n",
      "✅ Batch #157 escrito: 50000 filas\n",
      "📝 Procesando batch #158...\n",
      "✅ Batch #158 escrito: 50000 filas\n",
      "📝 Procesando batch #159...\n",
      "✅ Batch #159 escrito: 50000 filas\n",
      "📝 Procesando batch #160...\n",
      "✅ Batch #160 escrito: 14762 filas\n",
      "📝 Procesando batch #161...\n",
      "✅ Batch #161 escrito: 50000 filas\n",
      "📝 Procesando batch #162...\n",
      "✅ Batch #162 escrito: 50000 filas\n",
      "📝 Procesando batch #163...\n",
      "✅ Batch #163 escrito: 50000 filas\n",
      "📝 Procesando batch #164...\n",
      "✅ Batch #164 escrito: 50000 filas\n",
      "📝 Procesando batch #165...\n",
      "✅ Batch #165 escrito: 50000 filas\n",
      "📝 Procesando batch #166...\n",
      "✅ Batch #166 escrito: 50000 filas\n",
      "📝 Procesando batch #167...\n",
      "✅ Batch #167 escrito: 50000 filas\n",
      "📝 Procesando batch #168...\n",
      "✅ Batch #168 escrito: 50000 filas\n",
      "📝 Procesando batch #169...\n",
      "✅ Batch #169 escrito: 50000 filas\n",
      "📝 Procesando batch #170...\n",
      "✅ Batch #170 escrito: 50000 filas\n",
      "📝 Procesando batch #171...\n",
      "✅ Batch #171 escrito: 50000 filas\n",
      "📝 Procesando batch #172...\n",
      "✅ Batch #172 escrito: 50000 filas\n",
      "📝 Procesando batch #173...\n",
      "✅ Batch #173 escrito: 50000 filas\n",
      "📝 Procesando batch #174...\n",
      "✅ Batch #174 escrito: 50000 filas\n",
      "📝 Procesando batch #175...\n",
      "✅ Batch #175 escrito: 50000 filas\n",
      "📝 Procesando batch #176...\n",
      "✅ Batch #176 escrito: 50000 filas\n",
      "📝 Procesando batch #177...\n",
      "✅ Batch #177 escrito: 50000 filas\n",
      "📝 Procesando batch #178...\n",
      "✅ Batch #178 escrito: 50000 filas\n",
      "📝 Procesando batch #179...\n",
      "✅ Batch #179 escrito: 50000 filas\n",
      "📝 Procesando batch #180...\n",
      "✅ Batch #180 escrito: 50000 filas\n",
      "📝 Procesando batch #181...\n",
      "✅ Batch #181 escrito: 48576 filas\n",
      "📝 Procesando batch #182...\n",
      "✅ Batch #182 escrito: 50000 filas\n",
      "📝 Procesando batch #183...\n",
      "✅ Batch #183 escrito: 50000 filas\n",
      "📝 Procesando batch #184...\n",
      "✅ Batch #184 escrito: 50000 filas\n",
      "📝 Procesando batch #185...\n",
      "✅ Batch #185 escrito: 50000 filas\n",
      "📝 Procesando batch #186...\n",
      "✅ Batch #186 escrito: 50000 filas\n",
      "📝 Procesando batch #187...\n",
      "✅ Batch #187 escrito: 50000 filas\n",
      "📝 Procesando batch #188...\n",
      "✅ Batch #188 escrito: 50000 filas\n",
      "📝 Procesando batch #189...\n",
      "✅ Batch #189 escrito: 50000 filas\n",
      "📝 Procesando batch #190...\n",
      "✅ Batch #190 escrito: 50000 filas\n",
      "📝 Procesando batch #191...\n",
      "✅ Batch #191 escrito: 50000 filas\n",
      "📝 Procesando batch #192...\n",
      "✅ Batch #192 escrito: 50000 filas\n",
      "📝 Procesando batch #193...\n",
      "✅ Batch #193 escrito: 50000 filas\n",
      "📝 Procesando batch #194...\n",
      "✅ Batch #194 escrito: 50000 filas\n",
      "📝 Procesando batch #195...\n",
      "✅ Batch #195 escrito: 50000 filas\n",
      "📝 Procesando batch #196...\n",
      "✅ Batch #196 escrito: 50000 filas\n",
      "📝 Procesando batch #197...\n",
      "✅ Batch #197 escrito: 50000 filas\n",
      "📝 Procesando batch #198...\n",
      "✅ Batch #198 escrito: 50000 filas\n",
      "📝 Procesando batch #199...\n",
      "✅ Batch #199 escrito: 11133 filas\n",
      "📝 Procesando batch #200...\n",
      "✅ Batch #200 escrito: 50000 filas\n",
      "📝 Procesando batch #201...\n",
      "✅ Batch #201 escrito: 50000 filas\n",
      "📝 Procesando batch #202...\n",
      "✅ Batch #202 escrito: 50000 filas\n",
      "📝 Procesando batch #203...\n",
      "✅ Batch #203 escrito: 50000 filas\n",
      "📝 Procesando batch #204...\n",
      "✅ Batch #204 escrito: 50000 filas\n",
      "📝 Procesando batch #205...\n",
      "✅ Batch #205 escrito: 50000 filas\n",
      "📝 Procesando batch #206...\n",
      "✅ Batch #206 escrito: 50000 filas\n",
      "📝 Procesando batch #207...\n",
      "✅ Batch #207 escrito: 50000 filas\n",
      "📝 Procesando batch #208...\n",
      "✅ Batch #208 escrito: 50000 filas\n",
      "📝 Procesando batch #209...\n",
      "✅ Batch #209 escrito: 50000 filas\n",
      "📝 Procesando batch #210...\n",
      "✅ Batch #210 escrito: 50000 filas\n",
      "📝 Procesando batch #211...\n",
      "✅ Batch #211 escrito: 50000 filas\n",
      "📝 Procesando batch #212...\n",
      "✅ Batch #212 escrito: 50000 filas\n",
      "📝 Procesando batch #213...\n",
      "✅ Batch #213 escrito: 50000 filas\n",
      "📝 Procesando batch #214...\n",
      "✅ Batch #214 escrito: 50000 filas\n",
      "📝 Procesando batch #215...\n",
      "✅ Batch #215 escrito: 50000 filas\n",
      "📝 Procesando batch #216...\n",
      "✅ Batch #216 escrito: 50000 filas\n",
      "📝 Procesando batch #217...\n",
      "✅ Batch #217 escrito: 50000 filas\n",
      "📝 Procesando batch #218...\n",
      "✅ Batch #218 escrito: 50000 filas\n",
      "📝 Procesando batch #219...\n",
      "✅ Batch #219 escrito: 50000 filas\n",
      "📝 Procesando batch #220...\n",
      "✅ Batch #220 escrito: 48576 filas\n",
      "📝 Procesando batch #221...\n",
      "✅ Batch #221 escrito: 50000 filas\n",
      "📝 Procesando batch #222...\n",
      "✅ Batch #222 escrito: 50000 filas\n",
      "📝 Procesando batch #223...\n",
      "✅ Batch #223 escrito: 50000 filas\n",
      "📝 Procesando batch #224...\n",
      "✅ Batch #224 escrito: 50000 filas\n",
      "📝 Procesando batch #225...\n",
      "✅ Batch #225 escrito: 50000 filas\n",
      "📝 Procesando batch #226...\n",
      "✅ Batch #226 escrito: 50000 filas\n",
      "📝 Procesando batch #227...\n",
      "✅ Batch #227 escrito: 50000 filas\n",
      "📝 Procesando batch #228...\n",
      "✅ Batch #228 escrito: 50000 filas\n",
      "📝 Procesando batch #229...\n",
      "✅ Batch #229 escrito: 50000 filas\n",
      "📝 Procesando batch #230...\n",
      "✅ Batch #230 escrito: 50000 filas\n",
      "📝 Procesando batch #231...\n",
      "✅ Batch #231 escrito: 50000 filas\n",
      "📝 Procesando batch #232...\n",
      "✅ Batch #232 escrito: 50000 filas\n",
      "📝 Procesando batch #233...\n",
      "✅ Batch #233 escrito: 50000 filas\n",
      "📝 Procesando batch #234...\n",
      "✅ Batch #234 escrito: 50000 filas\n",
      "📝 Procesando batch #235...\n",
      "✅ Batch #235 escrito: 50000 filas\n",
      "📝 Procesando batch #236...\n",
      "✅ Batch #236 escrito: 50000 filas\n",
      "📝 Procesando batch #237...\n",
      "✅ Batch #237 escrito: 17815 filas\n",
      "📝 Procesando batch #238...\n",
      "✅ Batch #238 escrito: 50000 filas\n",
      "📝 Procesando batch #239...\n",
      "✅ Batch #239 escrito: 50000 filas\n",
      "📝 Procesando batch #240...\n",
      "✅ Batch #240 escrito: 50000 filas\n",
      "📝 Procesando batch #241...\n",
      "✅ Batch #241 escrito: 50000 filas\n",
      "📝 Procesando batch #242...\n",
      "✅ Batch #242 escrito: 50000 filas\n",
      "📝 Procesando batch #243...\n",
      "✅ Batch #243 escrito: 50000 filas\n",
      "📝 Procesando batch #244...\n",
      "✅ Batch #244 escrito: 50000 filas\n",
      "📝 Procesando batch #245...\n",
      "✅ Batch #245 escrito: 50000 filas\n",
      "📝 Procesando batch #246...\n",
      "✅ Batch #246 escrito: 50000 filas\n",
      "📝 Procesando batch #247...\n",
      "✅ Batch #247 escrito: 50000 filas\n",
      "📝 Procesando batch #248...\n",
      "✅ Batch #248 escrito: 50000 filas\n",
      "📝 Procesando batch #249...\n",
      "✅ Batch #249 escrito: 50000 filas\n",
      "📝 Procesando batch #250...\n",
      "✅ Batch #250 escrito: 50000 filas\n",
      "📝 Procesando batch #251...\n",
      "✅ Batch #251 escrito: 50000 filas\n",
      "📝 Procesando batch #252...\n",
      "✅ Batch #252 escrito: 50000 filas\n",
      "📝 Procesando batch #253...\n",
      "✅ Batch #253 escrito: 50000 filas\n",
      "📝 Procesando batch #254...\n",
      "✅ Batch #254 escrito: 50000 filas\n",
      "📝 Procesando batch #255...\n",
      "✅ Batch #255 escrito: 50000 filas\n",
      "📝 Procesando batch #256...\n",
      "✅ Batch #256 escrito: 50000 filas\n",
      "📝 Procesando batch #257...\n",
      "✅ Batch #257 escrito: 50000 filas\n",
      "📝 Procesando batch #258...\n",
      "✅ Batch #258 escrito: 48576 filas\n",
      "📝 Procesando batch #259...\n",
      "✅ Batch #259 escrito: 50000 filas\n",
      "📝 Procesando batch #260...\n",
      "✅ Batch #260 escrito: 50000 filas\n",
      "📝 Procesando batch #261...\n",
      "✅ Batch #261 escrito: 50000 filas\n",
      "📝 Procesando batch #262...\n",
      "✅ Batch #262 escrito: 50000 filas\n",
      "📝 Procesando batch #263...\n",
      "✅ Batch #263 escrito: 50000 filas\n",
      "📝 Procesando batch #264...\n",
      "✅ Batch #264 escrito: 50000 filas\n",
      "📝 Procesando batch #265...\n",
      "✅ Batch #265 escrito: 50000 filas\n",
      "📝 Procesando batch #266...\n",
      "✅ Batch #266 escrito: 50000 filas\n",
      "📝 Procesando batch #267...\n",
      "✅ Batch #267 escrito: 50000 filas\n",
      "📝 Procesando batch #268...\n",
      "✅ Batch #268 escrito: 50000 filas\n",
      "📝 Procesando batch #269...\n",
      "✅ Batch #269 escrito: 50000 filas\n",
      "📝 Procesando batch #270...\n",
      "✅ Batch #270 escrito: 50000 filas\n",
      "📝 Procesando batch #271...\n",
      "✅ Batch #271 escrito: 50000 filas\n",
      "📝 Procesando batch #272...\n",
      "✅ Batch #272 escrito: 50000 filas\n",
      "📝 Procesando batch #273...\n",
      "✅ Batch #273 escrito: 50000 filas\n",
      "📝 Procesando batch #274...\n",
      "✅ Batch #274 escrito: 50000 filas\n",
      "📝 Procesando batch #275...\n",
      "✅ Batch #275 escrito: 4847 filas\n",
      "📝 Procesando batch #276...\n",
      "✅ Batch #276 escrito: 50000 filas\n",
      "📝 Procesando batch #277...\n",
      "✅ Batch #277 escrito: 50000 filas\n",
      "📝 Procesando batch #278...\n",
      "✅ Batch #278 escrito: 50000 filas\n",
      "📝 Procesando batch #279...\n",
      "✅ Batch #279 escrito: 50000 filas\n",
      "📝 Procesando batch #280...\n",
      "✅ Batch #280 escrito: 50000 filas\n",
      "📝 Procesando batch #281...\n",
      "✅ Batch #281 escrito: 50000 filas\n",
      "📝 Procesando batch #282...\n",
      "✅ Batch #282 escrito: 50000 filas\n",
      "📝 Procesando batch #283...\n",
      "✅ Batch #283 escrito: 50000 filas\n",
      "📝 Procesando batch #284...\n",
      "✅ Batch #284 escrito: 50000 filas\n",
      "📝 Procesando batch #285...\n",
      "✅ Batch #285 escrito: 50000 filas\n",
      "📝 Procesando batch #286...\n",
      "✅ Batch #286 escrito: 50000 filas\n",
      "📝 Procesando batch #287...\n",
      "✅ Batch #287 escrito: 50000 filas\n",
      "📝 Procesando batch #288...\n",
      "✅ Batch #288 escrito: 50000 filas\n",
      "📝 Procesando batch #289...\n",
      "✅ Batch #289 escrito: 50000 filas\n",
      "📝 Procesando batch #290...\n",
      "✅ Batch #290 escrito: 50000 filas\n",
      "📝 Procesando batch #291...\n",
      "✅ Batch #291 escrito: 50000 filas\n",
      "📝 Procesando batch #292...\n",
      "✅ Batch #292 escrito: 50000 filas\n",
      "📝 Procesando batch #293...\n",
      "✅ Batch #293 escrito: 50000 filas\n",
      "📝 Procesando batch #294...\n",
      "✅ Batch #294 escrito: 50000 filas\n",
      "📝 Procesando batch #295...\n",
      "✅ Batch #295 escrito: 50000 filas\n",
      "📝 Procesando batch #296...\n",
      "✅ Batch #296 escrito: 48576 filas\n",
      "📝 Procesando batch #297...\n",
      "✅ Batch #297 escrito: 50000 filas\n",
      "📝 Procesando batch #298...\n",
      "✅ Batch #298 escrito: 50000 filas\n",
      "📝 Procesando batch #299...\n",
      "✅ Batch #299 escrito: 50000 filas\n",
      "📝 Procesando batch #300...\n",
      "✅ Batch #300 escrito: 50000 filas\n",
      "📝 Procesando batch #301...\n",
      "✅ Batch #301 escrito: 50000 filas\n",
      "📝 Procesando batch #302...\n",
      "✅ Batch #302 escrito: 50000 filas\n",
      "📝 Procesando batch #303...\n",
      "✅ Batch #303 escrito: 50000 filas\n",
      "📝 Procesando batch #304...\n",
      "✅ Batch #304 escrito: 50000 filas\n",
      "📝 Procesando batch #305...\n",
      "✅ Batch #305 escrito: 50000 filas\n",
      "📝 Procesando batch #306...\n",
      "✅ Batch #306 escrito: 50000 filas\n",
      "📝 Procesando batch #307...\n",
      "✅ Batch #307 escrito: 50000 filas\n",
      "📝 Procesando batch #308...\n",
      "✅ Batch #308 escrito: 50000 filas\n",
      "📝 Procesando batch #309...\n",
      "✅ Batch #309 escrito: 50000 filas\n",
      "📝 Procesando batch #310...\n",
      "✅ Batch #310 escrito: 50000 filas\n",
      "📝 Procesando batch #311...\n",
      "✅ Batch #311 escrito: 50000 filas\n",
      "📝 Procesando batch #312...\n",
      "✅ Batch #312 escrito: 50000 filas\n",
      "📝 Procesando batch #313...\n",
      "✅ Batch #313 escrito: 24217 filas\n",
      "📝 Procesando batch #314...\n",
      "✅ Batch #314 escrito: 50000 filas\n",
      "📝 Procesando batch #315...\n",
      "✅ Batch #315 escrito: 50000 filas\n",
      "📝 Procesando batch #316...\n",
      "✅ Batch #316 escrito: 50000 filas\n",
      "📝 Procesando batch #317...\n",
      "✅ Batch #317 escrito: 50000 filas\n",
      "📝 Procesando batch #318...\n",
      "✅ Batch #318 escrito: 50000 filas\n",
      "📝 Procesando batch #319...\n",
      "✅ Batch #319 escrito: 50000 filas\n",
      "📝 Procesando batch #320...\n",
      "✅ Batch #320 escrito: 50000 filas\n",
      "📝 Procesando batch #321...\n",
      "✅ Batch #321 escrito: 50000 filas\n",
      "📝 Procesando batch #322...\n",
      "✅ Batch #322 escrito: 50000 filas\n",
      "📝 Procesando batch #323...\n",
      "✅ Batch #323 escrito: 50000 filas\n",
      "📝 Procesando batch #324...\n",
      "✅ Batch #324 escrito: 50000 filas\n",
      "📝 Procesando batch #325...\n",
      "✅ Batch #325 escrito: 50000 filas\n",
      "📝 Procesando batch #326...\n",
      "✅ Batch #326 escrito: 50000 filas\n",
      "📝 Procesando batch #327...\n",
      "✅ Batch #327 escrito: 50000 filas\n",
      "📝 Procesando batch #328...\n",
      "✅ Batch #328 escrito: 50000 filas\n",
      "📝 Procesando batch #329...\n",
      "✅ Batch #329 escrito: 50000 filas\n",
      "📝 Procesando batch #330...\n",
      "✅ Batch #330 escrito: 50000 filas\n",
      "📝 Procesando batch #331...\n",
      "✅ Batch #331 escrito: 50000 filas\n",
      "📝 Procesando batch #332...\n",
      "✅ Batch #332 escrito: 50000 filas\n",
      "📝 Procesando batch #333...\n",
      "✅ Batch #333 escrito: 50000 filas\n",
      "📝 Procesando batch #334...\n",
      "✅ Batch #334 escrito: 48576 filas\n",
      "📝 Procesando batch #335...\n",
      "✅ Batch #335 escrito: 50000 filas\n",
      "📝 Procesando batch #336...\n",
      "✅ Batch #336 escrito: 50000 filas\n",
      "📝 Procesando batch #337...\n",
      "✅ Batch #337 escrito: 50000 filas\n",
      "📝 Procesando batch #338...\n",
      "✅ Batch #338 escrito: 50000 filas\n",
      "📝 Procesando batch #339...\n",
      "✅ Batch #339 escrito: 50000 filas\n",
      "📝 Procesando batch #340...\n",
      "✅ Batch #340 escrito: 50000 filas\n",
      "📝 Procesando batch #341...\n",
      "✅ Batch #341 escrito: 50000 filas\n",
      "📝 Procesando batch #342...\n",
      "✅ Batch #342 escrito: 50000 filas\n",
      "📝 Procesando batch #343...\n",
      "✅ Batch #343 escrito: 50000 filas\n",
      "📝 Procesando batch #344...\n",
      "✅ Batch #344 escrito: 50000 filas\n",
      "📝 Procesando batch #345...\n",
      "✅ Batch #345 escrito: 50000 filas\n",
      "📝 Procesando batch #346...\n",
      "✅ Batch #346 escrito: 50000 filas\n",
      "📝 Procesando batch #347...\n",
      "✅ Batch #347 escrito: 50000 filas\n",
      "📝 Procesando batch #348...\n",
      "✅ Batch #348 escrito: 50000 filas\n",
      "📝 Procesando batch #349...\n",
      "✅ Batch #349 escrito: 50000 filas\n",
      "📝 Procesando batch #350...\n",
      "✅ Batch #350 escrito: 46202 filas\n",
      "📝 Procesando batch #351...\n",
      "✅ Batch #351 escrito: 50000 filas\n",
      "📝 Procesando batch #352...\n",
      "✅ Batch #352 escrito: 50000 filas\n",
      "📝 Procesando batch #353...\n",
      "✅ Batch #353 escrito: 50000 filas\n",
      "📝 Procesando batch #354...\n",
      "✅ Batch #354 escrito: 50000 filas\n",
      "📝 Procesando batch #355...\n",
      "✅ Batch #355 escrito: 50000 filas\n",
      "📝 Procesando batch #356...\n",
      "✅ Batch #356 escrito: 50000 filas\n",
      "📝 Procesando batch #357...\n",
      "✅ Batch #357 escrito: 50000 filas\n",
      "📝 Procesando batch #358...\n",
      "✅ Batch #358 escrito: 50000 filas\n",
      "📝 Procesando batch #359...\n",
      "✅ Batch #359 escrito: 50000 filas\n",
      "📝 Procesando batch #360...\n",
      "✅ Batch #360 escrito: 50000 filas\n",
      "📝 Procesando batch #361...\n",
      "✅ Batch #361 escrito: 50000 filas\n",
      "📝 Procesando batch #362...\n",
      "✅ Batch #362 escrito: 50000 filas\n",
      "📝 Procesando batch #363...\n",
      "✅ Batch #363 escrito: 50000 filas\n",
      "📝 Procesando batch #364...\n",
      "✅ Batch #364 escrito: 50000 filas\n",
      "📝 Procesando batch #365...\n",
      "✅ Batch #365 escrito: 50000 filas\n",
      "📝 Procesando batch #366...\n",
      "✅ Batch #366 escrito: 50000 filas\n",
      "📝 Procesando batch #367...\n",
      "✅ Batch #367 escrito: 50000 filas\n",
      "📝 Procesando batch #368...\n",
      "✅ Batch #368 escrito: 50000 filas\n",
      "📝 Procesando batch #369...\n",
      "✅ Batch #369 escrito: 50000 filas\n",
      "📝 Procesando batch #370...\n",
      "✅ Batch #370 escrito: 50000 filas\n",
      "📝 Procesando batch #371...\n",
      "✅ Batch #371 escrito: 48576 filas\n",
      "📝 Procesando batch #372...\n",
      "✅ Batch #372 escrito: 50000 filas\n",
      "📝 Procesando batch #373...\n",
      "✅ Batch #373 escrito: 50000 filas\n",
      "📝 Procesando batch #374...\n",
      "✅ Batch #374 escrito: 50000 filas\n",
      "📝 Procesando batch #375...\n",
      "✅ Batch #375 escrito: 50000 filas\n",
      "📝 Procesando batch #376...\n",
      "✅ Batch #376 escrito: 50000 filas\n",
      "📝 Procesando batch #377...\n",
      "✅ Batch #377 escrito: 50000 filas\n",
      "📝 Procesando batch #378...\n",
      "✅ Batch #378 escrito: 50000 filas\n",
      "📝 Procesando batch #379...\n",
      "✅ Batch #379 escrito: 50000 filas\n",
      "📝 Procesando batch #380...\n",
      "✅ Batch #380 escrito: 50000 filas\n",
      "📝 Procesando batch #381...\n",
      "✅ Batch #381 escrito: 50000 filas\n",
      "📝 Procesando batch #382...\n",
      "✅ Batch #382 escrito: 50000 filas\n",
      "📝 Procesando batch #383...\n",
      "✅ Batch #383 escrito: 50000 filas\n",
      "📝 Procesando batch #384...\n",
      "✅ Batch #384 escrito: 50000 filas\n",
      "📝 Procesando batch #385...\n",
      "✅ Batch #385 escrito: 50000 filas\n",
      "📝 Procesando batch #386...\n",
      "✅ Batch #386 escrito: 50000 filas\n",
      "📝 Procesando batch #387...\n",
      "✅ Batch #387 escrito: 26902 filas\n",
      "📝 Procesando batch #388...\n",
      "✅ Batch #388 escrito: 50000 filas\n",
      "📝 Procesando batch #389...\n",
      "✅ Batch #389 escrito: 50000 filas\n",
      "📝 Procesando batch #390...\n",
      "✅ Batch #390 escrito: 50000 filas\n",
      "📝 Procesando batch #391...\n",
      "✅ Batch #391 escrito: 50000 filas\n",
      "📝 Procesando batch #392...\n",
      "✅ Batch #392 escrito: 50000 filas\n",
      "📝 Procesando batch #393...\n",
      "✅ Batch #393 escrito: 50000 filas\n",
      "📝 Procesando batch #394...\n",
      "✅ Batch #394 escrito: 50000 filas\n",
      "📝 Procesando batch #395...\n",
      "✅ Batch #395 escrito: 50000 filas\n",
      "📝 Procesando batch #396...\n",
      "✅ Batch #396 escrito: 50000 filas\n",
      "📝 Procesando batch #397...\n",
      "✅ Batch #397 escrito: 50000 filas\n",
      "📝 Procesando batch #398...\n",
      "✅ Batch #398 escrito: 50000 filas\n",
      "📝 Procesando batch #399...\n",
      "✅ Batch #399 escrito: 50000 filas\n",
      "📝 Procesando batch #400...\n",
      "✅ Batch #400 escrito: 50000 filas\n",
      "📝 Procesando batch #401...\n",
      "✅ Batch #401 escrito: 50000 filas\n",
      "📝 Procesando batch #402...\n",
      "✅ Batch #402 escrito: 50000 filas\n",
      "📝 Procesando batch #403...\n",
      "✅ Batch #403 escrito: 50000 filas\n",
      "📝 Procesando batch #404...\n",
      "✅ Batch #404 escrito: 50000 filas\n",
      "📝 Procesando batch #405...\n",
      "✅ Batch #405 escrito: 50000 filas\n",
      "📝 Procesando batch #406...\n",
      "✅ Batch #406 escrito: 50000 filas\n",
      "📝 Procesando batch #407...\n",
      "✅ Batch #407 escrito: 50000 filas\n",
      "📝 Procesando batch #408...\n",
      "✅ Batch #408 escrito: 48576 filas\n",
      "📝 Procesando batch #409...\n",
      "✅ Batch #409 escrito: 50000 filas\n",
      "📝 Procesando batch #410...\n",
      "✅ Batch #410 escrito: 50000 filas\n",
      "📝 Procesando batch #411...\n",
      "✅ Batch #411 escrito: 50000 filas\n",
      "📝 Procesando batch #412...\n",
      "✅ Batch #412 escrito: 50000 filas\n",
      "📝 Procesando batch #413...\n",
      "✅ Batch #413 escrito: 50000 filas\n",
      "📝 Procesando batch #414...\n",
      "✅ Batch #414 escrito: 50000 filas\n",
      "📝 Procesando batch #415...\n",
      "✅ Batch #415 escrito: 50000 filas\n",
      "📝 Procesando batch #416...\n",
      "✅ Batch #416 escrito: 50000 filas\n",
      "📝 Procesando batch #417...\n",
      "✅ Batch #417 escrito: 50000 filas\n",
      "📝 Procesando batch #418...\n",
      "✅ Batch #418 escrito: 50000 filas\n",
      "📝 Procesando batch #419...\n",
      "✅ Batch #419 escrito: 50000 filas\n",
      "📝 Procesando batch #420...\n",
      "✅ Batch #420 escrito: 50000 filas\n",
      "📝 Procesando batch #421...\n",
      "✅ Batch #421 escrito: 50000 filas\n",
      "📝 Procesando batch #422...\n",
      "✅ Batch #422 escrito: 50000 filas\n",
      "📝 Procesando batch #423...\n",
      "✅ Batch #423 escrito: 50000 filas\n",
      "📝 Procesando batch #424...\n",
      "✅ Batch #424 escrito: 50000 filas\n",
      "📝 Procesando batch #425...\n",
      "✅ Batch #425 escrito: 13306 filas\n",
      "📝 Procesando batch #426...\n",
      "✅ Batch #426 escrito: 50000 filas\n",
      "📝 Procesando batch #427...\n",
      "✅ Batch #427 escrito: 50000 filas\n",
      "📝 Procesando batch #428...\n",
      "✅ Batch #428 escrito: 50000 filas\n",
      "📝 Procesando batch #429...\n",
      "✅ Batch #429 escrito: 50000 filas\n",
      "📝 Procesando batch #430...\n",
      "✅ Batch #430 escrito: 50000 filas\n",
      "📝 Procesando batch #431...\n",
      "✅ Batch #431 escrito: 50000 filas\n",
      "📝 Procesando batch #432...\n",
      "✅ Batch #432 escrito: 50000 filas\n",
      "📝 Procesando batch #433...\n",
      "✅ Batch #433 escrito: 50000 filas\n",
      "📝 Procesando batch #434...\n",
      "✅ Batch #434 escrito: 50000 filas\n",
      "📝 Procesando batch #435...\n",
      "✅ Batch #435 escrito: 50000 filas\n",
      "📝 Procesando batch #436...\n",
      "✅ Batch #436 escrito: 50000 filas\n",
      "📝 Procesando batch #437...\n",
      "✅ Batch #437 escrito: 50000 filas\n",
      "📝 Procesando batch #438...\n",
      "✅ Batch #438 escrito: 50000 filas\n",
      "📝 Procesando batch #439...\n",
      "✅ Batch #439 escrito: 50000 filas\n",
      "📝 Procesando batch #440...\n",
      "✅ Batch #440 escrito: 50000 filas\n",
      "📝 Procesando batch #441...\n",
      "✅ Batch #441 escrito: 50000 filas\n",
      "📝 Procesando batch #442...\n",
      "✅ Batch #442 escrito: 50000 filas\n",
      "📝 Procesando batch #443...\n",
      "✅ Batch #443 escrito: 50000 filas\n",
      "📝 Procesando batch #444...\n",
      "✅ Batch #444 escrito: 50000 filas\n",
      "📝 Procesando batch #445...\n",
      "✅ Batch #445 escrito: 50000 filas\n",
      "📝 Procesando batch #446...\n",
      "✅ Batch #446 escrito: 48576 filas\n",
      "📝 Procesando batch #447...\n",
      "✅ Batch #447 escrito: 50000 filas\n",
      "📝 Procesando batch #448...\n",
      "✅ Batch #448 escrito: 50000 filas\n",
      "📝 Procesando batch #449...\n",
      "✅ Batch #449 escrito: 50000 filas\n",
      "📝 Procesando batch #450...\n",
      "✅ Batch #450 escrito: 50000 filas\n",
      "📝 Procesando batch #451...\n",
      "✅ Batch #451 escrito: 50000 filas\n",
      "📝 Procesando batch #452...\n",
      "✅ Batch #452 escrito: 50000 filas\n",
      "📝 Procesando batch #453...\n",
      "✅ Batch #453 escrito: 50000 filas\n",
      "📝 Procesando batch #454...\n",
      "✅ Batch #454 escrito: 50000 filas\n",
      "📝 Procesando batch #455...\n",
      "✅ Batch #455 escrito: 50000 filas\n",
      "📝 Procesando batch #456...\n",
      "✅ Batch #456 escrito: 50000 filas\n",
      "📝 Procesando batch #457...\n",
      "✅ Batch #457 escrito: 50000 filas\n",
      "📝 Procesando batch #458...\n",
      "✅ Batch #458 escrito: 50000 filas\n",
      "📝 Procesando batch #459...\n",
      "✅ Batch #459 escrito: 50000 filas\n",
      "📝 Procesando batch #460...\n",
      "✅ Batch #460 escrito: 50000 filas\n",
      "📝 Procesando batch #461...\n",
      "✅ Batch #461 escrito: 50000 filas\n",
      "📝 Procesando batch #462...\n",
      "✅ Batch #462 escrito: 50000 filas\n",
      "📝 Procesando batch #463...\n",
      "✅ Batch #463 escrito: 3268 filas\n",
      "📝 Procesando batch #464...\n",
      "✅ Batch #464 escrito: 50000 filas\n",
      "📝 Procesando batch #465...\n",
      "✅ Batch #465 escrito: 50000 filas\n",
      "📝 Procesando batch #466...\n",
      "✅ Batch #466 escrito: 50000 filas\n",
      "📝 Procesando batch #467...\n",
      "✅ Batch #467 escrito: 50000 filas\n",
      "📝 Procesando batch #468...\n",
      "✅ Batch #468 escrito: 50000 filas\n",
      "📝 Procesando batch #469...\n",
      "✅ Batch #469 escrito: 50000 filas\n",
      "📝 Procesando batch #470...\n",
      "✅ Batch #470 escrito: 50000 filas\n",
      "📝 Procesando batch #471...\n",
      "✅ Batch #471 escrito: 50000 filas\n",
      "📝 Procesando batch #472...\n",
      "✅ Batch #472 escrito: 50000 filas\n",
      "📝 Procesando batch #473...\n",
      "✅ Batch #473 escrito: 50000 filas\n",
      "📝 Procesando batch #474...\n",
      "✅ Batch #474 escrito: 50000 filas\n",
      "📝 Procesando batch #475...\n",
      "✅ Batch #475 escrito: 50000 filas\n",
      "📝 Procesando batch #476...\n",
      "✅ Batch #476 escrito: 50000 filas\n",
      "📝 Procesando batch #477...\n",
      "✅ Batch #477 escrito: 50000 filas\n",
      "📝 Procesando batch #478...\n",
      "✅ Batch #478 escrito: 50000 filas\n",
      "📝 Procesando batch #479...\n",
      "✅ Batch #479 escrito: 50000 filas\n",
      "📝 Procesando batch #480...\n",
      "✅ Batch #480 escrito: 50000 filas\n",
      "📝 Procesando batch #481...\n",
      "✅ Batch #481 escrito: 50000 filas\n",
      "📝 Procesando batch #482...\n",
      "✅ Batch #482 escrito: 50000 filas\n",
      "📝 Procesando batch #483...\n",
      "✅ Batch #483 escrito: 50000 filas\n",
      "📝 Procesando batch #484...\n",
      "✅ Batch #484 escrito: 48576 filas\n",
      "📝 Procesando batch #485...\n",
      "✅ Batch #485 escrito: 50000 filas\n",
      "📝 Procesando batch #486...\n",
      "✅ Batch #486 escrito: 50000 filas\n",
      "📝 Procesando batch #487...\n",
      "✅ Batch #487 escrito: 50000 filas\n",
      "📝 Procesando batch #488...\n",
      "✅ Batch #488 escrito: 50000 filas\n",
      "📝 Procesando batch #489...\n",
      "✅ Batch #489 escrito: 50000 filas\n",
      "📝 Procesando batch #490...\n",
      "✅ Batch #490 escrito: 50000 filas\n",
      "📝 Procesando batch #491...\n",
      "✅ Batch #491 escrito: 50000 filas\n",
      "📝 Procesando batch #492...\n",
      "✅ Batch #492 escrito: 50000 filas\n",
      "📝 Procesando batch #493...\n",
      "✅ Batch #493 escrito: 50000 filas\n",
      "📝 Procesando batch #494...\n",
      "✅ Batch #494 escrito: 50000 filas\n",
      "📝 Procesando batch #495...\n",
      "✅ Batch #495 escrito: 50000 filas\n",
      "📝 Procesando batch #496...\n",
      "✅ Batch #496 escrito: 50000 filas\n",
      "📝 Procesando batch #497...\n",
      "✅ Batch #497 escrito: 50000 filas\n",
      "📝 Procesando batch #498...\n",
      "✅ Batch #498 escrito: 50000 filas\n",
      "📝 Procesando batch #499...\n",
      "✅ Batch #499 escrito: 50000 filas\n",
      "📝 Procesando batch #500...\n",
      "✅ Batch #500 escrito: 50000 filas\n",
      "📝 Procesando batch #501...\n",
      "✅ Batch #501 escrito: 11594 filas\n",
      "📝 Procesando batch #502...\n",
      "✅ Batch #502 escrito: 50000 filas\n",
      "📝 Procesando batch #503...\n",
      "✅ Batch #503 escrito: 50000 filas\n",
      "📝 Procesando batch #504...\n",
      "✅ Batch #504 escrito: 50000 filas\n",
      "📝 Procesando batch #505...\n",
      "✅ Batch #505 escrito: 50000 filas\n",
      "📝 Procesando batch #506...\n",
      "✅ Batch #506 escrito: 50000 filas\n",
      "📝 Procesando batch #507...\n",
      "✅ Batch #507 escrito: 50000 filas\n",
      "📝 Procesando batch #508...\n",
      "✅ Batch #508 escrito: 50000 filas\n",
      "📝 Procesando batch #509...\n",
      "✅ Batch #509 escrito: 50000 filas\n",
      "📝 Procesando batch #510...\n",
      "✅ Batch #510 escrito: 50000 filas\n",
      "📝 Procesando batch #511...\n",
      "✅ Batch #511 escrito: 50000 filas\n",
      "📝 Procesando batch #512...\n",
      "✅ Batch #512 escrito: 50000 filas\n",
      "📝 Procesando batch #513...\n",
      "✅ Batch #513 escrito: 50000 filas\n",
      "📝 Procesando batch #514...\n",
      "✅ Batch #514 escrito: 50000 filas\n",
      "📝 Procesando batch #515...\n",
      "✅ Batch #515 escrito: 50000 filas\n",
      "📝 Procesando batch #516...\n",
      "✅ Batch #516 escrito: 50000 filas\n",
      "📝 Procesando batch #517...\n",
      "✅ Batch #517 escrito: 50000 filas\n",
      "📝 Procesando batch #518...\n",
      "✅ Batch #518 escrito: 50000 filas\n",
      "📝 Procesando batch #519...\n",
      "✅ Batch #519 escrito: 50000 filas\n",
      "📝 Procesando batch #520...\n",
      "✅ Batch #520 escrito: 50000 filas\n",
      "📝 Procesando batch #521...\n",
      "✅ Batch #521 escrito: 50000 filas\n",
      "📝 Procesando batch #522...\n",
      "✅ Batch #522 escrito: 48576 filas\n",
      "📝 Procesando batch #523...\n",
      "✅ Batch #523 escrito: 50000 filas\n",
      "📝 Procesando batch #524...\n",
      "✅ Batch #524 escrito: 50000 filas\n",
      "📝 Procesando batch #525...\n",
      "✅ Batch #525 escrito: 50000 filas\n",
      "📝 Procesando batch #526...\n",
      "✅ Batch #526 escrito: 50000 filas\n",
      "📝 Procesando batch #527...\n",
      "✅ Batch #527 escrito: 50000 filas\n",
      "📝 Procesando batch #528...\n",
      "✅ Batch #528 escrito: 50000 filas\n",
      "📝 Procesando batch #529...\n",
      "✅ Batch #529 escrito: 50000 filas\n",
      "📝 Procesando batch #530...\n",
      "✅ Batch #530 escrito: 50000 filas\n",
      "📝 Procesando batch #531...\n",
      "✅ Batch #531 escrito: 50000 filas\n",
      "📝 Procesando batch #532...\n",
      "✅ Batch #532 escrito: 50000 filas\n",
      "📝 Procesando batch #533...\n",
      "✅ Batch #533 escrito: 50000 filas\n",
      "📝 Procesando batch #534...\n",
      "✅ Batch #534 escrito: 50000 filas\n",
      "📝 Procesando batch #535...\n",
      "✅ Batch #535 escrito: 50000 filas\n",
      "📝 Procesando batch #536...\n",
      "✅ Batch #536 escrito: 50000 filas\n",
      "📝 Procesando batch #537...\n",
      "✅ Batch #537 escrito: 50000 filas\n",
      "📝 Procesando batch #538...\n",
      "✅ Batch #538 escrito: 9803 filas\n",
      "📝 Procesando batch #539...\n",
      "✅ Batch #539 escrito: 50000 filas\n",
      "📝 Procesando batch #540...\n",
      "✅ Batch #540 escrito: 50000 filas\n",
      "📝 Procesando batch #541...\n",
      "✅ Batch #541 escrito: 50000 filas\n",
      "📝 Procesando batch #542...\n",
      "✅ Batch #542 escrito: 50000 filas\n",
      "📝 Procesando batch #543...\n",
      "✅ Batch #543 escrito: 50000 filas\n",
      "📝 Procesando batch #544...\n",
      "✅ Batch #544 escrito: 50000 filas\n",
      "📝 Procesando batch #545...\n",
      "✅ Batch #545 escrito: 50000 filas\n",
      "📝 Procesando batch #546...\n",
      "✅ Batch #546 escrito: 50000 filas\n",
      "📝 Procesando batch #547...\n",
      "✅ Batch #547 escrito: 50000 filas\n",
      "📝 Procesando batch #548...\n",
      "✅ Batch #548 escrito: 50000 filas\n",
      "📝 Procesando batch #549...\n",
      "✅ Batch #549 escrito: 50000 filas\n",
      "📝 Procesando batch #550...\n",
      "✅ Batch #550 escrito: 50000 filas\n",
      "📝 Procesando batch #551...\n",
      "✅ Batch #551 escrito: 50000 filas\n",
      "📝 Procesando batch #552...\n",
      "✅ Batch #552 escrito: 50000 filas\n",
      "📝 Procesando batch #553...\n",
      "✅ Batch #553 escrito: 50000 filas\n",
      "📝 Procesando batch #554...\n",
      "✅ Batch #554 escrito: 50000 filas\n",
      "📝 Procesando batch #555...\n",
      "✅ Batch #555 escrito: 50000 filas\n",
      "📝 Procesando batch #556...\n",
      "✅ Batch #556 escrito: 50000 filas\n",
      "📝 Procesando batch #557...\n",
      "✅ Batch #557 escrito: 50000 filas\n",
      "📝 Procesando batch #558...\n",
      "✅ Batch #558 escrito: 50000 filas\n",
      "📝 Procesando batch #559...\n",
      "✅ Batch #559 escrito: 48576 filas\n",
      "📝 Procesando batch #560...\n",
      "✅ Batch #560 escrito: 50000 filas\n",
      "📝 Procesando batch #561...\n",
      "✅ Batch #561 escrito: 50000 filas\n",
      "📝 Procesando batch #562...\n",
      "✅ Batch #562 escrito: 50000 filas\n",
      "📝 Procesando batch #563...\n",
      "✅ Batch #563 escrito: 50000 filas\n",
      "📝 Procesando batch #564...\n",
      "✅ Batch #564 escrito: 50000 filas\n",
      "📝 Procesando batch #565...\n",
      "✅ Batch #565 escrito: 50000 filas\n",
      "📝 Procesando batch #566...\n",
      "✅ Batch #566 escrito: 50000 filas\n",
      "📝 Procesando batch #567...\n",
      "✅ Batch #567 escrito: 50000 filas\n",
      "📝 Procesando batch #568...\n",
      "✅ Batch #568 escrito: 50000 filas\n",
      "📝 Procesando batch #569...\n",
      "✅ Batch #569 escrito: 50000 filas\n",
      "📝 Procesando batch #570...\n",
      "✅ Batch #570 escrito: 50000 filas\n",
      "📝 Procesando batch #571...\n",
      "✅ Batch #571 escrito: 50000 filas\n",
      "📝 Procesando batch #572...\n",
      "✅ Batch #572 escrito: 50000 filas\n",
      "📝 Procesando batch #573...\n",
      "✅ Batch #573 escrito: 50000 filas\n",
      "📝 Procesando batch #574...\n",
      "✅ Batch #574 escrito: 50000 filas\n",
      "📝 Procesando batch #575...\n",
      "✅ Batch #575 escrito: 50000 filas\n",
      "📝 Procesando batch #576...\n",
      "✅ Batch #576 escrito: 50000 filas\n",
      "📝 Procesando batch #577...\n",
      "✅ Batch #577 escrito: 5128 filas\n",
      "📝 Procesando batch #578...\n",
      "✅ Batch #578 escrito: 50000 filas\n",
      "📝 Procesando batch #579...\n",
      "✅ Batch #579 escrito: 50000 filas\n",
      "📝 Procesando batch #580...\n",
      "✅ Batch #580 escrito: 50000 filas\n",
      "📝 Procesando batch #581...\n",
      "✅ Batch #581 escrito: 50000 filas\n",
      "📝 Procesando batch #582...\n",
      "✅ Batch #582 escrito: 50000 filas\n",
      "📝 Procesando batch #583...\n",
      "✅ Batch #583 escrito: 50000 filas\n",
      "📝 Procesando batch #584...\n",
      "✅ Batch #584 escrito: 50000 filas\n",
      "📝 Procesando batch #585...\n",
      "✅ Batch #585 escrito: 50000 filas\n",
      "📝 Procesando batch #586...\n",
      "✅ Batch #586 escrito: 50000 filas\n",
      "📝 Procesando batch #587...\n",
      "✅ Batch #587 escrito: 50000 filas\n",
      "📝 Procesando batch #588...\n",
      "✅ Batch #588 escrito: 50000 filas\n",
      "📝 Procesando batch #589...\n",
      "✅ Batch #589 escrito: 50000 filas\n",
      "📝 Procesando batch #590...\n",
      "✅ Batch #590 escrito: 50000 filas\n",
      "📝 Procesando batch #591...\n",
      "✅ Batch #591 escrito: 50000 filas\n",
      "📝 Procesando batch #592...\n",
      "✅ Batch #592 escrito: 50000 filas\n",
      "📝 Procesando batch #593...\n",
      "✅ Batch #593 escrito: 50000 filas\n",
      "📝 Procesando batch #594...\n",
      "✅ Batch #594 escrito: 50000 filas\n",
      "📝 Procesando batch #595...\n",
      "✅ Batch #595 escrito: 50000 filas\n",
      "📝 Procesando batch #596...\n",
      "✅ Batch #596 escrito: 50000 filas\n",
      "📝 Procesando batch #597...\n",
      "✅ Batch #597 escrito: 50000 filas\n",
      "📝 Procesando batch #598...\n",
      "✅ Batch #598 escrito: 48576 filas\n",
      "📝 Procesando batch #599...\n",
      "✅ Batch #599 escrito: 50000 filas\n",
      "📝 Procesando batch #600...\n",
      "✅ Batch #600 escrito: 50000 filas\n",
      "📝 Procesando batch #601...\n",
      "✅ Batch #601 escrito: 50000 filas\n",
      "📝 Procesando batch #602...\n",
      "✅ Batch #602 escrito: 50000 filas\n",
      "📝 Procesando batch #603...\n",
      "✅ Batch #603 escrito: 50000 filas\n",
      "📝 Procesando batch #604...\n",
      "✅ Batch #604 escrito: 50000 filas\n",
      "📝 Procesando batch #605...\n",
      "✅ Batch #605 escrito: 50000 filas\n",
      "📝 Procesando batch #606...\n",
      "✅ Batch #606 escrito: 50000 filas\n",
      "📝 Procesando batch #607...\n",
      "✅ Batch #607 escrito: 50000 filas\n",
      "📝 Procesando batch #608...\n",
      "✅ Batch #608 escrito: 50000 filas\n",
      "📝 Procesando batch #609...\n",
      "✅ Batch #609 escrito: 50000 filas\n",
      "📝 Procesando batch #610...\n",
      "✅ Batch #610 escrito: 50000 filas\n",
      "📝 Procesando batch #611...\n",
      "✅ Batch #611 escrito: 50000 filas\n",
      "📝 Procesando batch #612...\n",
      "✅ Batch #612 escrito: 50000 filas\n",
      "📝 Procesando batch #613...\n",
      "✅ Batch #613 escrito: 50000 filas\n",
      "📝 Procesando batch #614...\n",
      "✅ Batch #614 escrito: 50000 filas\n",
      "📝 Procesando batch #615...\n",
      "✅ Batch #615 escrito: 50000 filas\n",
      "📝 Procesando batch #616...\n",
      "✅ Batch #616 escrito: 50000 filas\n",
      "📝 Procesando batch #617...\n",
      "✅ Batch #617 escrito: 50000 filas\n",
      "📝 Procesando batch #618...\n",
      "✅ Batch #618 escrito: 9417 filas\n",
      "📝 Procesando batch #619...\n",
      "✅ Batch #619 escrito: 50000 filas\n",
      "📝 Procesando batch #620...\n",
      "✅ Batch #620 escrito: 50000 filas\n",
      "📝 Procesando batch #621...\n",
      "✅ Batch #621 escrito: 50000 filas\n",
      "📝 Procesando batch #622...\n",
      "✅ Batch #622 escrito: 50000 filas\n",
      "📝 Procesando batch #623...\n",
      "✅ Batch #623 escrito: 50000 filas\n",
      "📝 Procesando batch #624...\n",
      "✅ Batch #624 escrito: 50000 filas\n",
      "📝 Procesando batch #625...\n",
      "✅ Batch #625 escrito: 50000 filas\n",
      "📝 Procesando batch #626...\n",
      "✅ Batch #626 escrito: 50000 filas\n",
      "📝 Procesando batch #627...\n",
      "✅ Batch #627 escrito: 50000 filas\n",
      "📝 Procesando batch #628...\n",
      "✅ Batch #628 escrito: 50000 filas\n",
      "📝 Procesando batch #629...\n",
      "✅ Batch #629 escrito: 50000 filas\n",
      "📝 Procesando batch #630...\n",
      "✅ Batch #630 escrito: 50000 filas\n",
      "📝 Procesando batch #631...\n",
      "✅ Batch #631 escrito: 50000 filas\n",
      "📝 Procesando batch #632...\n",
      "✅ Batch #632 escrito: 50000 filas\n",
      "📝 Procesando batch #633...\n",
      "✅ Batch #633 escrito: 50000 filas\n",
      "📝 Procesando batch #634...\n",
      "✅ Batch #634 escrito: 50000 filas\n",
      "📝 Procesando batch #635...\n",
      "✅ Batch #635 escrito: 50000 filas\n",
      "📝 Procesando batch #636...\n",
      "✅ Batch #636 escrito: 50000 filas\n",
      "📝 Procesando batch #637...\n",
      "✅ Batch #637 escrito: 50000 filas\n",
      "📝 Procesando batch #638...\n",
      "✅ Batch #638 escrito: 50000 filas\n",
      "📝 Procesando batch #639...\n",
      "✅ Batch #639 escrito: 48576 filas\n",
      "📝 Procesando batch #640...\n",
      "✅ Batch #640 escrito: 50000 filas\n",
      "📝 Procesando batch #641...\n",
      "✅ Batch #641 escrito: 50000 filas\n",
      "📝 Procesando batch #642...\n",
      "✅ Batch #642 escrito: 50000 filas\n",
      "📝 Procesando batch #643...\n",
      "✅ Batch #643 escrito: 50000 filas\n",
      "📝 Procesando batch #644...\n",
      "✅ Batch #644 escrito: 50000 filas\n",
      "📝 Procesando batch #645...\n",
      "✅ Batch #645 escrito: 50000 filas\n",
      "📝 Procesando batch #646...\n",
      "✅ Batch #646 escrito: 50000 filas\n",
      "📝 Procesando batch #647...\n",
      "✅ Batch #647 escrito: 50000 filas\n",
      "📝 Procesando batch #648...\n",
      "✅ Batch #648 escrito: 50000 filas\n",
      "📝 Procesando batch #649...\n",
      "✅ Batch #649 escrito: 50000 filas\n",
      "📝 Procesando batch #650...\n",
      "✅ Batch #650 escrito: 50000 filas\n",
      "📝 Procesando batch #651...\n",
      "✅ Batch #651 escrito: 50000 filas\n",
      "📝 Procesando batch #652...\n",
      "✅ Batch #652 escrito: 50000 filas\n",
      "📝 Procesando batch #653...\n",
      "✅ Batch #653 escrito: 50000 filas\n",
      "📝 Procesando batch #654...\n",
      "✅ Batch #654 escrito: 50000 filas\n",
      "📝 Procesando batch #655...\n",
      "✅ Batch #655 escrito: 50000 filas\n",
      "📝 Procesando batch #656...\n",
      "✅ Batch #656 escrito: 50000 filas\n",
      "📝 Procesando batch #657...\n",
      "✅ Batch #657 escrito: 4821 filas\n",
      "📝 Procesando batch #658...\n",
      "✅ Batch #658 escrito: 50000 filas\n",
      "📝 Procesando batch #659...\n",
      "✅ Batch #659 escrito: 50000 filas\n",
      "📝 Procesando batch #660...\n",
      "✅ Batch #660 escrito: 50000 filas\n",
      "📝 Procesando batch #661...\n",
      "✅ Batch #661 escrito: 50000 filas\n",
      "📝 Procesando batch #662...\n",
      "✅ Batch #662 escrito: 50000 filas\n",
      "📝 Procesando batch #663...\n",
      "✅ Batch #663 escrito: 50000 filas\n",
      "📝 Procesando batch #664...\n",
      "✅ Batch #664 escrito: 50000 filas\n",
      "📝 Procesando batch #665...\n",
      "✅ Batch #665 escrito: 50000 filas\n",
      "📝 Procesando batch #666...\n",
      "✅ Batch #666 escrito: 50000 filas\n",
      "📝 Procesando batch #667...\n",
      "✅ Batch #667 escrito: 50000 filas\n",
      "📝 Procesando batch #668...\n",
      "✅ Batch #668 escrito: 50000 filas\n",
      "📝 Procesando batch #669...\n",
      "✅ Batch #669 escrito: 50000 filas\n",
      "📝 Procesando batch #670...\n",
      "✅ Batch #670 escrito: 50000 filas\n",
      "📝 Procesando batch #671...\n",
      "✅ Batch #671 escrito: 50000 filas\n",
      "📝 Procesando batch #672...\n",
      "✅ Batch #672 escrito: 50000 filas\n",
      "📝 Procesando batch #673...\n",
      "✅ Batch #673 escrito: 50000 filas\n",
      "📝 Procesando batch #674...\n",
      "✅ Batch #674 escrito: 50000 filas\n",
      "📝 Procesando batch #675...\n",
      "✅ Batch #675 escrito: 50000 filas\n",
      "📝 Procesando batch #676...\n",
      "✅ Batch #676 escrito: 50000 filas\n",
      "📝 Procesando batch #677...\n",
      "✅ Batch #677 escrito: 50000 filas\n",
      "📝 Procesando batch #678...\n",
      "✅ Batch #678 escrito: 48576 filas\n",
      "📝 Procesando batch #679...\n",
      "✅ Batch #679 escrito: 50000 filas\n",
      "📝 Procesando batch #680...\n",
      "✅ Batch #680 escrito: 50000 filas\n",
      "📝 Procesando batch #681...\n",
      "✅ Batch #681 escrito: 50000 filas\n",
      "📝 Procesando batch #682...\n",
      "✅ Batch #682 escrito: 50000 filas\n",
      "📝 Procesando batch #683...\n",
      "✅ Batch #683 escrito: 50000 filas\n",
      "📝 Procesando batch #684...\n",
      "✅ Batch #684 escrito: 50000 filas\n",
      "📝 Procesando batch #685...\n",
      "✅ Batch #685 escrito: 50000 filas\n",
      "📝 Procesando batch #686...\n",
      "✅ Batch #686 escrito: 50000 filas\n",
      "📝 Procesando batch #687...\n",
      "✅ Batch #687 escrito: 50000 filas\n",
      "📝 Procesando batch #688...\n",
      "✅ Batch #688 escrito: 50000 filas\n",
      "📝 Procesando batch #689...\n",
      "✅ Batch #689 escrito: 50000 filas\n",
      "📝 Procesando batch #690...\n",
      "✅ Batch #690 escrito: 50000 filas\n",
      "📝 Procesando batch #691...\n",
      "✅ Batch #691 escrito: 50000 filas\n",
      "📝 Procesando batch #692...\n",
      "✅ Batch #692 escrito: 50000 filas\n",
      "📝 Procesando batch #693...\n",
      "✅ Batch #693 escrito: 50000 filas\n",
      "📝 Procesando batch #694...\n",
      "✅ Batch #694 escrito: 50000 filas\n",
      "📝 Procesando batch #695...\n",
      "✅ Batch #695 escrito: 19120 filas\n",
      "📝 Procesando batch #696...\n",
      "✅ Batch #696 escrito: 50000 filas\n",
      "📝 Procesando batch #697...\n",
      "✅ Batch #697 escrito: 50000 filas\n",
      "📝 Procesando batch #698...\n",
      "✅ Batch #698 escrito: 50000 filas\n",
      "📝 Procesando batch #699...\n",
      "✅ Batch #699 escrito: 50000 filas\n",
      "📝 Procesando batch #700...\n",
      "✅ Batch #700 escrito: 50000 filas\n",
      "📝 Procesando batch #701...\n",
      "✅ Batch #701 escrito: 50000 filas\n",
      "📝 Procesando batch #702...\n",
      "✅ Batch #702 escrito: 50000 filas\n",
      "📝 Procesando batch #703...\n",
      "✅ Batch #703 escrito: 50000 filas\n",
      "📝 Procesando batch #704...\n",
      "✅ Batch #704 escrito: 50000 filas\n",
      "📝 Procesando batch #705...\n",
      "✅ Batch #705 escrito: 50000 filas\n",
      "📝 Procesando batch #706...\n",
      "✅ Batch #706 escrito: 50000 filas\n",
      "📝 Procesando batch #707...\n",
      "✅ Batch #707 escrito: 50000 filas\n",
      "📝 Procesando batch #708...\n",
      "✅ Batch #708 escrito: 50000 filas\n",
      "📝 Procesando batch #709...\n",
      "✅ Batch #709 escrito: 50000 filas\n",
      "📝 Procesando batch #710...\n",
      "✅ Batch #710 escrito: 50000 filas\n",
      "📝 Procesando batch #711...\n",
      "✅ Batch #711 escrito: 50000 filas\n",
      "📝 Procesando batch #712...\n",
      "✅ Batch #712 escrito: 50000 filas\n",
      "📝 Procesando batch #713...\n",
      "✅ Batch #713 escrito: 50000 filas\n",
      "📝 Procesando batch #714...\n",
      "✅ Batch #714 escrito: 50000 filas\n",
      "📝 Procesando batch #715...\n",
      "✅ Batch #715 escrito: 50000 filas\n",
      "📝 Procesando batch #716...\n",
      "✅ Batch #716 escrito: 48576 filas\n",
      "📝 Procesando batch #717...\n",
      "✅ Batch #717 escrito: 50000 filas\n",
      "📝 Procesando batch #718...\n",
      "✅ Batch #718 escrito: 50000 filas\n",
      "📝 Procesando batch #719...\n",
      "✅ Batch #719 escrito: 50000 filas\n",
      "📝 Procesando batch #720...\n",
      "✅ Batch #720 escrito: 50000 filas\n",
      "📝 Procesando batch #721...\n",
      "✅ Batch #721 escrito: 50000 filas\n",
      "📝 Procesando batch #722...\n",
      "✅ Batch #722 escrito: 50000 filas\n",
      "📝 Procesando batch #723...\n",
      "✅ Batch #723 escrito: 50000 filas\n",
      "📝 Procesando batch #724...\n",
      "✅ Batch #724 escrito: 50000 filas\n",
      "📝 Procesando batch #725...\n",
      "✅ Batch #725 escrito: 50000 filas\n",
      "📝 Procesando batch #726...\n",
      "✅ Batch #726 escrito: 50000 filas\n",
      "📝 Procesando batch #727...\n",
      "✅ Batch #727 escrito: 50000 filas\n",
      "📝 Procesando batch #728...\n",
      "✅ Batch #728 escrito: 50000 filas\n",
      "📝 Procesando batch #729...\n",
      "✅ Batch #729 escrito: 50000 filas\n",
      "📝 Procesando batch #730...\n",
      "✅ Batch #730 escrito: 50000 filas\n",
      "📝 Procesando batch #731...\n",
      "✅ Batch #731 escrito: 50000 filas\n",
      "📝 Procesando batch #732...\n",
      "✅ Batch #732 escrito: 50000 filas\n",
      "📝 Procesando batch #733...\n",
      "✅ Batch #733 escrito: 50000 filas\n",
      "📝 Procesando batch #734...\n",
      "✅ Batch #734 escrito: 30070 filas\n",
      "📝 Procesando batch #735...\n",
      "✅ Batch #735 escrito: 50000 filas\n",
      "📝 Procesando batch #736...\n",
      "✅ Batch #736 escrito: 50000 filas\n",
      "📝 Procesando batch #737...\n",
      "✅ Batch #737 escrito: 50000 filas\n",
      "📝 Procesando batch #738...\n",
      "✅ Batch #738 escrito: 50000 filas\n",
      "📝 Procesando batch #739...\n",
      "✅ Batch #739 escrito: 50000 filas\n",
      "📝 Procesando batch #740...\n",
      "✅ Batch #740 escrito: 50000 filas\n",
      "📝 Procesando batch #741...\n",
      "✅ Batch #741 escrito: 50000 filas\n",
      "📝 Procesando batch #742...\n",
      "✅ Batch #742 escrito: 50000 filas\n",
      "📝 Procesando batch #743...\n",
      "✅ Batch #743 escrito: 50000 filas\n",
      "📝 Procesando batch #744...\n",
      "✅ Batch #744 escrito: 50000 filas\n",
      "📝 Procesando batch #745...\n",
      "✅ Batch #745 escrito: 50000 filas\n",
      "📝 Procesando batch #746...\n",
      "✅ Batch #746 escrito: 50000 filas\n",
      "📝 Procesando batch #747...\n",
      "✅ Batch #747 escrito: 50000 filas\n",
      "📝 Procesando batch #748...\n",
      "✅ Batch #748 escrito: 50000 filas\n",
      "📝 Procesando batch #749...\n",
      "✅ Batch #749 escrito: 50000 filas\n",
      "📝 Procesando batch #750...\n",
      "✅ Batch #750 escrito: 50000 filas\n",
      "📝 Procesando batch #751...\n",
      "✅ Batch #751 escrito: 50000 filas\n",
      "📝 Procesando batch #752...\n",
      "✅ Batch #752 escrito: 50000 filas\n",
      "📝 Procesando batch #753...\n",
      "✅ Batch #753 escrito: 50000 filas\n",
      "📝 Procesando batch #754...\n",
      "✅ Batch #754 escrito: 50000 filas\n",
      "📝 Procesando batch #755...\n",
      "✅ Batch #755 escrito: 48576 filas\n",
      "📝 Procesando batch #756...\n",
      "✅ Batch #756 escrito: 50000 filas\n",
      "📝 Procesando batch #757...\n",
      "✅ Batch #757 escrito: 50000 filas\n",
      "📝 Procesando batch #758...\n",
      "✅ Batch #758 escrito: 50000 filas\n",
      "📝 Procesando batch #759...\n",
      "✅ Batch #759 escrito: 50000 filas\n",
      "📝 Procesando batch #760...\n",
      "✅ Batch #760 escrito: 50000 filas\n",
      "📝 Procesando batch #761...\n",
      "✅ Batch #761 escrito: 50000 filas\n",
      "📝 Procesando batch #762...\n",
      "✅ Batch #762 escrito: 50000 filas\n",
      "📝 Procesando batch #763...\n",
      "✅ Batch #763 escrito: 50000 filas\n",
      "📝 Procesando batch #764...\n",
      "✅ Batch #764 escrito: 50000 filas\n",
      "📝 Procesando batch #765...\n",
      "✅ Batch #765 escrito: 50000 filas\n",
      "📝 Procesando batch #766...\n",
      "✅ Batch #766 escrito: 50000 filas\n",
      "📝 Procesando batch #767...\n",
      "✅ Batch #767 escrito: 50000 filas\n",
      "📝 Procesando batch #768...\n",
      "✅ Batch #768 escrito: 50000 filas\n",
      "📝 Procesando batch #769...\n",
      "✅ Batch #769 escrito: 50000 filas\n",
      "📝 Procesando batch #770...\n",
      "✅ Batch #770 escrito: 50000 filas\n",
      "📝 Procesando batch #771...\n",
      "✅ Batch #771 escrito: 50000 filas\n",
      "📝 Procesando batch #772...\n",
      "✅ Batch #772 escrito: 50000 filas\n",
      "📝 Procesando batch #773...\n",
      "✅ Batch #773 escrito: 25158 filas\n",
      "📝 Procesando batch #774...\n",
      "✅ Batch #774 escrito: 50000 filas\n",
      "📝 Procesando batch #775...\n",
      "✅ Batch #775 escrito: 50000 filas\n",
      "📝 Procesando batch #776...\n",
      "✅ Batch #776 escrito: 50000 filas\n",
      "📝 Procesando batch #777...\n",
      "✅ Batch #777 escrito: 50000 filas\n",
      "📝 Procesando batch #778...\n",
      "✅ Batch #778 escrito: 50000 filas\n",
      "📝 Procesando batch #779...\n",
      "✅ Batch #779 escrito: 50000 filas\n",
      "📝 Procesando batch #780...\n",
      "✅ Batch #780 escrito: 50000 filas\n",
      "📝 Procesando batch #781...\n",
      "✅ Batch #781 escrito: 50000 filas\n",
      "📝 Procesando batch #782...\n",
      "✅ Batch #782 escrito: 50000 filas\n",
      "📝 Procesando batch #783...\n",
      "✅ Batch #783 escrito: 50000 filas\n",
      "📝 Procesando batch #784...\n",
      "✅ Batch #784 escrito: 50000 filas\n",
      "📝 Procesando batch #785...\n",
      "✅ Batch #785 escrito: 50000 filas\n",
      "📝 Procesando batch #786...\n",
      "✅ Batch #786 escrito: 50000 filas\n",
      "📝 Procesando batch #787...\n",
      "✅ Batch #787 escrito: 50000 filas\n",
      "📝 Procesando batch #788...\n",
      "✅ Batch #788 escrito: 50000 filas\n",
      "📝 Procesando batch #789...\n",
      "✅ Batch #789 escrito: 50000 filas\n",
      "📝 Procesando batch #790...\n",
      "✅ Batch #790 escrito: 50000 filas\n",
      "📝 Procesando batch #791...\n",
      "✅ Batch #791 escrito: 50000 filas\n",
      "📝 Procesando batch #792...\n",
      "✅ Batch #792 escrito: 50000 filas\n",
      "📝 Procesando batch #793...\n",
      "✅ Batch #793 escrito: 50000 filas\n",
      "📝 Procesando batch #794...\n",
      "✅ Batch #794 escrito: 48576 filas\n",
      "📝 Procesando batch #795...\n",
      "✅ Batch #795 escrito: 50000 filas\n",
      "📝 Procesando batch #796...\n",
      "✅ Batch #796 escrito: 50000 filas\n",
      "📝 Procesando batch #797...\n",
      "✅ Batch #797 escrito: 50000 filas\n",
      "📝 Procesando batch #798...\n",
      "✅ Batch #798 escrito: 50000 filas\n",
      "📝 Procesando batch #799...\n",
      "✅ Batch #799 escrito: 50000 filas\n",
      "📝 Procesando batch #800...\n",
      "✅ Batch #800 escrito: 50000 filas\n",
      "📝 Procesando batch #801...\n",
      "✅ Batch #801 escrito: 50000 filas\n",
      "📝 Procesando batch #802...\n",
      "✅ Batch #802 escrito: 50000 filas\n",
      "📝 Procesando batch #803...\n",
      "✅ Batch #803 escrito: 50000 filas\n",
      "📝 Procesando batch #804...\n",
      "✅ Batch #804 escrito: 50000 filas\n",
      "📝 Procesando batch #805...\n",
      "✅ Batch #805 escrito: 50000 filas\n",
      "📝 Procesando batch #806...\n",
      "✅ Batch #806 escrito: 50000 filas\n",
      "📝 Procesando batch #807...\n",
      "✅ Batch #807 escrito: 50000 filas\n",
      "📝 Procesando batch #808...\n",
      "✅ Batch #808 escrito: 50000 filas\n",
      "📝 Procesando batch #809...\n",
      "✅ Batch #809 escrito: 50000 filas\n",
      "📝 Procesando batch #810...\n",
      "✅ Batch #810 escrito: 50000 filas\n",
      "📝 Procesando batch #811...\n",
      "✅ Batch #811 escrito: 28002 filas\n",
      "📝 Procesando batch #812...\n",
      "✅ Batch #812 escrito: 50000 filas\n",
      "📝 Procesando batch #813...\n",
      "✅ Batch #813 escrito: 50000 filas\n",
      "📝 Procesando batch #814...\n",
      "✅ Batch #814 escrito: 50000 filas\n",
      "📝 Procesando batch #815...\n",
      "✅ Batch #815 escrito: 50000 filas\n",
      "📝 Procesando batch #816...\n",
      "✅ Batch #816 escrito: 50000 filas\n",
      "📝 Procesando batch #817...\n",
      "✅ Batch #817 escrito: 50000 filas\n",
      "📝 Procesando batch #818...\n",
      "✅ Batch #818 escrito: 50000 filas\n",
      "📝 Procesando batch #819...\n",
      "✅ Batch #819 escrito: 50000 filas\n",
      "📝 Procesando batch #820...\n",
      "✅ Batch #820 escrito: 50000 filas\n",
      "📝 Procesando batch #821...\n",
      "✅ Batch #821 escrito: 50000 filas\n",
      "📝 Procesando batch #822...\n",
      "✅ Batch #822 escrito: 50000 filas\n",
      "📝 Procesando batch #823...\n",
      "✅ Batch #823 escrito: 50000 filas\n",
      "📝 Procesando batch #824...\n",
      "✅ Batch #824 escrito: 50000 filas\n",
      "📝 Procesando batch #825...\n",
      "✅ Batch #825 escrito: 50000 filas\n",
      "📝 Procesando batch #826...\n",
      "✅ Batch #826 escrito: 50000 filas\n",
      "📝 Procesando batch #827...\n",
      "✅ Batch #827 escrito: 50000 filas\n",
      "📝 Procesando batch #828...\n",
      "✅ Batch #828 escrito: 50000 filas\n",
      "📝 Procesando batch #829...\n",
      "✅ Batch #829 escrito: 50000 filas\n",
      "📝 Procesando batch #830...\n",
      "✅ Batch #830 escrito: 50000 filas\n",
      "📝 Procesando batch #831...\n",
      "✅ Batch #831 escrito: 50000 filas\n",
      "📝 Procesando batch #832...\n",
      "✅ Batch #832 escrito: 48576 filas\n",
      "📝 Procesando batch #833...\n",
      "✅ Batch #833 escrito: 50000 filas\n",
      "📝 Procesando batch #834...\n",
      "✅ Batch #834 escrito: 50000 filas\n",
      "📝 Procesando batch #835...\n",
      "✅ Batch #835 escrito: 50000 filas\n",
      "📝 Procesando batch #836...\n",
      "✅ Batch #836 escrito: 50000 filas\n",
      "📝 Procesando batch #837...\n",
      "✅ Batch #837 escrito: 50000 filas\n",
      "📝 Procesando batch #838...\n",
      "✅ Batch #838 escrito: 50000 filas\n",
      "📝 Procesando batch #839...\n",
      "✅ Batch #839 escrito: 50000 filas\n",
      "📝 Procesando batch #840...\n",
      "✅ Batch #840 escrito: 50000 filas\n",
      "📝 Procesando batch #841...\n",
      "✅ Batch #841 escrito: 50000 filas\n",
      "📝 Procesando batch #842...\n",
      "✅ Batch #842 escrito: 50000 filas\n",
      "📝 Procesando batch #843...\n",
      "✅ Batch #843 escrito: 50000 filas\n",
      "📝 Procesando batch #844...\n",
      "✅ Batch #844 escrito: 50000 filas\n",
      "📝 Procesando batch #845...\n",
      "✅ Batch #845 escrito: 50000 filas\n",
      "📝 Procesando batch #846...\n",
      "✅ Batch #846 escrito: 50000 filas\n",
      "📝 Procesando batch #847...\n",
      "✅ Batch #847 escrito: 50000 filas\n",
      "📝 Procesando batch #848...\n",
      "✅ Batch #848 escrito: 50000 filas\n",
      "📝 Procesando batch #849...\n",
      "✅ Batch #849 escrito: 26076 filas\n",
      "📝 Procesando batch #850...\n",
      "✅ Batch #850 escrito: 50000 filas\n",
      "📝 Procesando batch #851...\n",
      "✅ Batch #851 escrito: 50000 filas\n",
      "📝 Procesando batch #852...\n",
      "✅ Batch #852 escrito: 50000 filas\n",
      "📝 Procesando batch #853...\n",
      "✅ Batch #853 escrito: 50000 filas\n",
      "📝 Procesando batch #854...\n",
      "✅ Batch #854 escrito: 50000 filas\n",
      "📝 Procesando batch #855...\n",
      "✅ Batch #855 escrito: 50000 filas\n",
      "📝 Procesando batch #856...\n",
      "✅ Batch #856 escrito: 50000 filas\n",
      "📝 Procesando batch #857...\n",
      "✅ Batch #857 escrito: 50000 filas\n",
      "📝 Procesando batch #858...\n",
      "✅ Batch #858 escrito: 50000 filas\n",
      "📝 Procesando batch #859...\n",
      "✅ Batch #859 escrito: 50000 filas\n",
      "📝 Procesando batch #860...\n",
      "✅ Batch #860 escrito: 50000 filas\n",
      "📝 Procesando batch #861...\n",
      "✅ Batch #861 escrito: 50000 filas\n",
      "📝 Procesando batch #862...\n",
      "✅ Batch #862 escrito: 50000 filas\n",
      "📝 Procesando batch #863...\n",
      "✅ Batch #863 escrito: 50000 filas\n",
      "📝 Procesando batch #864...\n",
      "✅ Batch #864 escrito: 50000 filas\n",
      "📝 Procesando batch #865...\n",
      "✅ Batch #865 escrito: 50000 filas\n",
      "📝 Procesando batch #866...\n",
      "✅ Batch #866 escrito: 50000 filas\n",
      "📝 Procesando batch #867...\n",
      "✅ Batch #867 escrito: 50000 filas\n",
      "📝 Procesando batch #868...\n",
      "✅ Batch #868 escrito: 50000 filas\n",
      "📝 Procesando batch #869...\n",
      "✅ Batch #869 escrito: 50000 filas\n",
      "📝 Procesando batch #870...\n",
      "✅ Batch #870 escrito: 48576 filas\n",
      "📝 Procesando batch #871...\n",
      "✅ Batch #871 escrito: 50000 filas\n",
      "📝 Procesando batch #872...\n",
      "✅ Batch #872 escrito: 50000 filas\n",
      "📝 Procesando batch #873...\n",
      "✅ Batch #873 escrito: 50000 filas\n",
      "📝 Procesando batch #874...\n",
      "✅ Batch #874 escrito: 50000 filas\n",
      "📝 Procesando batch #875...\n",
      "✅ Batch #875 escrito: 50000 filas\n",
      "📝 Procesando batch #876...\n",
      "✅ Batch #876 escrito: 50000 filas\n",
      "📝 Procesando batch #877...\n",
      "✅ Batch #877 escrito: 50000 filas\n",
      "📝 Procesando batch #878...\n",
      "✅ Batch #878 escrito: 50000 filas\n",
      "📝 Procesando batch #879...\n",
      "✅ Batch #879 escrito: 50000 filas\n",
      "📝 Procesando batch #880...\n",
      "✅ Batch #880 escrito: 50000 filas\n",
      "📝 Procesando batch #881...\n",
      "✅ Batch #881 escrito: 50000 filas\n",
      "📝 Procesando batch #882...\n",
      "✅ Batch #882 escrito: 50000 filas\n",
      "📝 Procesando batch #883...\n",
      "✅ Batch #883 escrito: 50000 filas\n",
      "📝 Procesando batch #884...\n",
      "✅ Batch #884 escrito: 50000 filas\n",
      "📝 Procesando batch #885...\n",
      "✅ Batch #885 escrito: 50000 filas\n",
      "📝 Procesando batch #886...\n",
      "✅ Batch #886 escrito: 50000 filas\n",
      "📝 Procesando batch #887...\n",
      "✅ Batch #887 escrito: 50000 filas\n",
      "📝 Procesando batch #888...\n",
      "✅ Batch #888 escrito: 7454 filas\n",
      "📝 Procesando batch #889...\n",
      "✅ Batch #889 escrito: 50000 filas\n",
      "📝 Procesando batch #890...\n",
      "✅ Batch #890 escrito: 50000 filas\n",
      "📝 Procesando batch #891...\n",
      "✅ Batch #891 escrito: 50000 filas\n",
      "📝 Procesando batch #892...\n",
      "✅ Batch #892 escrito: 50000 filas\n",
      "📝 Procesando batch #893...\n",
      "✅ Batch #893 escrito: 50000 filas\n",
      "📝 Procesando batch #894...\n",
      "✅ Batch #894 escrito: 50000 filas\n",
      "📝 Procesando batch #895...\n",
      "✅ Batch #895 escrito: 50000 filas\n",
      "📝 Procesando batch #896...\n",
      "✅ Batch #896 escrito: 50000 filas\n",
      "📝 Procesando batch #897...\n",
      "✅ Batch #897 escrito: 50000 filas\n",
      "📝 Procesando batch #898...\n",
      "✅ Batch #898 escrito: 50000 filas\n",
      "📝 Procesando batch #899...\n",
      "✅ Batch #899 escrito: 50000 filas\n",
      "📝 Procesando batch #900...\n",
      "✅ Batch #900 escrito: 50000 filas\n",
      "📝 Procesando batch #901...\n",
      "✅ Batch #901 escrito: 50000 filas\n",
      "📝 Procesando batch #902...\n",
      "✅ Batch #902 escrito: 50000 filas\n",
      "📝 Procesando batch #903...\n",
      "✅ Batch #903 escrito: 50000 filas\n",
      "📝 Procesando batch #904...\n",
      "✅ Batch #904 escrito: 50000 filas\n",
      "📝 Procesando batch #905...\n",
      "✅ Batch #905 escrito: 50000 filas\n",
      "📝 Procesando batch #906...\n",
      "✅ Batch #906 escrito: 50000 filas\n",
      "📝 Procesando batch #907...\n",
      "✅ Batch #907 escrito: 50000 filas\n",
      "📝 Procesando batch #908...\n",
      "✅ Batch #908 escrito: 50000 filas\n",
      "📝 Procesando batch #909...\n",
      "✅ Batch #909 escrito: 48576 filas\n",
      "📝 Procesando batch #910...\n",
      "✅ Batch #910 escrito: 50000 filas\n",
      "📝 Procesando batch #911...\n",
      "✅ Batch #911 escrito: 50000 filas\n",
      "📝 Procesando batch #912...\n",
      "✅ Batch #912 escrito: 50000 filas\n",
      "📝 Procesando batch #913...\n",
      "✅ Batch #913 escrito: 50000 filas\n",
      "📝 Procesando batch #914...\n",
      "✅ Batch #914 escrito: 50000 filas\n",
      "📝 Procesando batch #915...\n",
      "✅ Batch #915 escrito: 50000 filas\n",
      "📝 Procesando batch #916...\n",
      "✅ Batch #916 escrito: 50000 filas\n",
      "📝 Procesando batch #917...\n",
      "✅ Batch #917 escrito: 50000 filas\n",
      "📝 Procesando batch #918...\n",
      "✅ Batch #918 escrito: 50000 filas\n",
      "📝 Procesando batch #919...\n",
      "✅ Batch #919 escrito: 50000 filas\n",
      "📝 Procesando batch #920...\n",
      "✅ Batch #920 escrito: 50000 filas\n",
      "📝 Procesando batch #921...\n",
      "✅ Batch #921 escrito: 50000 filas\n",
      "📝 Procesando batch #922...\n",
      "✅ Batch #922 escrito: 50000 filas\n",
      "📝 Procesando batch #923...\n",
      "✅ Batch #923 escrito: 50000 filas\n",
      "📝 Procesando batch #924...\n",
      "✅ Batch #924 escrito: 50000 filas\n",
      "📝 Procesando batch #925...\n",
      "✅ Batch #925 escrito: 50000 filas\n",
      "📝 Procesando batch #926...\n",
      "✅ Batch #926 escrito: 22909 filas\n",
      "📝 Procesando batch #927...\n",
      "✅ Batch #927 escrito: 50000 filas\n",
      "📝 Procesando batch #928...\n",
      "✅ Batch #928 escrito: 50000 filas\n",
      "📝 Procesando batch #929...\n",
      "✅ Batch #929 escrito: 50000 filas\n",
      "📝 Procesando batch #930...\n",
      "✅ Batch #930 escrito: 50000 filas\n",
      "📝 Procesando batch #931...\n",
      "✅ Batch #931 escrito: 50000 filas\n",
      "📝 Procesando batch #932...\n",
      "✅ Batch #932 escrito: 50000 filas\n",
      "📝 Procesando batch #933...\n",
      "✅ Batch #933 escrito: 50000 filas\n",
      "📝 Procesando batch #934...\n",
      "✅ Batch #934 escrito: 50000 filas\n",
      "📝 Procesando batch #935...\n",
      "✅ Batch #935 escrito: 50000 filas\n",
      "📝 Procesando batch #936...\n",
      "✅ Batch #936 escrito: 50000 filas\n",
      "📝 Procesando batch #937...\n",
      "✅ Batch #937 escrito: 50000 filas\n",
      "📝 Procesando batch #938...\n",
      "✅ Batch #938 escrito: 50000 filas\n",
      "📝 Procesando batch #939...\n",
      "✅ Batch #939 escrito: 50000 filas\n",
      "📝 Procesando batch #940...\n",
      "✅ Batch #940 escrito: 50000 filas\n",
      "📝 Procesando batch #941...\n",
      "✅ Batch #941 escrito: 50000 filas\n",
      "📝 Procesando batch #942...\n",
      "✅ Batch #942 escrito: 50000 filas\n",
      "📝 Procesando batch #943...\n",
      "✅ Batch #943 escrito: 50000 filas\n",
      "📝 Procesando batch #944...\n",
      "✅ Batch #944 escrito: 50000 filas\n",
      "📝 Procesando batch #945...\n",
      "✅ Batch #945 escrito: 50000 filas\n",
      "📝 Procesando batch #946...\n",
      "✅ Batch #946 escrito: 50000 filas\n",
      "📝 Procesando batch #947...\n",
      "✅ Batch #947 escrito: 48576 filas\n",
      "📝 Procesando batch #948...\n",
      "✅ Batch #948 escrito: 50000 filas\n",
      "📝 Procesando batch #949...\n",
      "✅ Batch #949 escrito: 50000 filas\n",
      "📝 Procesando batch #950...\n",
      "✅ Batch #950 escrito: 50000 filas\n",
      "📝 Procesando batch #951...\n",
      "✅ Batch #951 escrito: 50000 filas\n",
      "📝 Procesando batch #952...\n",
      "✅ Batch #952 escrito: 50000 filas\n",
      "📝 Procesando batch #953...\n",
      "✅ Batch #953 escrito: 50000 filas\n",
      "📝 Procesando batch #954...\n",
      "✅ Batch #954 escrito: 50000 filas\n",
      "📝 Procesando batch #955...\n",
      "✅ Batch #955 escrito: 50000 filas\n",
      "📝 Procesando batch #956...\n",
      "✅ Batch #956 escrito: 50000 filas\n",
      "📝 Procesando batch #957...\n",
      "✅ Batch #957 escrito: 50000 filas\n",
      "📝 Procesando batch #958...\n",
      "✅ Batch #958 escrito: 50000 filas\n",
      "📝 Procesando batch #959...\n",
      "✅ Batch #959 escrito: 50000 filas\n",
      "📝 Procesando batch #960...\n",
      "✅ Batch #960 escrito: 50000 filas\n",
      "📝 Procesando batch #961...\n",
      "✅ Batch #961 escrito: 50000 filas\n",
      "📝 Procesando batch #962...\n",
      "✅ Batch #962 escrito: 50000 filas\n",
      "📝 Procesando batch #963...\n",
      "✅ Batch #963 escrito: 32373 filas\n",
      "📝 Procesando batch #964...\n",
      "✅ Batch #964 escrito: 50000 filas\n",
      "📝 Procesando batch #965...\n",
      "✅ Batch #965 escrito: 50000 filas\n",
      "📝 Procesando batch #966...\n",
      "✅ Batch #966 escrito: 50000 filas\n",
      "📝 Procesando batch #967...\n",
      "✅ Batch #967 escrito: 50000 filas\n",
      "📝 Procesando batch #968...\n",
      "✅ Batch #968 escrito: 50000 filas\n",
      "📝 Procesando batch #969...\n",
      "✅ Batch #969 escrito: 50000 filas\n",
      "📝 Procesando batch #970...\n",
      "✅ Batch #970 escrito: 50000 filas\n",
      "📝 Procesando batch #971...\n",
      "✅ Batch #971 escrito: 50000 filas\n",
      "📝 Procesando batch #972...\n",
      "✅ Batch #972 escrito: 50000 filas\n",
      "📝 Procesando batch #973...\n",
      "✅ Batch #973 escrito: 50000 filas\n",
      "📝 Procesando batch #974...\n",
      "✅ Batch #974 escrito: 50000 filas\n",
      "📝 Procesando batch #975...\n",
      "✅ Batch #975 escrito: 50000 filas\n",
      "📝 Procesando batch #976...\n",
      "✅ Batch #976 escrito: 50000 filas\n",
      "📝 Procesando batch #977...\n",
      "✅ Batch #977 escrito: 50000 filas\n",
      "📝 Procesando batch #978...\n",
      "✅ Batch #978 escrito: 50000 filas\n",
      "📝 Procesando batch #979...\n",
      "✅ Batch #979 escrito: 50000 filas\n",
      "📝 Procesando batch #980...\n",
      "✅ Batch #980 escrito: 50000 filas\n",
      "📝 Procesando batch #981...\n",
      "✅ Batch #981 escrito: 50000 filas\n",
      "📝 Procesando batch #982...\n",
      "✅ Batch #982 escrito: 50000 filas\n",
      "📝 Procesando batch #983...\n",
      "✅ Batch #983 escrito: 50000 filas\n",
      "📝 Procesando batch #984...\n",
      "✅ Batch #984 escrito: 48576 filas\n",
      "📝 Procesando batch #985...\n",
      "✅ Batch #985 escrito: 50000 filas\n",
      "📝 Procesando batch #986...\n",
      "✅ Batch #986 escrito: 50000 filas\n",
      "📝 Procesando batch #987...\n",
      "✅ Batch #987 escrito: 50000 filas\n",
      "📝 Procesando batch #988...\n",
      "✅ Batch #988 escrito: 50000 filas\n",
      "📝 Procesando batch #989...\n",
      "✅ Batch #989 escrito: 50000 filas\n",
      "📝 Procesando batch #990...\n",
      "✅ Batch #990 escrito: 50000 filas\n",
      "📝 Procesando batch #991...\n",
      "✅ Batch #991 escrito: 50000 filas\n",
      "📝 Procesando batch #992...\n",
      "✅ Batch #992 escrito: 50000 filas\n",
      "📝 Procesando batch #993...\n",
      "✅ Batch #993 escrito: 50000 filas\n",
      "📝 Procesando batch #994...\n",
      "✅ Batch #994 escrito: 50000 filas\n",
      "📝 Procesando batch #995...\n",
      "✅ Batch #995 escrito: 50000 filas\n",
      "📝 Procesando batch #996...\n",
      "✅ Batch #996 escrito: 50000 filas\n",
      "📝 Procesando batch #997...\n",
      "✅ Batch #997 escrito: 50000 filas\n",
      "📝 Procesando batch #998...\n",
      "✅ Batch #998 escrito: 50000 filas\n",
      "📝 Procesando batch #999...\n",
      "✅ Batch #999 escrito: 50000 filas\n",
      "📝 Procesando batch #1000...\n",
      "✅ Batch #1000 escrito: 50000 filas\n",
      "📝 Procesando batch #1001...\n",
      "✅ Batch #1001 escrito: 10846 filas\n",
      "📝 Procesando batch #1002...\n",
      "✅ Batch #1002 escrito: 50000 filas\n",
      "📝 Procesando batch #1003...\n",
      "✅ Batch #1003 escrito: 50000 filas\n",
      "📝 Procesando batch #1004...\n",
      "✅ Batch #1004 escrito: 50000 filas\n",
      "📝 Procesando batch #1005...\n",
      "✅ Batch #1005 escrito: 50000 filas\n",
      "📝 Procesando batch #1006...\n",
      "✅ Batch #1006 escrito: 50000 filas\n",
      "📝 Procesando batch #1007...\n",
      "✅ Batch #1007 escrito: 50000 filas\n",
      "📝 Procesando batch #1008...\n",
      "✅ Batch #1008 escrito: 50000 filas\n",
      "📝 Procesando batch #1009...\n",
      "✅ Batch #1009 escrito: 50000 filas\n",
      "📝 Procesando batch #1010...\n",
      "✅ Batch #1010 escrito: 50000 filas\n",
      "📝 Procesando batch #1011...\n",
      "✅ Batch #1011 escrito: 50000 filas\n",
      "📝 Procesando batch #1012...\n",
      "✅ Batch #1012 escrito: 50000 filas\n",
      "📝 Procesando batch #1013...\n",
      "✅ Batch #1013 escrito: 50000 filas\n",
      "📝 Procesando batch #1014...\n",
      "✅ Batch #1014 escrito: 50000 filas\n",
      "📝 Procesando batch #1015...\n",
      "✅ Batch #1015 escrito: 50000 filas\n",
      "📝 Procesando batch #1016...\n",
      "✅ Batch #1016 escrito: 50000 filas\n",
      "📝 Procesando batch #1017...\n",
      "✅ Batch #1017 escrito: 50000 filas\n",
      "📝 Procesando batch #1018...\n",
      "✅ Batch #1018 escrito: 50000 filas\n",
      "📝 Procesando batch #1019...\n",
      "✅ Batch #1019 escrito: 50000 filas\n",
      "📝 Procesando batch #1020...\n",
      "✅ Batch #1020 escrito: 50000 filas\n",
      "📝 Procesando batch #1021...\n",
      "✅ Batch #1021 escrito: 50000 filas\n",
      "📝 Procesando batch #1022...\n",
      "✅ Batch #1022 escrito: 48576 filas\n",
      "📝 Procesando batch #1023...\n",
      "✅ Batch #1023 escrito: 50000 filas\n",
      "📝 Procesando batch #1024...\n",
      "✅ Batch #1024 escrito: 50000 filas\n",
      "📝 Procesando batch #1025...\n",
      "✅ Batch #1025 escrito: 50000 filas\n",
      "📝 Procesando batch #1026...\n",
      "✅ Batch #1026 escrito: 50000 filas\n",
      "📝 Procesando batch #1027...\n",
      "✅ Batch #1027 escrito: 50000 filas\n",
      "📝 Procesando batch #1028...\n",
      "✅ Batch #1028 escrito: 50000 filas\n",
      "📝 Procesando batch #1029...\n",
      "✅ Batch #1029 escrito: 50000 filas\n",
      "📝 Procesando batch #1030...\n",
      "✅ Batch #1030 escrito: 50000 filas\n",
      "📝 Procesando batch #1031...\n",
      "✅ Batch #1031 escrito: 50000 filas\n",
      "📝 Procesando batch #1032...\n",
      "✅ Batch #1032 escrito: 50000 filas\n",
      "📝 Procesando batch #1033...\n",
      "✅ Batch #1033 escrito: 50000 filas\n",
      "📝 Procesando batch #1034...\n",
      "✅ Batch #1034 escrito: 50000 filas\n",
      "📝 Procesando batch #1035...\n",
      "✅ Batch #1035 escrito: 50000 filas\n",
      "📝 Procesando batch #1036...\n",
      "✅ Batch #1036 escrito: 50000 filas\n",
      "📝 Procesando batch #1037...\n",
      "✅ Batch #1037 escrito: 50000 filas\n",
      "📝 Procesando batch #1038...\n",
      "✅ Batch #1038 escrito: 32269 filas\n",
      "📝 Procesando batch #1039...\n",
      "✅ Batch #1039 escrito: 50000 filas\n",
      "📝 Procesando batch #1040...\n",
      "✅ Batch #1040 escrito: 50000 filas\n",
      "📝 Procesando batch #1041...\n",
      "✅ Batch #1041 escrito: 50000 filas\n",
      "📝 Procesando batch #1042...\n",
      "✅ Batch #1042 escrito: 50000 filas\n",
      "📝 Procesando batch #1043...\n",
      "✅ Batch #1043 escrito: 50000 filas\n",
      "📝 Procesando batch #1044...\n",
      "✅ Batch #1044 escrito: 50000 filas\n",
      "📝 Procesando batch #1045...\n",
      "✅ Batch #1045 escrito: 50000 filas\n",
      "📝 Procesando batch #1046...\n",
      "✅ Batch #1046 escrito: 50000 filas\n",
      "📝 Procesando batch #1047...\n",
      "✅ Batch #1047 escrito: 50000 filas\n",
      "📝 Procesando batch #1048...\n",
      "✅ Batch #1048 escrito: 50000 filas\n",
      "📝 Procesando batch #1049...\n",
      "✅ Batch #1049 escrito: 50000 filas\n",
      "📝 Procesando batch #1050...\n",
      "✅ Batch #1050 escrito: 50000 filas\n",
      "📝 Procesando batch #1051...\n",
      "✅ Batch #1051 escrito: 50000 filas\n",
      "📝 Procesando batch #1052...\n",
      "✅ Batch #1052 escrito: 50000 filas\n",
      "📝 Procesando batch #1053...\n",
      "✅ Batch #1053 escrito: 50000 filas\n",
      "📝 Procesando batch #1054...\n",
      "✅ Batch #1054 escrito: 50000 filas\n",
      "📝 Procesando batch #1055...\n",
      "✅ Batch #1055 escrito: 50000 filas\n",
      "📝 Procesando batch #1056...\n",
      "✅ Batch #1056 escrito: 50000 filas\n",
      "📝 Procesando batch #1057...\n",
      "✅ Batch #1057 escrito: 50000 filas\n",
      "📝 Procesando batch #1058...\n",
      "✅ Batch #1058 escrito: 50000 filas\n",
      "📝 Procesando batch #1059...\n",
      "✅ Batch #1059 escrito: 48576 filas\n",
      "📝 Procesando batch #1060...\n",
      "✅ Batch #1060 escrito: 50000 filas\n",
      "📝 Procesando batch #1061...\n",
      "✅ Batch #1061 escrito: 50000 filas\n",
      "📝 Procesando batch #1062...\n",
      "✅ Batch #1062 escrito: 50000 filas\n",
      "📝 Procesando batch #1063...\n",
      "✅ Batch #1063 escrito: 50000 filas\n",
      "📝 Procesando batch #1064...\n",
      "✅ Batch #1064 escrito: 50000 filas\n",
      "📝 Procesando batch #1065...\n",
      "✅ Batch #1065 escrito: 50000 filas\n",
      "📝 Procesando batch #1066...\n",
      "✅ Batch #1066 escrito: 50000 filas\n",
      "📝 Procesando batch #1067...\n",
      "✅ Batch #1067 escrito: 50000 filas\n",
      "📝 Procesando batch #1068...\n",
      "✅ Batch #1068 escrito: 50000 filas\n",
      "📝 Procesando batch #1069...\n",
      "✅ Batch #1069 escrito: 50000 filas\n",
      "📝 Procesando batch #1070...\n",
      "✅ Batch #1070 escrito: 50000 filas\n",
      "📝 Procesando batch #1071...\n",
      "✅ Batch #1071 escrito: 50000 filas\n",
      "📝 Procesando batch #1072...\n",
      "✅ Batch #1072 escrito: 50000 filas\n",
      "📝 Procesando batch #1073...\n",
      "✅ Batch #1073 escrito: 50000 filas\n",
      "📝 Procesando batch #1074...\n",
      "✅ Batch #1074 escrito: 50000 filas\n",
      "📝 Procesando batch #1075...\n",
      "✅ Batch #1075 escrito: 40702 filas\n",
      "📝 Procesando batch #1076...\n",
      "✅ Batch #1076 escrito: 50000 filas\n",
      "📝 Procesando batch #1077...\n",
      "✅ Batch #1077 escrito: 50000 filas\n",
      "📝 Procesando batch #1078...\n",
      "✅ Batch #1078 escrito: 50000 filas\n",
      "📝 Procesando batch #1079...\n",
      "✅ Batch #1079 escrito: 50000 filas\n",
      "📝 Procesando batch #1080...\n",
      "✅ Batch #1080 escrito: 50000 filas\n",
      "📝 Procesando batch #1081...\n",
      "✅ Batch #1081 escrito: 50000 filas\n",
      "📝 Procesando batch #1082...\n",
      "✅ Batch #1082 escrito: 50000 filas\n",
      "📝 Procesando batch #1083...\n",
      "✅ Batch #1083 escrito: 50000 filas\n",
      "📝 Procesando batch #1084...\n",
      "✅ Batch #1084 escrito: 50000 filas\n",
      "📝 Procesando batch #1085...\n",
      "✅ Batch #1085 escrito: 50000 filas\n",
      "📝 Procesando batch #1086...\n",
      "✅ Batch #1086 escrito: 50000 filas\n",
      "📝 Procesando batch #1087...\n",
      "✅ Batch #1087 escrito: 50000 filas\n",
      "📝 Procesando batch #1088...\n",
      "✅ Batch #1088 escrito: 50000 filas\n",
      "📝 Procesando batch #1089...\n",
      "✅ Batch #1089 escrito: 50000 filas\n",
      "📝 Procesando batch #1090...\n",
      "✅ Batch #1090 escrito: 50000 filas\n",
      "📝 Procesando batch #1091...\n",
      "✅ Batch #1091 escrito: 50000 filas\n",
      "📝 Procesando batch #1092...\n",
      "✅ Batch #1092 escrito: 50000 filas\n",
      "📝 Procesando batch #1093...\n",
      "✅ Batch #1093 escrito: 50000 filas\n",
      "📝 Procesando batch #1094...\n",
      "✅ Batch #1094 escrito: 50000 filas\n",
      "📝 Procesando batch #1095...\n",
      "✅ Batch #1095 escrito: 50000 filas\n",
      "📝 Procesando batch #1096...\n",
      "✅ Batch #1096 escrito: 48576 filas\n",
      "📝 Procesando batch #1097...\n",
      "✅ Batch #1097 escrito: 50000 filas\n",
      "📝 Procesando batch #1098...\n",
      "✅ Batch #1098 escrito: 50000 filas\n",
      "📝 Procesando batch #1099...\n",
      "✅ Batch #1099 escrito: 50000 filas\n",
      "📝 Procesando batch #1100...\n",
      "✅ Batch #1100 escrito: 50000 filas\n",
      "📝 Procesando batch #1101...\n",
      "✅ Batch #1101 escrito: 50000 filas\n",
      "📝 Procesando batch #1102...\n",
      "✅ Batch #1102 escrito: 50000 filas\n",
      "📝 Procesando batch #1103...\n",
      "✅ Batch #1103 escrito: 50000 filas\n",
      "📝 Procesando batch #1104...\n",
      "✅ Batch #1104 escrito: 50000 filas\n",
      "📝 Procesando batch #1105...\n",
      "✅ Batch #1105 escrito: 50000 filas\n",
      "📝 Procesando batch #1106...\n",
      "✅ Batch #1106 escrito: 50000 filas\n",
      "📝 Procesando batch #1107...\n",
      "✅ Batch #1107 escrito: 50000 filas\n",
      "📝 Procesando batch #1108...\n",
      "✅ Batch #1108 escrito: 50000 filas\n",
      "📝 Procesando batch #1109...\n",
      "✅ Batch #1109 escrito: 50000 filas\n",
      "📝 Procesando batch #1110...\n",
      "✅ Batch #1110 escrito: 50000 filas\n",
      "📝 Procesando batch #1111...\n",
      "✅ Batch #1111 escrito: 50000 filas\n",
      "📝 Procesando batch #1112...\n",
      "✅ Batch #1112 escrito: 25788 filas\n",
      "📝 Procesando batch #1113...\n",
      "✅ Batch #1113 escrito: 50000 filas\n",
      "📝 Procesando batch #1114...\n",
      "✅ Batch #1114 escrito: 50000 filas\n",
      "📝 Procesando batch #1115...\n",
      "✅ Batch #1115 escrito: 50000 filas\n",
      "📝 Procesando batch #1116...\n",
      "✅ Batch #1116 escrito: 50000 filas\n",
      "📝 Procesando batch #1117...\n",
      "✅ Batch #1117 escrito: 50000 filas\n",
      "📝 Procesando batch #1118...\n",
      "✅ Batch #1118 escrito: 50000 filas\n",
      "📝 Procesando batch #1119...\n",
      "✅ Batch #1119 escrito: 50000 filas\n",
      "📝 Procesando batch #1120...\n",
      "✅ Batch #1120 escrito: 50000 filas\n",
      "📝 Procesando batch #1121...\n",
      "✅ Batch #1121 escrito: 50000 filas\n",
      "📝 Procesando batch #1122...\n",
      "✅ Batch #1122 escrito: 50000 filas\n",
      "📝 Procesando batch #1123...\n",
      "✅ Batch #1123 escrito: 50000 filas\n",
      "📝 Procesando batch #1124...\n",
      "✅ Batch #1124 escrito: 50000 filas\n",
      "📝 Procesando batch #1125...\n",
      "✅ Batch #1125 escrito: 50000 filas\n",
      "📝 Procesando batch #1126...\n",
      "✅ Batch #1126 escrito: 50000 filas\n",
      "📝 Procesando batch #1127...\n",
      "✅ Batch #1127 escrito: 50000 filas\n",
      "📝 Procesando batch #1128...\n",
      "✅ Batch #1128 escrito: 50000 filas\n",
      "📝 Procesando batch #1129...\n",
      "✅ Batch #1129 escrito: 50000 filas\n",
      "📝 Procesando batch #1130...\n",
      "✅ Batch #1130 escrito: 50000 filas\n",
      "📝 Procesando batch #1131...\n",
      "✅ Batch #1131 escrito: 50000 filas\n",
      "📝 Procesando batch #1132...\n",
      "✅ Batch #1132 escrito: 50000 filas\n",
      "📝 Procesando batch #1133...\n",
      "✅ Batch #1133 escrito: 48576 filas\n",
      "📝 Procesando batch #1134...\n",
      "✅ Batch #1134 escrito: 50000 filas\n",
      "📝 Procesando batch #1135...\n",
      "✅ Batch #1135 escrito: 50000 filas\n",
      "📝 Procesando batch #1136...\n",
      "✅ Batch #1136 escrito: 50000 filas\n",
      "📝 Procesando batch #1137...\n",
      "✅ Batch #1137 escrito: 50000 filas\n",
      "📝 Procesando batch #1138...\n",
      "✅ Batch #1138 escrito: 50000 filas\n",
      "📝 Procesando batch #1139...\n",
      "✅ Batch #1139 escrito: 50000 filas\n",
      "📝 Procesando batch #1140...\n",
      "✅ Batch #1140 escrito: 50000 filas\n",
      "📝 Procesando batch #1141...\n",
      "✅ Batch #1141 escrito: 50000 filas\n",
      "📝 Procesando batch #1142...\n",
      "✅ Batch #1142 escrito: 50000 filas\n",
      "📝 Procesando batch #1143...\n",
      "✅ Batch #1143 escrito: 50000 filas\n",
      "📝 Procesando batch #1144...\n",
      "✅ Batch #1144 escrito: 50000 filas\n",
      "📝 Procesando batch #1145...\n",
      "✅ Batch #1145 escrito: 50000 filas\n",
      "📝 Procesando batch #1146...\n",
      "✅ Batch #1146 escrito: 50000 filas\n",
      "📝 Procesando batch #1147...\n",
      "✅ Batch #1147 escrito: 50000 filas\n",
      "📝 Procesando batch #1148...\n",
      "✅ Batch #1148 escrito: 50000 filas\n",
      "📝 Procesando batch #1149...\n",
      "✅ Batch #1149 escrito: 50000 filas\n",
      "📝 Procesando batch #1150...\n",
      "✅ Batch #1150 escrito: 344 filas\n",
      "📝 Procesando batch #1151...\n",
      "✅ Batch #1151 escrito: 50000 filas\n",
      "📝 Procesando batch #1152...\n",
      "✅ Batch #1152 escrito: 50000 filas\n",
      "📝 Procesando batch #1153...\n",
      "✅ Batch #1153 escrito: 50000 filas\n",
      "📝 Procesando batch #1154...\n",
      "✅ Batch #1154 escrito: 50000 filas\n",
      "📝 Procesando batch #1155...\n",
      "✅ Batch #1155 escrito: 50000 filas\n",
      "📝 Procesando batch #1156...\n",
      "✅ Batch #1156 escrito: 50000 filas\n",
      "📝 Procesando batch #1157...\n",
      "✅ Batch #1157 escrito: 50000 filas\n",
      "📝 Procesando batch #1158...\n",
      "✅ Batch #1158 escrito: 50000 filas\n",
      "📝 Procesando batch #1159...\n",
      "✅ Batch #1159 escrito: 50000 filas\n",
      "📝 Procesando batch #1160...\n",
      "✅ Batch #1160 escrito: 50000 filas\n",
      "📝 Procesando batch #1161...\n",
      "✅ Batch #1161 escrito: 50000 filas\n",
      "📝 Procesando batch #1162...\n",
      "✅ Batch #1162 escrito: 50000 filas\n",
      "📝 Procesando batch #1163...\n",
      "✅ Batch #1163 escrito: 50000 filas\n",
      "📝 Procesando batch #1164...\n",
      "✅ Batch #1164 escrito: 50000 filas\n",
      "📝 Procesando batch #1165...\n",
      "✅ Batch #1165 escrito: 50000 filas\n",
      "📝 Procesando batch #1166...\n",
      "✅ Batch #1166 escrito: 50000 filas\n",
      "📝 Procesando batch #1167...\n",
      "✅ Batch #1167 escrito: 50000 filas\n",
      "📝 Procesando batch #1168...\n",
      "✅ Batch #1168 escrito: 50000 filas\n",
      "📝 Procesando batch #1169...\n",
      "✅ Batch #1169 escrito: 50000 filas\n",
      "📝 Procesando batch #1170...\n",
      "✅ Batch #1170 escrito: 50000 filas\n",
      "📝 Procesando batch #1171...\n",
      "✅ Batch #1171 escrito: 48576 filas\n",
      "📝 Procesando batch #1172...\n",
      "✅ Batch #1172 escrito: 50000 filas\n",
      "📝 Procesando batch #1173...\n",
      "✅ Batch #1173 escrito: 50000 filas\n",
      "📝 Procesando batch #1174...\n",
      "✅ Batch #1174 escrito: 50000 filas\n",
      "📝 Procesando batch #1175...\n",
      "✅ Batch #1175 escrito: 50000 filas\n",
      "📝 Procesando batch #1176...\n",
      "✅ Batch #1176 escrito: 50000 filas\n",
      "📝 Procesando batch #1177...\n",
      "✅ Batch #1177 escrito: 50000 filas\n",
      "📝 Procesando batch #1178...\n",
      "✅ Batch #1178 escrito: 50000 filas\n",
      "📝 Procesando batch #1179...\n",
      "✅ Batch #1179 escrito: 50000 filas\n",
      "📝 Procesando batch #1180...\n",
      "✅ Batch #1180 escrito: 50000 filas\n",
      "📝 Procesando batch #1181...\n",
      "✅ Batch #1181 escrito: 50000 filas\n",
      "📝 Procesando batch #1182...\n",
      "✅ Batch #1182 escrito: 50000 filas\n",
      "📝 Procesando batch #1183...\n",
      "✅ Batch #1183 escrito: 50000 filas\n",
      "📝 Procesando batch #1184...\n",
      "✅ Batch #1184 escrito: 50000 filas\n",
      "📝 Procesando batch #1185...\n",
      "✅ Batch #1185 escrito: 50000 filas\n",
      "📝 Procesando batch #1186...\n",
      "✅ Batch #1186 escrito: 50000 filas\n",
      "📝 Procesando batch #1187...\n",
      "✅ Batch #1187 escrito: 50000 filas\n",
      "📝 Procesando batch #1188...\n",
      "✅ Batch #1188 escrito: 26089 filas\n",
      "📝 Procesando batch #1189...\n",
      "✅ Batch #1189 escrito: 50000 filas\n",
      "📝 Procesando batch #1190...\n",
      "✅ Batch #1190 escrito: 50000 filas\n",
      "📝 Procesando batch #1191...\n",
      "✅ Batch #1191 escrito: 50000 filas\n",
      "📝 Procesando batch #1192...\n",
      "✅ Batch #1192 escrito: 50000 filas\n",
      "📝 Procesando batch #1193...\n",
      "✅ Batch #1193 escrito: 50000 filas\n",
      "📝 Procesando batch #1194...\n",
      "✅ Batch #1194 escrito: 50000 filas\n",
      "📝 Procesando batch #1195...\n",
      "✅ Batch #1195 escrito: 50000 filas\n",
      "📝 Procesando batch #1196...\n",
      "✅ Batch #1196 escrito: 50000 filas\n",
      "📝 Procesando batch #1197...\n",
      "✅ Batch #1197 escrito: 50000 filas\n",
      "📝 Procesando batch #1198...\n",
      "✅ Batch #1198 escrito: 50000 filas\n",
      "📝 Procesando batch #1199...\n",
      "✅ Batch #1199 escrito: 50000 filas\n",
      "📝 Procesando batch #1200...\n",
      "✅ Batch #1200 escrito: 50000 filas\n",
      "📝 Procesando batch #1201...\n",
      "✅ Batch #1201 escrito: 50000 filas\n",
      "📝 Procesando batch #1202...\n",
      "✅ Batch #1202 escrito: 50000 filas\n",
      "📝 Procesando batch #1203...\n",
      "✅ Batch #1203 escrito: 50000 filas\n",
      "📝 Procesando batch #1204...\n",
      "✅ Batch #1204 escrito: 50000 filas\n",
      "📝 Procesando batch #1205...\n",
      "✅ Batch #1205 escrito: 50000 filas\n",
      "📝 Procesando batch #1206...\n",
      "✅ Batch #1206 escrito: 50000 filas\n",
      "📝 Procesando batch #1207...\n",
      "✅ Batch #1207 escrito: 50000 filas\n",
      "📝 Procesando batch #1208...\n",
      "✅ Batch #1208 escrito: 50000 filas\n",
      "📝 Procesando batch #1209...\n",
      "✅ Batch #1209 escrito: 48576 filas\n",
      "📝 Procesando batch #1210...\n",
      "✅ Batch #1210 escrito: 50000 filas\n",
      "📝 Procesando batch #1211...\n",
      "✅ Batch #1211 escrito: 50000 filas\n",
      "📝 Procesando batch #1212...\n",
      "✅ Batch #1212 escrito: 50000 filas\n",
      "📝 Procesando batch #1213...\n",
      "✅ Batch #1213 escrito: 50000 filas\n",
      "📝 Procesando batch #1214...\n",
      "✅ Batch #1214 escrito: 50000 filas\n",
      "📝 Procesando batch #1215...\n",
      "✅ Batch #1215 escrito: 50000 filas\n",
      "📝 Procesando batch #1216...\n",
      "✅ Batch #1216 escrito: 50000 filas\n",
      "📝 Procesando batch #1217...\n",
      "✅ Batch #1217 escrito: 50000 filas\n",
      "📝 Procesando batch #1218...\n",
      "✅ Batch #1218 escrito: 50000 filas\n",
      "📝 Procesando batch #1219...\n",
      "✅ Batch #1219 escrito: 50000 filas\n",
      "📝 Procesando batch #1220...\n",
      "✅ Batch #1220 escrito: 50000 filas\n",
      "📝 Procesando batch #1221...\n",
      "✅ Batch #1221 escrito: 50000 filas\n",
      "📝 Procesando batch #1222...\n",
      "✅ Batch #1222 escrito: 50000 filas\n",
      "📝 Procesando batch #1223...\n",
      "✅ Batch #1223 escrito: 50000 filas\n",
      "📝 Procesando batch #1224...\n",
      "✅ Batch #1224 escrito: 50000 filas\n",
      "📝 Procesando batch #1225...\n",
      "✅ Batch #1225 escrito: 50000 filas\n",
      "📝 Procesando batch #1226...\n",
      "✅ Batch #1226 escrito: 38861 filas\n",
      "📝 Procesando batch #1227...\n",
      "✅ Batch #1227 escrito: 50000 filas\n",
      "📝 Procesando batch #1228...\n",
      "✅ Batch #1228 escrito: 50000 filas\n",
      "📝 Procesando batch #1229...\n",
      "✅ Batch #1229 escrito: 50000 filas\n",
      "📝 Procesando batch #1230...\n",
      "✅ Batch #1230 escrito: 50000 filas\n",
      "📝 Procesando batch #1231...\n",
      "✅ Batch #1231 escrito: 50000 filas\n",
      "📝 Procesando batch #1232...\n",
      "✅ Batch #1232 escrito: 50000 filas\n",
      "📝 Procesando batch #1233...\n",
      "✅ Batch #1233 escrito: 50000 filas\n",
      "📝 Procesando batch #1234...\n",
      "✅ Batch #1234 escrito: 50000 filas\n",
      "📝 Procesando batch #1235...\n",
      "✅ Batch #1235 escrito: 50000 filas\n",
      "📝 Procesando batch #1236...\n",
      "✅ Batch #1236 escrito: 50000 filas\n",
      "📝 Procesando batch #1237...\n",
      "✅ Batch #1237 escrito: 50000 filas\n",
      "📝 Procesando batch #1238...\n",
      "✅ Batch #1238 escrito: 50000 filas\n",
      "📝 Procesando batch #1239...\n",
      "✅ Batch #1239 escrito: 50000 filas\n",
      "📝 Procesando batch #1240...\n",
      "✅ Batch #1240 escrito: 50000 filas\n",
      "📝 Procesando batch #1241...\n",
      "✅ Batch #1241 escrito: 50000 filas\n",
      "📝 Procesando batch #1242...\n",
      "✅ Batch #1242 escrito: 50000 filas\n",
      "📝 Procesando batch #1243...\n",
      "✅ Batch #1243 escrito: 50000 filas\n",
      "📝 Procesando batch #1244...\n",
      "✅ Batch #1244 escrito: 50000 filas\n",
      "📝 Procesando batch #1245...\n",
      "✅ Batch #1245 escrito: 50000 filas\n",
      "📝 Procesando batch #1246...\n",
      "✅ Batch #1246 escrito: 50000 filas\n",
      "📝 Procesando batch #1247...\n",
      "✅ Batch #1247 escrito: 48576 filas\n",
      "📝 Procesando batch #1248...\n",
      "✅ Batch #1248 escrito: 50000 filas\n",
      "📝 Procesando batch #1249...\n",
      "✅ Batch #1249 escrito: 50000 filas\n",
      "📝 Procesando batch #1250...\n",
      "✅ Batch #1250 escrito: 50000 filas\n",
      "📝 Procesando batch #1251...\n",
      "✅ Batch #1251 escrito: 50000 filas\n",
      "📝 Procesando batch #1252...\n",
      "✅ Batch #1252 escrito: 50000 filas\n",
      "📝 Procesando batch #1253...\n",
      "✅ Batch #1253 escrito: 50000 filas\n",
      "📝 Procesando batch #1254...\n",
      "✅ Batch #1254 escrito: 50000 filas\n",
      "📝 Procesando batch #1255...\n",
      "✅ Batch #1255 escrito: 50000 filas\n",
      "📝 Procesando batch #1256...\n",
      "✅ Batch #1256 escrito: 50000 filas\n",
      "📝 Procesando batch #1257...\n",
      "✅ Batch #1257 escrito: 50000 filas\n",
      "📝 Procesando batch #1258...\n",
      "✅ Batch #1258 escrito: 50000 filas\n",
      "📝 Procesando batch #1259...\n",
      "✅ Batch #1259 escrito: 50000 filas\n",
      "📝 Procesando batch #1260...\n",
      "✅ Batch #1260 escrito: 50000 filas\n",
      "📝 Procesando batch #1261...\n",
      "✅ Batch #1261 escrito: 50000 filas\n",
      "📝 Procesando batch #1262...\n",
      "✅ Batch #1262 escrito: 50000 filas\n",
      "📝 Procesando batch #1263...\n",
      "✅ Batch #1263 escrito: 50000 filas\n",
      "📝 Procesando batch #1264...\n",
      "✅ Batch #1264 escrito: 492 filas\n",
      "📝 Procesando batch #1265...\n",
      "✅ Batch #1265 escrito: 50000 filas\n",
      "📝 Procesando batch #1266...\n",
      "✅ Batch #1266 escrito: 50000 filas\n",
      "📝 Procesando batch #1267...\n",
      "✅ Batch #1267 escrito: 50000 filas\n",
      "📝 Procesando batch #1268...\n",
      "✅ Batch #1268 escrito: 50000 filas\n",
      "📝 Procesando batch #1269...\n",
      "✅ Batch #1269 escrito: 50000 filas\n",
      "📝 Procesando batch #1270...\n",
      "✅ Batch #1270 escrito: 50000 filas\n",
      "📝 Procesando batch #1271...\n",
      "✅ Batch #1271 escrito: 50000 filas\n",
      "📝 Procesando batch #1272...\n",
      "✅ Batch #1272 escrito: 50000 filas\n",
      "📝 Procesando batch #1273...\n",
      "✅ Batch #1273 escrito: 50000 filas\n",
      "📝 Procesando batch #1274...\n",
      "✅ Batch #1274 escrito: 50000 filas\n",
      "📝 Procesando batch #1275...\n",
      "✅ Batch #1275 escrito: 50000 filas\n",
      "📝 Procesando batch #1276...\n",
      "✅ Batch #1276 escrito: 50000 filas\n",
      "📝 Procesando batch #1277...\n",
      "✅ Batch #1277 escrito: 50000 filas\n",
      "📝 Procesando batch #1278...\n",
      "✅ Batch #1278 escrito: 50000 filas\n",
      "📝 Procesando batch #1279...\n",
      "✅ Batch #1279 escrito: 50000 filas\n",
      "📝 Procesando batch #1280...\n",
      "✅ Batch #1280 escrito: 50000 filas\n",
      "📝 Procesando batch #1281...\n",
      "✅ Batch #1281 escrito: 50000 filas\n",
      "📝 Procesando batch #1282...\n",
      "✅ Batch #1282 escrito: 50000 filas\n",
      "📝 Procesando batch #1283...\n",
      "✅ Batch #1283 escrito: 50000 filas\n",
      "📝 Procesando batch #1284...\n",
      "✅ Batch #1284 escrito: 50000 filas\n",
      "📝 Procesando batch #1285...\n",
      "✅ Batch #1285 escrito: 48576 filas\n",
      "📝 Procesando batch #1286...\n",
      "✅ Batch #1286 escrito: 50000 filas\n",
      "📝 Procesando batch #1287...\n",
      "✅ Batch #1287 escrito: 50000 filas\n",
      "📝 Procesando batch #1288...\n",
      "✅ Batch #1288 escrito: 50000 filas\n",
      "📝 Procesando batch #1289...\n",
      "✅ Batch #1289 escrito: 50000 filas\n",
      "📝 Procesando batch #1290...\n",
      "✅ Batch #1290 escrito: 50000 filas\n",
      "📝 Procesando batch #1291...\n",
      "✅ Batch #1291 escrito: 50000 filas\n",
      "📝 Procesando batch #1292...\n",
      "✅ Batch #1292 escrito: 50000 filas\n",
      "📝 Procesando batch #1293...\n",
      "✅ Batch #1293 escrito: 50000 filas\n",
      "📝 Procesando batch #1294...\n",
      "✅ Batch #1294 escrito: 50000 filas\n",
      "📝 Procesando batch #1295...\n",
      "✅ Batch #1295 escrito: 50000 filas\n",
      "📝 Procesando batch #1296...\n",
      "✅ Batch #1296 escrito: 50000 filas\n",
      "📝 Procesando batch #1297...\n",
      "✅ Batch #1297 escrito: 50000 filas\n",
      "📝 Procesando batch #1298...\n",
      "✅ Batch #1298 escrito: 50000 filas\n",
      "📝 Procesando batch #1299...\n",
      "✅ Batch #1299 escrito: 50000 filas\n",
      "📝 Procesando batch #1300...\n",
      "✅ Batch #1300 escrito: 50000 filas\n",
      "📝 Procesando batch #1301...\n",
      "✅ Batch #1301 escrito: 50000 filas\n",
      "📝 Procesando batch #1302...\n",
      "✅ Batch #1302 escrito: 7877 filas\n",
      "📝 Procesando batch #1303...\n",
      "✅ Batch #1303 escrito: 50000 filas\n",
      "📝 Procesando batch #1304...\n",
      "✅ Batch #1304 escrito: 50000 filas\n",
      "📝 Procesando batch #1305...\n",
      "✅ Batch #1305 escrito: 50000 filas\n",
      "📝 Procesando batch #1306...\n",
      "✅ Batch #1306 escrito: 50000 filas\n",
      "📝 Procesando batch #1307...\n",
      "✅ Batch #1307 escrito: 50000 filas\n",
      "📝 Procesando batch #1308...\n",
      "✅ Batch #1308 escrito: 50000 filas\n",
      "📝 Procesando batch #1309...\n",
      "✅ Batch #1309 escrito: 50000 filas\n",
      "📝 Procesando batch #1310...\n",
      "✅ Batch #1310 escrito: 50000 filas\n",
      "📝 Procesando batch #1311...\n",
      "✅ Batch #1311 escrito: 50000 filas\n",
      "📝 Procesando batch #1312...\n",
      "✅ Batch #1312 escrito: 50000 filas\n",
      "📝 Procesando batch #1313...\n",
      "✅ Batch #1313 escrito: 50000 filas\n",
      "📝 Procesando batch #1314...\n",
      "✅ Batch #1314 escrito: 50000 filas\n",
      "📝 Procesando batch #1315...\n",
      "✅ Batch #1315 escrito: 50000 filas\n",
      "📝 Procesando batch #1316...\n",
      "✅ Batch #1316 escrito: 50000 filas\n",
      "📝 Procesando batch #1317...\n",
      "✅ Batch #1317 escrito: 50000 filas\n",
      "📝 Procesando batch #1318...\n",
      "✅ Batch #1318 escrito: 50000 filas\n",
      "📝 Procesando batch #1319...\n",
      "✅ Batch #1319 escrito: 50000 filas\n",
      "📝 Procesando batch #1320...\n",
      "✅ Batch #1320 escrito: 50000 filas\n",
      "📝 Procesando batch #1321...\n",
      "✅ Batch #1321 escrito: 50000 filas\n",
      "📝 Procesando batch #1322...\n",
      "✅ Batch #1322 escrito: 50000 filas\n",
      "📝 Procesando batch #1323...\n",
      "✅ Batch #1323 escrito: 48576 filas\n",
      "📝 Procesando batch #1324...\n",
      "✅ Batch #1324 escrito: 50000 filas\n",
      "📝 Procesando batch #1325...\n",
      "✅ Batch #1325 escrito: 50000 filas\n",
      "📝 Procesando batch #1326...\n",
      "✅ Batch #1326 escrito: 50000 filas\n",
      "📝 Procesando batch #1327...\n",
      "✅ Batch #1327 escrito: 50000 filas\n",
      "📝 Procesando batch #1328...\n",
      "✅ Batch #1328 escrito: 50000 filas\n",
      "📝 Procesando batch #1329...\n",
      "✅ Batch #1329 escrito: 50000 filas\n",
      "📝 Procesando batch #1330...\n",
      "✅ Batch #1330 escrito: 50000 filas\n",
      "📝 Procesando batch #1331...\n",
      "✅ Batch #1331 escrito: 50000 filas\n",
      "📝 Procesando batch #1332...\n",
      "✅ Batch #1332 escrito: 50000 filas\n",
      "📝 Procesando batch #1333...\n",
      "✅ Batch #1333 escrito: 50000 filas\n",
      "📝 Procesando batch #1334...\n",
      "✅ Batch #1334 escrito: 50000 filas\n",
      "📝 Procesando batch #1335...\n",
      "✅ Batch #1335 escrito: 50000 filas\n",
      "📝 Procesando batch #1336...\n",
      "✅ Batch #1336 escrito: 50000 filas\n",
      "📝 Procesando batch #1337...\n",
      "✅ Batch #1337 escrito: 50000 filas\n",
      "📝 Procesando batch #1338...\n",
      "✅ Batch #1338 escrito: 50000 filas\n",
      "📝 Procesando batch #1339...\n",
      "✅ Batch #1339 escrito: 50000 filas\n",
      "📝 Procesando batch #1340...\n",
      "✅ Batch #1340 escrito: 31071 filas\n",
      "📝 Procesando batch #1341...\n",
      "✅ Batch #1341 escrito: 50000 filas\n",
      "📝 Procesando batch #1342...\n",
      "✅ Batch #1342 escrito: 50000 filas\n",
      "📝 Procesando batch #1343...\n",
      "✅ Batch #1343 escrito: 50000 filas\n",
      "📝 Procesando batch #1344...\n",
      "✅ Batch #1344 escrito: 50000 filas\n",
      "📝 Procesando batch #1345...\n",
      "✅ Batch #1345 escrito: 50000 filas\n",
      "📝 Procesando batch #1346...\n",
      "✅ Batch #1346 escrito: 50000 filas\n",
      "📝 Procesando batch #1347...\n",
      "✅ Batch #1347 escrito: 50000 filas\n",
      "📝 Procesando batch #1348...\n",
      "✅ Batch #1348 escrito: 50000 filas\n",
      "📝 Procesando batch #1349...\n",
      "✅ Batch #1349 escrito: 50000 filas\n",
      "📝 Procesando batch #1350...\n",
      "✅ Batch #1350 escrito: 50000 filas\n",
      "📝 Procesando batch #1351...\n",
      "✅ Batch #1351 escrito: 50000 filas\n",
      "📝 Procesando batch #1352...\n",
      "✅ Batch #1352 escrito: 50000 filas\n",
      "📝 Procesando batch #1353...\n",
      "✅ Batch #1353 escrito: 50000 filas\n",
      "📝 Procesando batch #1354...\n",
      "✅ Batch #1354 escrito: 50000 filas\n",
      "📝 Procesando batch #1355...\n",
      "✅ Batch #1355 escrito: 50000 filas\n",
      "📝 Procesando batch #1356...\n",
      "✅ Batch #1356 escrito: 50000 filas\n",
      "📝 Procesando batch #1357...\n",
      "✅ Batch #1357 escrito: 50000 filas\n",
      "📝 Procesando batch #1358...\n",
      "✅ Batch #1358 escrito: 50000 filas\n",
      "📝 Procesando batch #1359...\n",
      "✅ Batch #1359 escrito: 50000 filas\n",
      "📝 Procesando batch #1360...\n",
      "✅ Batch #1360 escrito: 50000 filas\n",
      "📝 Procesando batch #1361...\n",
      "✅ Batch #1361 escrito: 48576 filas\n",
      "📝 Procesando batch #1362...\n",
      "✅ Batch #1362 escrito: 50000 filas\n",
      "📝 Procesando batch #1363...\n",
      "✅ Batch #1363 escrito: 50000 filas\n",
      "📝 Procesando batch #1364...\n",
      "✅ Batch #1364 escrito: 50000 filas\n",
      "📝 Procesando batch #1365...\n",
      "✅ Batch #1365 escrito: 50000 filas\n",
      "📝 Procesando batch #1366...\n",
      "✅ Batch #1366 escrito: 50000 filas\n",
      "📝 Procesando batch #1367...\n",
      "✅ Batch #1367 escrito: 50000 filas\n",
      "📝 Procesando batch #1368...\n",
      "✅ Batch #1368 escrito: 50000 filas\n",
      "📝 Procesando batch #1369...\n",
      "✅ Batch #1369 escrito: 50000 filas\n",
      "📝 Procesando batch #1370...\n",
      "✅ Batch #1370 escrito: 50000 filas\n",
      "📝 Procesando batch #1371...\n",
      "✅ Batch #1371 escrito: 50000 filas\n",
      "📝 Procesando batch #1372...\n",
      "✅ Batch #1372 escrito: 50000 filas\n",
      "📝 Procesando batch #1373...\n",
      "✅ Batch #1373 escrito: 50000 filas\n",
      "📝 Procesando batch #1374...\n",
      "✅ Batch #1374 escrito: 50000 filas\n",
      "📝 Procesando batch #1375...\n",
      "✅ Batch #1375 escrito: 50000 filas\n",
      "📝 Procesando batch #1376...\n",
      "✅ Batch #1376 escrito: 50000 filas\n",
      "📝 Procesando batch #1377...\n",
      "✅ Batch #1377 escrito: 50000 filas\n",
      "📝 Procesando batch #1378...\n",
      "✅ Batch #1378 escrito: 50000 filas\n",
      "📝 Procesando batch #1379...\n",
      "✅ Batch #1379 escrito: 6590 filas\n",
      "📝 Procesando batch #1380...\n",
      "✅ Batch #1380 escrito: 50000 filas\n",
      "📝 Procesando batch #1381...\n",
      "✅ Batch #1381 escrito: 50000 filas\n",
      "📝 Procesando batch #1382...\n",
      "✅ Batch #1382 escrito: 50000 filas\n",
      "📝 Procesando batch #1383...\n",
      "✅ Batch #1383 escrito: 50000 filas\n",
      "📝 Procesando batch #1384...\n",
      "✅ Batch #1384 escrito: 50000 filas\n",
      "📝 Procesando batch #1385...\n",
      "✅ Batch #1385 escrito: 50000 filas\n",
      "📝 Procesando batch #1386...\n",
      "✅ Batch #1386 escrito: 50000 filas\n",
      "📝 Procesando batch #1387...\n",
      "✅ Batch #1387 escrito: 50000 filas\n",
      "📝 Procesando batch #1388...\n",
      "✅ Batch #1388 escrito: 50000 filas\n",
      "📝 Procesando batch #1389...\n",
      "✅ Batch #1389 escrito: 50000 filas\n",
      "📝 Procesando batch #1390...\n",
      "✅ Batch #1390 escrito: 50000 filas\n",
      "📝 Procesando batch #1391...\n",
      "✅ Batch #1391 escrito: 50000 filas\n",
      "📝 Procesando batch #1392...\n",
      "✅ Batch #1392 escrito: 50000 filas\n",
      "📝 Procesando batch #1393...\n",
      "✅ Batch #1393 escrito: 50000 filas\n",
      "📝 Procesando batch #1394...\n",
      "✅ Batch #1394 escrito: 50000 filas\n",
      "📝 Procesando batch #1395...\n",
      "✅ Batch #1395 escrito: 50000 filas\n",
      "📝 Procesando batch #1396...\n",
      "✅ Batch #1396 escrito: 50000 filas\n",
      "📝 Procesando batch #1397...\n",
      "✅ Batch #1397 escrito: 50000 filas\n",
      "📝 Procesando batch #1398...\n",
      "✅ Batch #1398 escrito: 50000 filas\n",
      "📝 Procesando batch #1399...\n",
      "✅ Batch #1399 escrito: 50000 filas\n",
      "📝 Procesando batch #1400...\n",
      "✅ Batch #1400 escrito: 48576 filas\n",
      "📝 Procesando batch #1401...\n",
      "✅ Batch #1401 escrito: 50000 filas\n",
      "📝 Procesando batch #1402...\n",
      "✅ Batch #1402 escrito: 50000 filas\n",
      "📝 Procesando batch #1403...\n",
      "✅ Batch #1403 escrito: 50000 filas\n",
      "📝 Procesando batch #1404...\n",
      "✅ Batch #1404 escrito: 50000 filas\n",
      "📝 Procesando batch #1405...\n",
      "✅ Batch #1405 escrito: 50000 filas\n",
      "📝 Procesando batch #1406...\n",
      "✅ Batch #1406 escrito: 50000 filas\n",
      "📝 Procesando batch #1407...\n",
      "✅ Batch #1407 escrito: 50000 filas\n",
      "📝 Procesando batch #1408...\n",
      "✅ Batch #1408 escrito: 50000 filas\n",
      "📝 Procesando batch #1409...\n",
      "✅ Batch #1409 escrito: 50000 filas\n",
      "📝 Procesando batch #1410...\n",
      "✅ Batch #1410 escrito: 50000 filas\n",
      "📝 Procesando batch #1411...\n",
      "✅ Batch #1411 escrito: 50000 filas\n",
      "📝 Procesando batch #1412...\n",
      "✅ Batch #1412 escrito: 50000 filas\n",
      "📝 Procesando batch #1413...\n",
      "✅ Batch #1413 escrito: 50000 filas\n",
      "📝 Procesando batch #1414...\n",
      "✅ Batch #1414 escrito: 50000 filas\n",
      "📝 Procesando batch #1415...\n",
      "✅ Batch #1415 escrito: 50000 filas\n",
      "📝 Procesando batch #1416...\n",
      "✅ Batch #1416 escrito: 50000 filas\n",
      "📝 Procesando batch #1417...\n",
      "✅ Batch #1417 escrito: 50000 filas\n",
      "📝 Procesando batch #1418...\n",
      "✅ Batch #1418 escrito: 50000 filas\n",
      "📝 Procesando batch #1419...\n",
      "✅ Batch #1419 escrito: 50000 filas\n",
      "📝 Procesando batch #1420...\n",
      "✅ Batch #1420 escrito: 50000 filas\n",
      "📝 Procesando batch #1421...\n",
      "✅ Batch #1421 escrito: 48576 filas\n",
      "📝 Procesando batch #1422...\n",
      "✅ Batch #1422 escrito: 50000 filas\n",
      "📝 Procesando batch #1423...\n",
      "✅ Batch #1423 escrito: 50000 filas\n",
      "📝 Procesando batch #1424...\n",
      "✅ Batch #1424 escrito: 50000 filas\n",
      "📝 Procesando batch #1425...\n",
      "✅ Batch #1425 escrito: 50000 filas\n",
      "📝 Procesando batch #1426...\n",
      "✅ Batch #1426 escrito: 50000 filas\n",
      "📝 Procesando batch #1427...\n",
      "✅ Batch #1427 escrito: 50000 filas\n",
      "📝 Procesando batch #1428...\n",
      "✅ Batch #1428 escrito: 50000 filas\n",
      "📝 Procesando batch #1429...\n",
      "✅ Batch #1429 escrito: 50000 filas\n",
      "📝 Procesando batch #1430...\n",
      "✅ Batch #1430 escrito: 50000 filas\n",
      "📝 Procesando batch #1431...\n",
      "✅ Batch #1431 escrito: 50000 filas\n",
      "📝 Procesando batch #1432...\n",
      "✅ Batch #1432 escrito: 50000 filas\n",
      "📝 Procesando batch #1433...\n",
      "✅ Batch #1433 escrito: 50000 filas\n",
      "📝 Procesando batch #1434...\n",
      "✅ Batch #1434 escrito: 50000 filas\n",
      "📝 Procesando batch #1435...\n",
      "✅ Batch #1435 escrito: 50000 filas\n",
      "📝 Procesando batch #1436...\n",
      "✅ Batch #1436 escrito: 50000 filas\n",
      "📝 Procesando batch #1437...\n",
      "✅ Batch #1437 escrito: 50000 filas\n",
      "📝 Procesando batch #1438...\n",
      "✅ Batch #1438 escrito: 50000 filas\n",
      "📝 Procesando batch #1439...\n",
      "✅ Batch #1439 escrito: 50000 filas\n",
      "📝 Procesando batch #1440...\n",
      "✅ Batch #1440 escrito: 50000 filas\n",
      "📝 Procesando batch #1441...\n",
      "✅ Batch #1441 escrito: 50000 filas\n",
      "📝 Procesando batch #1442...\n",
      "✅ Batch #1442 escrito: 48576 filas\n",
      "📝 Procesando batch #1443...\n",
      "✅ Batch #1443 escrito: 50000 filas\n",
      "📝 Procesando batch #1444...\n",
      "✅ Batch #1444 escrito: 50000 filas\n",
      "📝 Procesando batch #1445...\n",
      "✅ Batch #1445 escrito: 50000 filas\n",
      "📝 Procesando batch #1446...\n",
      "✅ Batch #1446 escrito: 50000 filas\n",
      "📝 Procesando batch #1447...\n",
      "✅ Batch #1447 escrito: 50000 filas\n",
      "📝 Procesando batch #1448...\n",
      "✅ Batch #1448 escrito: 50000 filas\n",
      "📝 Procesando batch #1449...\n",
      "✅ Batch #1449 escrito: 50000 filas\n",
      "📝 Procesando batch #1450...\n",
      "✅ Batch #1450 escrito: 50000 filas\n",
      "📝 Procesando batch #1451...\n",
      "✅ Batch #1451 escrito: 50000 filas\n",
      "📝 Procesando batch #1452...\n",
      "✅ Batch #1452 escrito: 50000 filas\n",
      "📝 Procesando batch #1453...\n",
      "✅ Batch #1453 escrito: 50000 filas\n",
      "📝 Procesando batch #1454...\n",
      "✅ Batch #1454 escrito: 50000 filas\n",
      "📝 Procesando batch #1455...\n",
      "✅ Batch #1455 escrito: 50000 filas\n",
      "📝 Procesando batch #1456...\n",
      "✅ Batch #1456 escrito: 50000 filas\n",
      "📝 Procesando batch #1457...\n",
      "✅ Batch #1457 escrito: 50000 filas\n",
      "📝 Procesando batch #1458...\n",
      "✅ Batch #1458 escrito: 30850 filas\n",
      "✅ Guardado: dataset_eb_kepler_labeled_fixed.parquet\n"
     ]
    }
   ],
   "source": [
    "%run src/script_5a_normalize_ids.py --mision kepler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b435f89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Procesando TESS → dataset_eb_tess_labeled.parquet\n",
      "📝 Procesando batch #1...\n",
      "✅ Batch #1 guardado en dataset_eb_tess_labeled_batch_0000.parquet (50000 filas)\n",
      "📝 Procesando batch #2...\n",
      "✅ Batch #2 guardado en dataset_eb_tess_labeled_batch_0001.parquet (50000 filas)\n",
      "📝 Procesando batch #3...\n",
      "✅ Batch #3 guardado en dataset_eb_tess_labeled_batch_0002.parquet (50000 filas)\n",
      "📝 Procesando batch #4...\n",
      "✅ Batch #4 guardado en dataset_eb_tess_labeled_batch_0003.parquet (50000 filas)\n",
      "📝 Procesando batch #5...\n",
      "✅ Batch #5 guardado en dataset_eb_tess_labeled_batch_0004.parquet (50000 filas)\n",
      "📝 Procesando batch #6...\n",
      "✅ Batch #6 guardado en dataset_eb_tess_labeled_batch_0005.parquet (50000 filas)\n",
      "📝 Procesando batch #7...\n",
      "✅ Batch #7 guardado en dataset_eb_tess_labeled_batch_0006.parquet (50000 filas)\n",
      "📝 Procesando batch #8...\n",
      "✅ Batch #8 guardado en dataset_eb_tess_labeled_batch_0007.parquet (50000 filas)\n",
      "📝 Procesando batch #9...\n",
      "✅ Batch #9 guardado en dataset_eb_tess_labeled_batch_0008.parquet (50000 filas)\n",
      "📝 Procesando batch #10...\n",
      "✅ Batch #10 guardado en dataset_eb_tess_labeled_batch_0009.parquet (50000 filas)\n",
      "📝 Procesando batch #11...\n",
      "✅ Batch #11 guardado en dataset_eb_tess_labeled_batch_0010.parquet (50000 filas)\n",
      "📝 Procesando batch #12...\n",
      "✅ Batch #12 guardado en dataset_eb_tess_labeled_batch_0011.parquet (50000 filas)\n",
      "📝 Procesando batch #13...\n",
      "✅ Batch #13 guardado en dataset_eb_tess_labeled_batch_0012.parquet (50000 filas)\n",
      "📝 Procesando batch #14...\n",
      "✅ Batch #14 guardado en dataset_eb_tess_labeled_batch_0013.parquet (50000 filas)\n",
      "📝 Procesando batch #15...\n",
      "✅ Batch #15 guardado en dataset_eb_tess_labeled_batch_0014.parquet (50000 filas)\n",
      "📝 Procesando batch #16...\n",
      "✅ Batch #16 guardado en dataset_eb_tess_labeled_batch_0015.parquet (50000 filas)\n",
      "📝 Procesando batch #17...\n",
      "✅ Batch #17 guardado en dataset_eb_tess_labeled_batch_0016.parquet (50000 filas)\n",
      "📝 Procesando batch #18...\n",
      "✅ Batch #18 guardado en dataset_eb_tess_labeled_batch_0017.parquet (50000 filas)\n",
      "📝 Procesando batch #19...\n",
      "✅ Batch #19 guardado en dataset_eb_tess_labeled_batch_0018.parquet (50000 filas)\n",
      "📝 Procesando batch #20...\n",
      "✅ Batch #20 guardado en dataset_eb_tess_labeled_batch_0019.parquet (50000 filas)\n",
      "📝 Procesando batch #21...\n",
      "✅ Batch #21 guardado en dataset_eb_tess_labeled_batch_0020.parquet (48576 filas)\n",
      "📝 Procesando batch #22...\n",
      "✅ Batch #22 guardado en dataset_eb_tess_labeled_batch_0021.parquet (50000 filas)\n",
      "📝 Procesando batch #23...\n",
      "✅ Batch #23 guardado en dataset_eb_tess_labeled_batch_0022.parquet (50000 filas)\n",
      "📝 Procesando batch #24...\n",
      "✅ Batch #24 guardado en dataset_eb_tess_labeled_batch_0023.parquet (50000 filas)\n",
      "📝 Procesando batch #25...\n",
      "✅ Batch #25 guardado en dataset_eb_tess_labeled_batch_0024.parquet (50000 filas)\n",
      "📝 Procesando batch #26...\n",
      "✅ Batch #26 guardado en dataset_eb_tess_labeled_batch_0025.parquet (50000 filas)\n",
      "📝 Procesando batch #27...\n",
      "✅ Batch #27 guardado en dataset_eb_tess_labeled_batch_0026.parquet (50000 filas)\n",
      "📝 Procesando batch #28...\n",
      "✅ Batch #28 guardado en dataset_eb_tess_labeled_batch_0027.parquet (50000 filas)\n",
      "📝 Procesando batch #29...\n",
      "✅ Batch #29 guardado en dataset_eb_tess_labeled_batch_0028.parquet (50000 filas)\n",
      "📝 Procesando batch #30...\n",
      "✅ Batch #30 guardado en dataset_eb_tess_labeled_batch_0029.parquet (50000 filas)\n",
      "📝 Procesando batch #31...\n",
      "✅ Batch #31 guardado en dataset_eb_tess_labeled_batch_0030.parquet (50000 filas)\n",
      "📝 Procesando batch #32...\n",
      "✅ Batch #32 guardado en dataset_eb_tess_labeled_batch_0031.parquet (50000 filas)\n",
      "📝 Procesando batch #33...\n",
      "✅ Batch #33 guardado en dataset_eb_tess_labeled_batch_0032.parquet (50000 filas)\n",
      "📝 Procesando batch #34...\n",
      "✅ Batch #34 guardado en dataset_eb_tess_labeled_batch_0033.parquet (50000 filas)\n",
      "📝 Procesando batch #35...\n",
      "✅ Batch #35 guardado en dataset_eb_tess_labeled_batch_0034.parquet (50000 filas)\n",
      "📝 Procesando batch #36...\n",
      "✅ Batch #36 guardado en dataset_eb_tess_labeled_batch_0035.parquet (50000 filas)\n",
      "📝 Procesando batch #37...\n",
      "✅ Batch #37 guardado en dataset_eb_tess_labeled_batch_0036.parquet (50000 filas)\n",
      "📝 Procesando batch #38...\n",
      "✅ Batch #38 guardado en dataset_eb_tess_labeled_batch_0037.parquet (50000 filas)\n",
      "📝 Procesando batch #39...\n",
      "✅ Batch #39 guardado en dataset_eb_tess_labeled_batch_0038.parquet (50000 filas)\n",
      "📝 Procesando batch #40...\n",
      "✅ Batch #40 guardado en dataset_eb_tess_labeled_batch_0039.parquet (50000 filas)\n",
      "📝 Procesando batch #41...\n",
      "✅ Batch #41 guardado en dataset_eb_tess_labeled_batch_0040.parquet (50000 filas)\n",
      "📝 Procesando batch #42...\n",
      "✅ Batch #42 guardado en dataset_eb_tess_labeled_batch_0041.parquet (48576 filas)\n",
      "📝 Procesando batch #43...\n",
      "✅ Batch #43 guardado en dataset_eb_tess_labeled_batch_0042.parquet (50000 filas)\n",
      "📝 Procesando batch #44...\n",
      "✅ Batch #44 guardado en dataset_eb_tess_labeled_batch_0043.parquet (50000 filas)\n",
      "📝 Procesando batch #45...\n",
      "✅ Batch #45 guardado en dataset_eb_tess_labeled_batch_0044.parquet (50000 filas)\n",
      "📝 Procesando batch #46...\n",
      "✅ Batch #46 guardado en dataset_eb_tess_labeled_batch_0045.parquet (50000 filas)\n",
      "📝 Procesando batch #47...\n",
      "✅ Batch #47 guardado en dataset_eb_tess_labeled_batch_0046.parquet (50000 filas)\n",
      "📝 Procesando batch #48...\n",
      "✅ Batch #48 guardado en dataset_eb_tess_labeled_batch_0047.parquet (50000 filas)\n",
      "📝 Procesando batch #49...\n",
      "✅ Batch #49 guardado en dataset_eb_tess_labeled_batch_0048.parquet (50000 filas)\n",
      "📝 Procesando batch #50...\n",
      "✅ Batch #50 guardado en dataset_eb_tess_labeled_batch_0049.parquet (50000 filas)\n",
      "📝 Procesando batch #51...\n",
      "✅ Batch #51 guardado en dataset_eb_tess_labeled_batch_0050.parquet (50000 filas)\n",
      "📝 Procesando batch #52...\n",
      "✅ Batch #52 guardado en dataset_eb_tess_labeled_batch_0051.parquet (50000 filas)\n",
      "📝 Procesando batch #53...\n",
      "✅ Batch #53 guardado en dataset_eb_tess_labeled_batch_0052.parquet (50000 filas)\n",
      "📝 Procesando batch #54...\n",
      "✅ Batch #54 guardado en dataset_eb_tess_labeled_batch_0053.parquet (50000 filas)\n",
      "📝 Procesando batch #55...\n",
      "✅ Batch #55 guardado en dataset_eb_tess_labeled_batch_0054.parquet (50000 filas)\n",
      "📝 Procesando batch #56...\n",
      "✅ Batch #56 guardado en dataset_eb_tess_labeled_batch_0055.parquet (50000 filas)\n",
      "📝 Procesando batch #57...\n",
      "✅ Batch #57 guardado en dataset_eb_tess_labeled_batch_0056.parquet (50000 filas)\n",
      "📝 Procesando batch #58...\n",
      "✅ Batch #58 guardado en dataset_eb_tess_labeled_batch_0057.parquet (50000 filas)\n",
      "📝 Procesando batch #59...\n",
      "✅ Batch #59 guardado en dataset_eb_tess_labeled_batch_0058.parquet (50000 filas)\n",
      "📝 Procesando batch #60...\n",
      "✅ Batch #60 guardado en dataset_eb_tess_labeled_batch_0059.parquet (50000 filas)\n",
      "📝 Procesando batch #61...\n",
      "✅ Batch #61 guardado en dataset_eb_tess_labeled_batch_0060.parquet (50000 filas)\n",
      "📝 Procesando batch #62...\n",
      "✅ Batch #62 guardado en dataset_eb_tess_labeled_batch_0061.parquet (50000 filas)\n",
      "📝 Procesando batch #63...\n",
      "✅ Batch #63 guardado en dataset_eb_tess_labeled_batch_0062.parquet (48576 filas)\n",
      "📝 Procesando batch #64...\n",
      "✅ Batch #64 guardado en dataset_eb_tess_labeled_batch_0063.parquet (50000 filas)\n",
      "📝 Procesando batch #65...\n",
      "✅ Batch #65 guardado en dataset_eb_tess_labeled_batch_0064.parquet (50000 filas)\n",
      "📝 Procesando batch #66...\n",
      "✅ Batch #66 guardado en dataset_eb_tess_labeled_batch_0065.parquet (50000 filas)\n",
      "📝 Procesando batch #67...\n",
      "✅ Batch #67 guardado en dataset_eb_tess_labeled_batch_0066.parquet (50000 filas)\n",
      "📝 Procesando batch #68...\n",
      "✅ Batch #68 guardado en dataset_eb_tess_labeled_batch_0067.parquet (50000 filas)\n",
      "📝 Procesando batch #69...\n",
      "✅ Batch #69 guardado en dataset_eb_tess_labeled_batch_0068.parquet (50000 filas)\n",
      "📝 Procesando batch #70...\n",
      "✅ Batch #70 guardado en dataset_eb_tess_labeled_batch_0069.parquet (50000 filas)\n",
      "📝 Procesando batch #71...\n",
      "✅ Batch #71 guardado en dataset_eb_tess_labeled_batch_0070.parquet (50000 filas)\n",
      "📝 Procesando batch #72...\n",
      "✅ Batch #72 guardado en dataset_eb_tess_labeled_batch_0071.parquet (50000 filas)\n",
      "📝 Procesando batch #73...\n",
      "✅ Batch #73 guardado en dataset_eb_tess_labeled_batch_0072.parquet (50000 filas)\n",
      "📝 Procesando batch #74...\n",
      "✅ Batch #74 guardado en dataset_eb_tess_labeled_batch_0073.parquet (50000 filas)\n",
      "📝 Procesando batch #75...\n",
      "✅ Batch #75 guardado en dataset_eb_tess_labeled_batch_0074.parquet (50000 filas)\n",
      "📝 Procesando batch #76...\n",
      "✅ Batch #76 guardado en dataset_eb_tess_labeled_batch_0075.parquet (50000 filas)\n",
      "📝 Procesando batch #77...\n",
      "✅ Batch #77 guardado en dataset_eb_tess_labeled_batch_0076.parquet (50000 filas)\n",
      "📝 Procesando batch #78...\n",
      "✅ Batch #78 guardado en dataset_eb_tess_labeled_batch_0077.parquet (50000 filas)\n",
      "📝 Procesando batch #79...\n",
      "✅ Batch #79 guardado en dataset_eb_tess_labeled_batch_0078.parquet (50000 filas)\n",
      "📝 Procesando batch #80...\n",
      "✅ Batch #80 guardado en dataset_eb_tess_labeled_batch_0079.parquet (50000 filas)\n",
      "📝 Procesando batch #81...\n",
      "✅ Batch #81 guardado en dataset_eb_tess_labeled_batch_0080.parquet (50000 filas)\n",
      "📝 Procesando batch #82...\n",
      "✅ Batch #82 guardado en dataset_eb_tess_labeled_batch_0081.parquet (50000 filas)\n",
      "📝 Procesando batch #83...\n",
      "✅ Batch #83 guardado en dataset_eb_tess_labeled_batch_0082.parquet (50000 filas)\n",
      "📝 Procesando batch #84...\n",
      "✅ Batch #84 guardado en dataset_eb_tess_labeled_batch_0083.parquet (48576 filas)\n",
      "📝 Procesando batch #85...\n",
      "✅ Batch #85 guardado en dataset_eb_tess_labeled_batch_0084.parquet (50000 filas)\n",
      "📝 Procesando batch #86...\n",
      "✅ Batch #86 guardado en dataset_eb_tess_labeled_batch_0085.parquet (50000 filas)\n",
      "📝 Procesando batch #87...\n",
      "✅ Batch #87 guardado en dataset_eb_tess_labeled_batch_0086.parquet (50000 filas)\n",
      "📝 Procesando batch #88...\n",
      "✅ Batch #88 guardado en dataset_eb_tess_labeled_batch_0087.parquet (50000 filas)\n",
      "📝 Procesando batch #89...\n",
      "✅ Batch #89 guardado en dataset_eb_tess_labeled_batch_0088.parquet (50000 filas)\n",
      "📝 Procesando batch #90...\n",
      "✅ Batch #90 guardado en dataset_eb_tess_labeled_batch_0089.parquet (50000 filas)\n",
      "📝 Procesando batch #91...\n",
      "✅ Batch #91 guardado en dataset_eb_tess_labeled_batch_0090.parquet (50000 filas)\n",
      "📝 Procesando batch #92...\n",
      "✅ Batch #92 guardado en dataset_eb_tess_labeled_batch_0091.parquet (50000 filas)\n",
      "📝 Procesando batch #93...\n",
      "✅ Batch #93 guardado en dataset_eb_tess_labeled_batch_0092.parquet (50000 filas)\n",
      "📝 Procesando batch #94...\n",
      "✅ Batch #94 guardado en dataset_eb_tess_labeled_batch_0093.parquet (50000 filas)\n",
      "📝 Procesando batch #95...\n",
      "✅ Batch #95 guardado en dataset_eb_tess_labeled_batch_0094.parquet (50000 filas)\n",
      "📝 Procesando batch #96...\n",
      "✅ Batch #96 guardado en dataset_eb_tess_labeled_batch_0095.parquet (50000 filas)\n",
      "📝 Procesando batch #97...\n",
      "✅ Batch #97 guardado en dataset_eb_tess_labeled_batch_0096.parquet (50000 filas)\n",
      "📝 Procesando batch #98...\n",
      "✅ Batch #98 guardado en dataset_eb_tess_labeled_batch_0097.parquet (50000 filas)\n",
      "📝 Procesando batch #99...\n",
      "✅ Batch #99 guardado en dataset_eb_tess_labeled_batch_0098.parquet (50000 filas)\n",
      "📝 Procesando batch #100...\n",
      "✅ Batch #100 guardado en dataset_eb_tess_labeled_batch_0099.parquet (50000 filas)\n",
      "📝 Procesando batch #101...\n",
      "✅ Batch #101 guardado en dataset_eb_tess_labeled_batch_0100.parquet (50000 filas)\n",
      "📝 Procesando batch #102...\n",
      "✅ Batch #102 guardado en dataset_eb_tess_labeled_batch_0101.parquet (50000 filas)\n",
      "📝 Procesando batch #103...\n",
      "✅ Batch #103 guardado en dataset_eb_tess_labeled_batch_0102.parquet (50000 filas)\n",
      "📝 Procesando batch #104...\n",
      "✅ Batch #104 guardado en dataset_eb_tess_labeled_batch_0103.parquet (50000 filas)\n",
      "📝 Procesando batch #105...\n",
      "✅ Batch #105 guardado en dataset_eb_tess_labeled_batch_0104.parquet (48576 filas)\n",
      "📝 Procesando batch #106...\n",
      "✅ Batch #106 guardado en dataset_eb_tess_labeled_batch_0105.parquet (50000 filas)\n",
      "📝 Procesando batch #107...\n",
      "✅ Batch #107 guardado en dataset_eb_tess_labeled_batch_0106.parquet (50000 filas)\n",
      "📝 Procesando batch #108...\n",
      "✅ Batch #108 guardado en dataset_eb_tess_labeled_batch_0107.parquet (50000 filas)\n",
      "📝 Procesando batch #109...\n",
      "✅ Batch #109 guardado en dataset_eb_tess_labeled_batch_0108.parquet (50000 filas)\n",
      "📝 Procesando batch #110...\n",
      "✅ Batch #110 guardado en dataset_eb_tess_labeled_batch_0109.parquet (50000 filas)\n",
      "📝 Procesando batch #111...\n",
      "✅ Batch #111 guardado en dataset_eb_tess_labeled_batch_0110.parquet (50000 filas)\n",
      "📝 Procesando batch #112...\n",
      "✅ Batch #112 guardado en dataset_eb_tess_labeled_batch_0111.parquet (50000 filas)\n",
      "📝 Procesando batch #113...\n",
      "✅ Batch #113 guardado en dataset_eb_tess_labeled_batch_0112.parquet (50000 filas)\n",
      "📝 Procesando batch #114...\n",
      "✅ Batch #114 guardado en dataset_eb_tess_labeled_batch_0113.parquet (50000 filas)\n",
      "📝 Procesando batch #115...\n",
      "✅ Batch #115 guardado en dataset_eb_tess_labeled_batch_0114.parquet (50000 filas)\n",
      "📝 Procesando batch #116...\n",
      "✅ Batch #116 guardado en dataset_eb_tess_labeled_batch_0115.parquet (50000 filas)\n",
      "📝 Procesando batch #117...\n",
      "✅ Batch #117 guardado en dataset_eb_tess_labeled_batch_0116.parquet (50000 filas)\n",
      "📝 Procesando batch #118...\n",
      "✅ Batch #118 guardado en dataset_eb_tess_labeled_batch_0117.parquet (50000 filas)\n",
      "📝 Procesando batch #119...\n",
      "✅ Batch #119 guardado en dataset_eb_tess_labeled_batch_0118.parquet (50000 filas)\n",
      "📝 Procesando batch #120...\n",
      "✅ Batch #120 guardado en dataset_eb_tess_labeled_batch_0119.parquet (50000 filas)\n",
      "📝 Procesando batch #121...\n",
      "✅ Batch #121 guardado en dataset_eb_tess_labeled_batch_0120.parquet (50000 filas)\n",
      "📝 Procesando batch #122...\n",
      "✅ Batch #122 guardado en dataset_eb_tess_labeled_batch_0121.parquet (50000 filas)\n",
      "📝 Procesando batch #123...\n",
      "✅ Batch #123 guardado en dataset_eb_tess_labeled_batch_0122.parquet (50000 filas)\n",
      "📝 Procesando batch #124...\n",
      "✅ Batch #124 guardado en dataset_eb_tess_labeled_batch_0123.parquet (50000 filas)\n",
      "📝 Procesando batch #125...\n",
      "✅ Batch #125 guardado en dataset_eb_tess_labeled_batch_0124.parquet (50000 filas)\n",
      "📝 Procesando batch #126...\n",
      "✅ Batch #126 guardado en dataset_eb_tess_labeled_batch_0125.parquet (48576 filas)\n",
      "📝 Procesando batch #127...\n",
      "✅ Batch #127 guardado en dataset_eb_tess_labeled_batch_0126.parquet (50000 filas)\n",
      "📝 Procesando batch #128...\n",
      "✅ Batch #128 guardado en dataset_eb_tess_labeled_batch_0127.parquet (50000 filas)\n",
      "📝 Procesando batch #129...\n",
      "✅ Batch #129 guardado en dataset_eb_tess_labeled_batch_0128.parquet (50000 filas)\n",
      "📝 Procesando batch #130...\n",
      "✅ Batch #130 guardado en dataset_eb_tess_labeled_batch_0129.parquet (50000 filas)\n",
      "📝 Procesando batch #131...\n",
      "✅ Batch #131 guardado en dataset_eb_tess_labeled_batch_0130.parquet (50000 filas)\n",
      "📝 Procesando batch #132...\n",
      "✅ Batch #132 guardado en dataset_eb_tess_labeled_batch_0131.parquet (50000 filas)\n",
      "📝 Procesando batch #133...\n",
      "✅ Batch #133 guardado en dataset_eb_tess_labeled_batch_0132.parquet (50000 filas)\n",
      "📝 Procesando batch #134...\n",
      "✅ Batch #134 guardado en dataset_eb_tess_labeled_batch_0133.parquet (50000 filas)\n",
      "📝 Procesando batch #135...\n",
      "✅ Batch #135 guardado en dataset_eb_tess_labeled_batch_0134.parquet (50000 filas)\n",
      "📝 Procesando batch #136...\n",
      "✅ Batch #136 guardado en dataset_eb_tess_labeled_batch_0135.parquet (50000 filas)\n",
      "📝 Procesando batch #137...\n",
      "✅ Batch #137 guardado en dataset_eb_tess_labeled_batch_0136.parquet (50000 filas)\n",
      "📝 Procesando batch #138...\n",
      "✅ Batch #138 guardado en dataset_eb_tess_labeled_batch_0137.parquet (50000 filas)\n",
      "📝 Procesando batch #139...\n",
      "✅ Batch #139 guardado en dataset_eb_tess_labeled_batch_0138.parquet (50000 filas)\n",
      "📝 Procesando batch #140...\n",
      "✅ Batch #140 guardado en dataset_eb_tess_labeled_batch_0139.parquet (50000 filas)\n",
      "📝 Procesando batch #141...\n",
      "✅ Batch #141 guardado en dataset_eb_tess_labeled_batch_0140.parquet (50000 filas)\n",
      "📝 Procesando batch #142...\n",
      "✅ Batch #142 guardado en dataset_eb_tess_labeled_batch_0141.parquet (50000 filas)\n",
      "📝 Procesando batch #143...\n",
      "✅ Batch #143 guardado en dataset_eb_tess_labeled_batch_0142.parquet (50000 filas)\n",
      "📝 Procesando batch #144...\n",
      "✅ Batch #144 guardado en dataset_eb_tess_labeled_batch_0143.parquet (50000 filas)\n",
      "📝 Procesando batch #145...\n",
      "✅ Batch #145 guardado en dataset_eb_tess_labeled_batch_0144.parquet (50000 filas)\n",
      "📝 Procesando batch #146...\n",
      "✅ Batch #146 guardado en dataset_eb_tess_labeled_batch_0145.parquet (50000 filas)\n",
      "📝 Procesando batch #147...\n",
      "✅ Batch #147 guardado en dataset_eb_tess_labeled_batch_0146.parquet (48576 filas)\n",
      "📝 Procesando batch #148...\n",
      "✅ Batch #148 guardado en dataset_eb_tess_labeled_batch_0147.parquet (50000 filas)\n",
      "📝 Procesando batch #149...\n",
      "✅ Batch #149 guardado en dataset_eb_tess_labeled_batch_0148.parquet (50000 filas)\n",
      "📝 Procesando batch #150...\n",
      "✅ Batch #150 guardado en dataset_eb_tess_labeled_batch_0149.parquet (50000 filas)\n",
      "📝 Procesando batch #151...\n",
      "✅ Batch #151 guardado en dataset_eb_tess_labeled_batch_0150.parquet (50000 filas)\n",
      "📝 Procesando batch #152...\n",
      "✅ Batch #152 guardado en dataset_eb_tess_labeled_batch_0151.parquet (50000 filas)\n",
      "📝 Procesando batch #153...\n",
      "✅ Batch #153 guardado en dataset_eb_tess_labeled_batch_0152.parquet (50000 filas)\n",
      "📝 Procesando batch #154...\n",
      "✅ Batch #154 guardado en dataset_eb_tess_labeled_batch_0153.parquet (50000 filas)\n",
      "📝 Procesando batch #155...\n",
      "✅ Batch #155 guardado en dataset_eb_tess_labeled_batch_0154.parquet (50000 filas)\n",
      "📝 Procesando batch #156...\n",
      "✅ Batch #156 guardado en dataset_eb_tess_labeled_batch_0155.parquet (50000 filas)\n",
      "📝 Procesando batch #157...\n",
      "✅ Batch #157 guardado en dataset_eb_tess_labeled_batch_0156.parquet (50000 filas)\n",
      "📝 Procesando batch #158...\n",
      "✅ Batch #158 guardado en dataset_eb_tess_labeled_batch_0157.parquet (50000 filas)\n",
      "📝 Procesando batch #159...\n",
      "✅ Batch #159 guardado en dataset_eb_tess_labeled_batch_0158.parquet (30250 filas)\n",
      "📝 Procesando batch #160...\n",
      "✅ Batch #160 guardado en dataset_eb_tess_labeled_batch_0159.parquet (50000 filas)\n",
      "📝 Procesando batch #161...\n",
      "✅ Batch #161 guardado en dataset_eb_tess_labeled_batch_0160.parquet (50000 filas)\n",
      "📝 Procesando batch #162...\n",
      "✅ Batch #162 guardado en dataset_eb_tess_labeled_batch_0161.parquet (50000 filas)\n",
      "📝 Procesando batch #163...\n",
      "✅ Batch #163 guardado en dataset_eb_tess_labeled_batch_0162.parquet (50000 filas)\n",
      "📝 Procesando batch #164...\n",
      "✅ Batch #164 guardado en dataset_eb_tess_labeled_batch_0163.parquet (50000 filas)\n",
      "📝 Procesando batch #165...\n",
      "✅ Batch #165 guardado en dataset_eb_tess_labeled_batch_0164.parquet (50000 filas)\n",
      "📝 Procesando batch #166...\n",
      "✅ Batch #166 guardado en dataset_eb_tess_labeled_batch_0165.parquet (50000 filas)\n",
      "📝 Procesando batch #167...\n",
      "✅ Batch #167 guardado en dataset_eb_tess_labeled_batch_0166.parquet (50000 filas)\n",
      "📝 Procesando batch #168...\n",
      "✅ Batch #168 guardado en dataset_eb_tess_labeled_batch_0167.parquet (50000 filas)\n",
      "📝 Procesando batch #169...\n",
      "✅ Batch #169 guardado en dataset_eb_tess_labeled_batch_0168.parquet (50000 filas)\n",
      "📝 Procesando batch #170...\n",
      "✅ Batch #170 guardado en dataset_eb_tess_labeled_batch_0169.parquet (50000 filas)\n",
      "📝 Procesando batch #171...\n",
      "✅ Batch #171 guardado en dataset_eb_tess_labeled_batch_0170.parquet (50000 filas)\n",
      "📝 Procesando batch #172...\n",
      "✅ Batch #172 guardado en dataset_eb_tess_labeled_batch_0171.parquet (50000 filas)\n",
      "📝 Procesando batch #173...\n",
      "✅ Batch #173 guardado en dataset_eb_tess_labeled_batch_0172.parquet (50000 filas)\n",
      "📝 Procesando batch #174...\n",
      "✅ Batch #174 guardado en dataset_eb_tess_labeled_batch_0173.parquet (50000 filas)\n",
      "📝 Procesando batch #175...\n",
      "✅ Batch #175 guardado en dataset_eb_tess_labeled_batch_0174.parquet (50000 filas)\n",
      "📝 Procesando batch #176...\n",
      "✅ Batch #176 guardado en dataset_eb_tess_labeled_batch_0175.parquet (50000 filas)\n",
      "📝 Procesando batch #177...\n",
      "✅ Batch #177 guardado en dataset_eb_tess_labeled_batch_0176.parquet (50000 filas)\n",
      "📝 Procesando batch #178...\n",
      "✅ Batch #178 guardado en dataset_eb_tess_labeled_batch_0177.parquet (50000 filas)\n",
      "📝 Procesando batch #179...\n",
      "✅ Batch #179 guardado en dataset_eb_tess_labeled_batch_0178.parquet (50000 filas)\n",
      "📝 Procesando batch #180...\n",
      "✅ Batch #180 guardado en dataset_eb_tess_labeled_batch_0179.parquet (48576 filas)\n",
      "📝 Procesando batch #181...\n",
      "✅ Batch #181 guardado en dataset_eb_tess_labeled_batch_0180.parquet (50000 filas)\n",
      "📝 Procesando batch #182...\n",
      "✅ Batch #182 guardado en dataset_eb_tess_labeled_batch_0181.parquet (50000 filas)\n",
      "📝 Procesando batch #183...\n",
      "✅ Batch #183 guardado en dataset_eb_tess_labeled_batch_0182.parquet (50000 filas)\n",
      "📝 Procesando batch #184...\n",
      "✅ Batch #184 guardado en dataset_eb_tess_labeled_batch_0183.parquet (50000 filas)\n",
      "📝 Procesando batch #185...\n",
      "✅ Batch #185 guardado en dataset_eb_tess_labeled_batch_0184.parquet (50000 filas)\n",
      "📝 Procesando batch #186...\n",
      "✅ Batch #186 guardado en dataset_eb_tess_labeled_batch_0185.parquet (50000 filas)\n",
      "📝 Procesando batch #187...\n",
      "✅ Batch #187 guardado en dataset_eb_tess_labeled_batch_0186.parquet (50000 filas)\n",
      "📝 Procesando batch #188...\n",
      "✅ Batch #188 guardado en dataset_eb_tess_labeled_batch_0187.parquet (50000 filas)\n",
      "📝 Procesando batch #189...\n",
      "✅ Batch #189 guardado en dataset_eb_tess_labeled_batch_0188.parquet (50000 filas)\n",
      "📝 Procesando batch #190...\n",
      "✅ Batch #190 guardado en dataset_eb_tess_labeled_batch_0189.parquet (50000 filas)\n",
      "📝 Procesando batch #191...\n",
      "✅ Batch #191 guardado en dataset_eb_tess_labeled_batch_0190.parquet (50000 filas)\n",
      "📝 Procesando batch #192...\n",
      "✅ Batch #192 guardado en dataset_eb_tess_labeled_batch_0191.parquet (50000 filas)\n",
      "📝 Procesando batch #193...\n",
      "✅ Batch #193 guardado en dataset_eb_tess_labeled_batch_0192.parquet (50000 filas)\n",
      "📝 Procesando batch #194...\n",
      "✅ Batch #194 guardado en dataset_eb_tess_labeled_batch_0193.parquet (50000 filas)\n",
      "📝 Procesando batch #195...\n",
      "✅ Batch #195 guardado en dataset_eb_tess_labeled_batch_0194.parquet (50000 filas)\n",
      "📝 Procesando batch #196...\n",
      "✅ Batch #196 guardado en dataset_eb_tess_labeled_batch_0195.parquet (50000 filas)\n",
      "📝 Procesando batch #197...\n",
      "✅ Batch #197 guardado en dataset_eb_tess_labeled_batch_0196.parquet (50000 filas)\n",
      "📝 Procesando batch #198...\n",
      "✅ Batch #198 guardado en dataset_eb_tess_labeled_batch_0197.parquet (50000 filas)\n",
      "📝 Procesando batch #199...\n",
      "✅ Batch #199 guardado en dataset_eb_tess_labeled_batch_0198.parquet (50000 filas)\n",
      "📝 Procesando batch #200...\n",
      "✅ Batch #200 guardado en dataset_eb_tess_labeled_batch_0199.parquet (50000 filas)\n",
      "📝 Procesando batch #201...\n",
      "✅ Batch #201 guardado en dataset_eb_tess_labeled_batch_0200.parquet (48576 filas)\n",
      "📝 Procesando batch #202...\n",
      "✅ Batch #202 guardado en dataset_eb_tess_labeled_batch_0201.parquet (50000 filas)\n",
      "📝 Procesando batch #203...\n",
      "✅ Batch #203 guardado en dataset_eb_tess_labeled_batch_0202.parquet (50000 filas)\n",
      "📝 Procesando batch #204...\n",
      "✅ Batch #204 guardado en dataset_eb_tess_labeled_batch_0203.parquet (50000 filas)\n",
      "📝 Procesando batch #205...\n",
      "✅ Batch #205 guardado en dataset_eb_tess_labeled_batch_0204.parquet (50000 filas)\n",
      "📝 Procesando batch #206...\n",
      "✅ Batch #206 guardado en dataset_eb_tess_labeled_batch_0205.parquet (50000 filas)\n",
      "📝 Procesando batch #207...\n",
      "✅ Batch #207 guardado en dataset_eb_tess_labeled_batch_0206.parquet (50000 filas)\n",
      "📝 Procesando batch #208...\n",
      "✅ Batch #208 guardado en dataset_eb_tess_labeled_batch_0207.parquet (50000 filas)\n",
      "📝 Procesando batch #209...\n",
      "✅ Batch #209 guardado en dataset_eb_tess_labeled_batch_0208.parquet (50000 filas)\n",
      "📝 Procesando batch #210...\n",
      "✅ Batch #210 guardado en dataset_eb_tess_labeled_batch_0209.parquet (50000 filas)\n",
      "📝 Procesando batch #211...\n",
      "✅ Batch #211 guardado en dataset_eb_tess_labeled_batch_0210.parquet (50000 filas)\n",
      "📝 Procesando batch #212...\n",
      "✅ Batch #212 guardado en dataset_eb_tess_labeled_batch_0211.parquet (50000 filas)\n",
      "📝 Procesando batch #213...\n",
      "✅ Batch #213 guardado en dataset_eb_tess_labeled_batch_0212.parquet (50000 filas)\n",
      "📝 Procesando batch #214...\n",
      "✅ Batch #214 guardado en dataset_eb_tess_labeled_batch_0213.parquet (50000 filas)\n",
      "📝 Procesando batch #215...\n",
      "✅ Batch #215 guardado en dataset_eb_tess_labeled_batch_0214.parquet (50000 filas)\n",
      "📝 Procesando batch #216...\n",
      "✅ Batch #216 guardado en dataset_eb_tess_labeled_batch_0215.parquet (50000 filas)\n",
      "📝 Procesando batch #217...\n",
      "✅ Batch #217 guardado en dataset_eb_tess_labeled_batch_0216.parquet (50000 filas)\n",
      "📝 Procesando batch #218...\n",
      "✅ Batch #218 guardado en dataset_eb_tess_labeled_batch_0217.parquet (50000 filas)\n",
      "📝 Procesando batch #219...\n",
      "✅ Batch #219 guardado en dataset_eb_tess_labeled_batch_0218.parquet (50000 filas)\n",
      "📝 Procesando batch #220...\n",
      "✅ Batch #220 guardado en dataset_eb_tess_labeled_batch_0219.parquet (50000 filas)\n",
      "📝 Procesando batch #221...\n",
      "✅ Batch #221 guardado en dataset_eb_tess_labeled_batch_0220.parquet (50000 filas)\n",
      "📝 Procesando batch #222...\n",
      "✅ Batch #222 guardado en dataset_eb_tess_labeled_batch_0221.parquet (48576 filas)\n",
      "📝 Procesando batch #223...\n",
      "✅ Batch #223 guardado en dataset_eb_tess_labeled_batch_0222.parquet (50000 filas)\n",
      "📝 Procesando batch #224...\n",
      "✅ Batch #224 guardado en dataset_eb_tess_labeled_batch_0223.parquet (50000 filas)\n",
      "📝 Procesando batch #225...\n",
      "✅ Batch #225 guardado en dataset_eb_tess_labeled_batch_0224.parquet (50000 filas)\n",
      "📝 Procesando batch #226...\n",
      "✅ Batch #226 guardado en dataset_eb_tess_labeled_batch_0225.parquet (50000 filas)\n",
      "📝 Procesando batch #227...\n",
      "✅ Batch #227 guardado en dataset_eb_tess_labeled_batch_0226.parquet (50000 filas)\n",
      "📝 Procesando batch #228...\n",
      "✅ Batch #228 guardado en dataset_eb_tess_labeled_batch_0227.parquet (50000 filas)\n",
      "📝 Procesando batch #229...\n",
      "✅ Batch #229 guardado en dataset_eb_tess_labeled_batch_0228.parquet (50000 filas)\n",
      "📝 Procesando batch #230...\n",
      "✅ Batch #230 guardado en dataset_eb_tess_labeled_batch_0229.parquet (50000 filas)\n",
      "📝 Procesando batch #231...\n",
      "✅ Batch #231 guardado en dataset_eb_tess_labeled_batch_0230.parquet (50000 filas)\n",
      "📝 Procesando batch #232...\n",
      "✅ Batch #232 guardado en dataset_eb_tess_labeled_batch_0231.parquet (50000 filas)\n",
      "📝 Procesando batch #233...\n",
      "✅ Batch #233 guardado en dataset_eb_tess_labeled_batch_0232.parquet (50000 filas)\n",
      "📝 Procesando batch #234...\n",
      "✅ Batch #234 guardado en dataset_eb_tess_labeled_batch_0233.parquet (50000 filas)\n",
      "📝 Procesando batch #235...\n",
      "✅ Batch #235 guardado en dataset_eb_tess_labeled_batch_0234.parquet (50000 filas)\n",
      "📝 Procesando batch #236...\n",
      "✅ Batch #236 guardado en dataset_eb_tess_labeled_batch_0235.parquet (50000 filas)\n",
      "📝 Procesando batch #237...\n",
      "✅ Batch #237 guardado en dataset_eb_tess_labeled_batch_0236.parquet (50000 filas)\n",
      "📝 Procesando batch #238...\n",
      "✅ Batch #238 guardado en dataset_eb_tess_labeled_batch_0237.parquet (50000 filas)\n",
      "📝 Procesando batch #239...\n",
      "✅ Batch #239 guardado en dataset_eb_tess_labeled_batch_0238.parquet (50000 filas)\n",
      "📝 Procesando batch #240...\n",
      "✅ Batch #240 guardado en dataset_eb_tess_labeled_batch_0239.parquet (50000 filas)\n",
      "📝 Procesando batch #241...\n",
      "✅ Batch #241 guardado en dataset_eb_tess_labeled_batch_0240.parquet (50000 filas)\n",
      "📝 Procesando batch #242...\n",
      "✅ Batch #242 guardado en dataset_eb_tess_labeled_batch_0241.parquet (50000 filas)\n",
      "📝 Procesando batch #243...\n",
      "✅ Batch #243 guardado en dataset_eb_tess_labeled_batch_0242.parquet (48576 filas)\n",
      "📝 Procesando batch #244...\n",
      "✅ Batch #244 guardado en dataset_eb_tess_labeled_batch_0243.parquet (50000 filas)\n",
      "📝 Procesando batch #245...\n",
      "✅ Batch #245 guardado en dataset_eb_tess_labeled_batch_0244.parquet (50000 filas)\n",
      "📝 Procesando batch #246...\n",
      "✅ Batch #246 guardado en dataset_eb_tess_labeled_batch_0245.parquet (50000 filas)\n",
      "📝 Procesando batch #247...\n",
      "✅ Batch #247 guardado en dataset_eb_tess_labeled_batch_0246.parquet (50000 filas)\n",
      "📝 Procesando batch #248...\n",
      "✅ Batch #248 guardado en dataset_eb_tess_labeled_batch_0247.parquet (50000 filas)\n",
      "📝 Procesando batch #249...\n",
      "✅ Batch #249 guardado en dataset_eb_tess_labeled_batch_0248.parquet (50000 filas)\n",
      "📝 Procesando batch #250...\n",
      "✅ Batch #250 guardado en dataset_eb_tess_labeled_batch_0249.parquet (50000 filas)\n",
      "📝 Procesando batch #251...\n",
      "✅ Batch #251 guardado en dataset_eb_tess_labeled_batch_0250.parquet (50000 filas)\n",
      "📝 Procesando batch #252...\n",
      "✅ Batch #252 guardado en dataset_eb_tess_labeled_batch_0251.parquet (50000 filas)\n",
      "📝 Procesando batch #253...\n",
      "✅ Batch #253 guardado en dataset_eb_tess_labeled_batch_0252.parquet (50000 filas)\n",
      "📝 Procesando batch #254...\n",
      "✅ Batch #254 guardado en dataset_eb_tess_labeled_batch_0253.parquet (50000 filas)\n",
      "📝 Procesando batch #255...\n",
      "✅ Batch #255 guardado en dataset_eb_tess_labeled_batch_0254.parquet (50000 filas)\n",
      "📝 Procesando batch #256...\n",
      "✅ Batch #256 guardado en dataset_eb_tess_labeled_batch_0255.parquet (50000 filas)\n",
      "📝 Procesando batch #257...\n",
      "✅ Batch #257 guardado en dataset_eb_tess_labeled_batch_0256.parquet (50000 filas)\n",
      "📝 Procesando batch #258...\n",
      "✅ Batch #258 guardado en dataset_eb_tess_labeled_batch_0257.parquet (50000 filas)\n",
      "📝 Procesando batch #259...\n",
      "✅ Batch #259 guardado en dataset_eb_tess_labeled_batch_0258.parquet (50000 filas)\n",
      "📝 Procesando batch #260...\n",
      "✅ Batch #260 guardado en dataset_eb_tess_labeled_batch_0259.parquet (50000 filas)\n",
      "📝 Procesando batch #261...\n",
      "✅ Batch #261 guardado en dataset_eb_tess_labeled_batch_0260.parquet (50000 filas)\n",
      "📝 Procesando batch #262...\n",
      "✅ Batch #262 guardado en dataset_eb_tess_labeled_batch_0261.parquet (50000 filas)\n",
      "📝 Procesando batch #263...\n",
      "✅ Batch #263 guardado en dataset_eb_tess_labeled_batch_0262.parquet (50000 filas)\n",
      "📝 Procesando batch #264...\n",
      "✅ Batch #264 guardado en dataset_eb_tess_labeled_batch_0263.parquet (48576 filas)\n",
      "📝 Procesando batch #265...\n",
      "✅ Batch #265 guardado en dataset_eb_tess_labeled_batch_0264.parquet (50000 filas)\n",
      "📝 Procesando batch #266...\n",
      "✅ Batch #266 guardado en dataset_eb_tess_labeled_batch_0265.parquet (50000 filas)\n",
      "📝 Procesando batch #267...\n",
      "✅ Batch #267 guardado en dataset_eb_tess_labeled_batch_0266.parquet (50000 filas)\n",
      "📝 Procesando batch #268...\n",
      "✅ Batch #268 guardado en dataset_eb_tess_labeled_batch_0267.parquet (50000 filas)\n",
      "📝 Procesando batch #269...\n",
      "✅ Batch #269 guardado en dataset_eb_tess_labeled_batch_0268.parquet (50000 filas)\n",
      "📝 Procesando batch #270...\n",
      "✅ Batch #270 guardado en dataset_eb_tess_labeled_batch_0269.parquet (50000 filas)\n",
      "📝 Procesando batch #271...\n",
      "✅ Batch #271 guardado en dataset_eb_tess_labeled_batch_0270.parquet (50000 filas)\n",
      "📝 Procesando batch #272...\n",
      "✅ Batch #272 guardado en dataset_eb_tess_labeled_batch_0271.parquet (50000 filas)\n",
      "📝 Procesando batch #273...\n",
      "✅ Batch #273 guardado en dataset_eb_tess_labeled_batch_0272.parquet (50000 filas)\n",
      "📝 Procesando batch #274...\n",
      "✅ Batch #274 guardado en dataset_eb_tess_labeled_batch_0273.parquet (50000 filas)\n",
      "📝 Procesando batch #275...\n",
      "✅ Batch #275 guardado en dataset_eb_tess_labeled_batch_0274.parquet (50000 filas)\n",
      "📝 Procesando batch #276...\n",
      "✅ Batch #276 guardado en dataset_eb_tess_labeled_batch_0275.parquet (50000 filas)\n",
      "📝 Procesando batch #277...\n",
      "✅ Batch #277 guardado en dataset_eb_tess_labeled_batch_0276.parquet (50000 filas)\n",
      "📝 Procesando batch #278...\n",
      "✅ Batch #278 guardado en dataset_eb_tess_labeled_batch_0277.parquet (50000 filas)\n",
      "📝 Procesando batch #279...\n",
      "✅ Batch #279 guardado en dataset_eb_tess_labeled_batch_0278.parquet (50000 filas)\n",
      "📝 Procesando batch #280...\n",
      "✅ Batch #280 guardado en dataset_eb_tess_labeled_batch_0279.parquet (50000 filas)\n",
      "📝 Procesando batch #281...\n",
      "✅ Batch #281 guardado en dataset_eb_tess_labeled_batch_0280.parquet (50000 filas)\n",
      "📝 Procesando batch #282...\n",
      "✅ Batch #282 guardado en dataset_eb_tess_labeled_batch_0281.parquet (50000 filas)\n",
      "📝 Procesando batch #283...\n",
      "✅ Batch #283 guardado en dataset_eb_tess_labeled_batch_0282.parquet (50000 filas)\n",
      "📝 Procesando batch #284...\n",
      "✅ Batch #284 guardado en dataset_eb_tess_labeled_batch_0283.parquet (50000 filas)\n",
      "📝 Procesando batch #285...\n",
      "✅ Batch #285 guardado en dataset_eb_tess_labeled_batch_0284.parquet (48576 filas)\n",
      "📝 Procesando batch #286...\n",
      "✅ Batch #286 guardado en dataset_eb_tess_labeled_batch_0285.parquet (50000 filas)\n",
      "📝 Procesando batch #287...\n",
      "✅ Batch #287 guardado en dataset_eb_tess_labeled_batch_0286.parquet (50000 filas)\n",
      "📝 Procesando batch #288...\n",
      "✅ Batch #288 guardado en dataset_eb_tess_labeled_batch_0287.parquet (50000 filas)\n",
      "📝 Procesando batch #289...\n",
      "✅ Batch #289 guardado en dataset_eb_tess_labeled_batch_0288.parquet (50000 filas)\n",
      "📝 Procesando batch #290...\n",
      "✅ Batch #290 guardado en dataset_eb_tess_labeled_batch_0289.parquet (50000 filas)\n",
      "📝 Procesando batch #291...\n",
      "✅ Batch #291 guardado en dataset_eb_tess_labeled_batch_0290.parquet (50000 filas)\n",
      "📝 Procesando batch #292...\n",
      "✅ Batch #292 guardado en dataset_eb_tess_labeled_batch_0291.parquet (50000 filas)\n",
      "📝 Procesando batch #293...\n",
      "✅ Batch #293 guardado en dataset_eb_tess_labeled_batch_0292.parquet (50000 filas)\n",
      "📝 Procesando batch #294...\n",
      "✅ Batch #294 guardado en dataset_eb_tess_labeled_batch_0293.parquet (50000 filas)\n",
      "📝 Procesando batch #295...\n",
      "✅ Batch #295 guardado en dataset_eb_tess_labeled_batch_0294.parquet (50000 filas)\n",
      "📝 Procesando batch #296...\n",
      "✅ Batch #296 guardado en dataset_eb_tess_labeled_batch_0295.parquet (50000 filas)\n",
      "📝 Procesando batch #297...\n",
      "✅ Batch #297 guardado en dataset_eb_tess_labeled_batch_0296.parquet (50000 filas)\n",
      "📝 Procesando batch #298...\n",
      "✅ Batch #298 guardado en dataset_eb_tess_labeled_batch_0297.parquet (50000 filas)\n",
      "📝 Procesando batch #299...\n",
      "✅ Batch #299 guardado en dataset_eb_tess_labeled_batch_0298.parquet (50000 filas)\n",
      "📝 Procesando batch #300...\n",
      "✅ Batch #300 guardado en dataset_eb_tess_labeled_batch_0299.parquet (50000 filas)\n",
      "📝 Procesando batch #301...\n",
      "✅ Batch #301 guardado en dataset_eb_tess_labeled_batch_0300.parquet (50000 filas)\n",
      "📝 Procesando batch #302...\n",
      "✅ Batch #302 guardado en dataset_eb_tess_labeled_batch_0301.parquet (50000 filas)\n",
      "📝 Procesando batch #303...\n",
      "✅ Batch #303 guardado en dataset_eb_tess_labeled_batch_0302.parquet (50000 filas)\n",
      "📝 Procesando batch #304...\n",
      "✅ Batch #304 guardado en dataset_eb_tess_labeled_batch_0303.parquet (50000 filas)\n",
      "📝 Procesando batch #305...\n",
      "✅ Batch #305 guardado en dataset_eb_tess_labeled_batch_0304.parquet (50000 filas)\n",
      "📝 Procesando batch #306...\n",
      "✅ Batch #306 guardado en dataset_eb_tess_labeled_batch_0305.parquet (48576 filas)\n",
      "📝 Procesando batch #307...\n",
      "✅ Batch #307 guardado en dataset_eb_tess_labeled_batch_0306.parquet (50000 filas)\n",
      "📝 Procesando batch #308...\n",
      "✅ Batch #308 guardado en dataset_eb_tess_labeled_batch_0307.parquet (50000 filas)\n",
      "📝 Procesando batch #309...\n",
      "✅ Batch #309 guardado en dataset_eb_tess_labeled_batch_0308.parquet (50000 filas)\n",
      "📝 Procesando batch #310...\n",
      "✅ Batch #310 guardado en dataset_eb_tess_labeled_batch_0309.parquet (50000 filas)\n",
      "📝 Procesando batch #311...\n",
      "✅ Batch #311 guardado en dataset_eb_tess_labeled_batch_0310.parquet (50000 filas)\n",
      "📝 Procesando batch #312...\n",
      "✅ Batch #312 guardado en dataset_eb_tess_labeled_batch_0311.parquet (50000 filas)\n",
      "📝 Procesando batch #313...\n",
      "✅ Batch #313 guardado en dataset_eb_tess_labeled_batch_0312.parquet (50000 filas)\n",
      "📝 Procesando batch #314...\n",
      "✅ Batch #314 guardado en dataset_eb_tess_labeled_batch_0313.parquet (50000 filas)\n",
      "📝 Procesando batch #315...\n",
      "✅ Batch #315 guardado en dataset_eb_tess_labeled_batch_0314.parquet (50000 filas)\n",
      "📝 Procesando batch #316...\n",
      "✅ Batch #316 guardado en dataset_eb_tess_labeled_batch_0315.parquet (50000 filas)\n",
      "📝 Procesando batch #317...\n",
      "✅ Batch #317 guardado en dataset_eb_tess_labeled_batch_0316.parquet (50000 filas)\n",
      "📝 Procesando batch #318...\n",
      "✅ Batch #318 guardado en dataset_eb_tess_labeled_batch_0317.parquet (50000 filas)\n",
      "📝 Procesando batch #319...\n",
      "✅ Batch #319 guardado en dataset_eb_tess_labeled_batch_0318.parquet (22283 filas)\n",
      "📝 Procesando batch #320...\n",
      "✅ Batch #320 guardado en dataset_eb_tess_labeled_batch_0319.parquet (50000 filas)\n",
      "📝 Procesando batch #321...\n",
      "✅ Batch #321 guardado en dataset_eb_tess_labeled_batch_0320.parquet (50000 filas)\n",
      "📝 Procesando batch #322...\n",
      "✅ Batch #322 guardado en dataset_eb_tess_labeled_batch_0321.parquet (50000 filas)\n",
      "📝 Procesando batch #323...\n",
      "✅ Batch #323 guardado en dataset_eb_tess_labeled_batch_0322.parquet (50000 filas)\n",
      "📝 Procesando batch #324...\n",
      "✅ Batch #324 guardado en dataset_eb_tess_labeled_batch_0323.parquet (50000 filas)\n",
      "📝 Procesando batch #325...\n",
      "✅ Batch #325 guardado en dataset_eb_tess_labeled_batch_0324.parquet (50000 filas)\n",
      "📝 Procesando batch #326...\n",
      "✅ Batch #326 guardado en dataset_eb_tess_labeled_batch_0325.parquet (50000 filas)\n",
      "📝 Procesando batch #327...\n",
      "✅ Batch #327 guardado en dataset_eb_tess_labeled_batch_0326.parquet (50000 filas)\n",
      "📝 Procesando batch #328...\n",
      "✅ Batch #328 guardado en dataset_eb_tess_labeled_batch_0327.parquet (50000 filas)\n",
      "📝 Procesando batch #329...\n",
      "✅ Batch #329 guardado en dataset_eb_tess_labeled_batch_0328.parquet (50000 filas)\n",
      "📝 Procesando batch #330...\n",
      "✅ Batch #330 guardado en dataset_eb_tess_labeled_batch_0329.parquet (50000 filas)\n",
      "📝 Procesando batch #331...\n",
      "✅ Batch #331 guardado en dataset_eb_tess_labeled_batch_0330.parquet (50000 filas)\n",
      "📝 Procesando batch #332...\n",
      "✅ Batch #332 guardado en dataset_eb_tess_labeled_batch_0331.parquet (50000 filas)\n",
      "📝 Procesando batch #333...\n",
      "✅ Batch #333 guardado en dataset_eb_tess_labeled_batch_0332.parquet (50000 filas)\n",
      "📝 Procesando batch #334...\n",
      "✅ Batch #334 guardado en dataset_eb_tess_labeled_batch_0333.parquet (50000 filas)\n",
      "📝 Procesando batch #335...\n",
      "✅ Batch #335 guardado en dataset_eb_tess_labeled_batch_0334.parquet (50000 filas)\n",
      "📝 Procesando batch #336...\n",
      "✅ Batch #336 guardado en dataset_eb_tess_labeled_batch_0335.parquet (50000 filas)\n",
      "📝 Procesando batch #337...\n",
      "✅ Batch #337 guardado en dataset_eb_tess_labeled_batch_0336.parquet (50000 filas)\n",
      "📝 Procesando batch #338...\n",
      "✅ Batch #338 guardado en dataset_eb_tess_labeled_batch_0337.parquet (50000 filas)\n",
      "📝 Procesando batch #339...\n",
      "✅ Batch #339 guardado en dataset_eb_tess_labeled_batch_0338.parquet (50000 filas)\n",
      "📝 Procesando batch #340...\n",
      "✅ Batch #340 guardado en dataset_eb_tess_labeled_batch_0339.parquet (48576 filas)\n",
      "📝 Procesando batch #341...\n",
      "✅ Batch #341 guardado en dataset_eb_tess_labeled_batch_0340.parquet (50000 filas)\n",
      "📝 Procesando batch #342...\n",
      "✅ Batch #342 guardado en dataset_eb_tess_labeled_batch_0341.parquet (50000 filas)\n",
      "📝 Procesando batch #343...\n",
      "✅ Batch #343 guardado en dataset_eb_tess_labeled_batch_0342.parquet (50000 filas)\n",
      "📝 Procesando batch #344...\n",
      "✅ Batch #344 guardado en dataset_eb_tess_labeled_batch_0343.parquet (50000 filas)\n",
      "📝 Procesando batch #345...\n",
      "✅ Batch #345 guardado en dataset_eb_tess_labeled_batch_0344.parquet (50000 filas)\n",
      "📝 Procesando batch #346...\n",
      "✅ Batch #346 guardado en dataset_eb_tess_labeled_batch_0345.parquet (50000 filas)\n",
      "📝 Procesando batch #347...\n",
      "✅ Batch #347 guardado en dataset_eb_tess_labeled_batch_0346.parquet (50000 filas)\n",
      "📝 Procesando batch #348...\n",
      "✅ Batch #348 guardado en dataset_eb_tess_labeled_batch_0347.parquet (50000 filas)\n",
      "📝 Procesando batch #349...\n",
      "✅ Batch #349 guardado en dataset_eb_tess_labeled_batch_0348.parquet (50000 filas)\n",
      "📝 Procesando batch #350...\n",
      "✅ Batch #350 guardado en dataset_eb_tess_labeled_batch_0349.parquet (50000 filas)\n",
      "📝 Procesando batch #351...\n",
      "✅ Batch #351 guardado en dataset_eb_tess_labeled_batch_0350.parquet (50000 filas)\n",
      "📝 Procesando batch #352...\n",
      "✅ Batch #352 guardado en dataset_eb_tess_labeled_batch_0351.parquet (50000 filas)\n",
      "📝 Procesando batch #353...\n",
      "✅ Batch #353 guardado en dataset_eb_tess_labeled_batch_0352.parquet (50000 filas)\n",
      "📝 Procesando batch #354...\n",
      "✅ Batch #354 guardado en dataset_eb_tess_labeled_batch_0353.parquet (50000 filas)\n",
      "📝 Procesando batch #355...\n",
      "✅ Batch #355 guardado en dataset_eb_tess_labeled_batch_0354.parquet (50000 filas)\n",
      "📝 Procesando batch #356...\n",
      "✅ Batch #356 guardado en dataset_eb_tess_labeled_batch_0355.parquet (50000 filas)\n",
      "📝 Procesando batch #357...\n",
      "✅ Batch #357 guardado en dataset_eb_tess_labeled_batch_0356.parquet (50000 filas)\n",
      "📝 Procesando batch #358...\n",
      "✅ Batch #358 guardado en dataset_eb_tess_labeled_batch_0357.parquet (50000 filas)\n",
      "📝 Procesando batch #359...\n",
      "✅ Batch #359 guardado en dataset_eb_tess_labeled_batch_0358.parquet (50000 filas)\n",
      "📝 Procesando batch #360...\n",
      "✅ Batch #360 guardado en dataset_eb_tess_labeled_batch_0359.parquet (50000 filas)\n",
      "📝 Procesando batch #361...\n",
      "✅ Batch #361 guardado en dataset_eb_tess_labeled_batch_0360.parquet (48576 filas)\n",
      "📝 Procesando batch #362...\n",
      "✅ Batch #362 guardado en dataset_eb_tess_labeled_batch_0361.parquet (50000 filas)\n",
      "📝 Procesando batch #363...\n",
      "✅ Batch #363 guardado en dataset_eb_tess_labeled_batch_0362.parquet (50000 filas)\n",
      "📝 Procesando batch #364...\n",
      "✅ Batch #364 guardado en dataset_eb_tess_labeled_batch_0363.parquet (50000 filas)\n",
      "📝 Procesando batch #365...\n",
      "✅ Batch #365 guardado en dataset_eb_tess_labeled_batch_0364.parquet (50000 filas)\n",
      "📝 Procesando batch #366...\n",
      "✅ Batch #366 guardado en dataset_eb_tess_labeled_batch_0365.parquet (50000 filas)\n",
      "📝 Procesando batch #367...\n",
      "✅ Batch #367 guardado en dataset_eb_tess_labeled_batch_0366.parquet (50000 filas)\n",
      "📝 Procesando batch #368...\n",
      "✅ Batch #368 guardado en dataset_eb_tess_labeled_batch_0367.parquet (50000 filas)\n",
      "📝 Procesando batch #369...\n",
      "✅ Batch #369 guardado en dataset_eb_tess_labeled_batch_0368.parquet (50000 filas)\n",
      "📝 Procesando batch #370...\n",
      "✅ Batch #370 guardado en dataset_eb_tess_labeled_batch_0369.parquet (50000 filas)\n",
      "📝 Procesando batch #371...\n",
      "✅ Batch #371 guardado en dataset_eb_tess_labeled_batch_0370.parquet (50000 filas)\n",
      "📝 Procesando batch #372...\n",
      "✅ Batch #372 guardado en dataset_eb_tess_labeled_batch_0371.parquet (50000 filas)\n",
      "📝 Procesando batch #373...\n",
      "✅ Batch #373 guardado en dataset_eb_tess_labeled_batch_0372.parquet (50000 filas)\n",
      "📝 Procesando batch #374...\n",
      "✅ Batch #374 guardado en dataset_eb_tess_labeled_batch_0373.parquet (50000 filas)\n",
      "📝 Procesando batch #375...\n",
      "✅ Batch #375 guardado en dataset_eb_tess_labeled_batch_0374.parquet (50000 filas)\n",
      "📝 Procesando batch #376...\n",
      "✅ Batch #376 guardado en dataset_eb_tess_labeled_batch_0375.parquet (50000 filas)\n",
      "📝 Procesando batch #377...\n",
      "✅ Batch #377 guardado en dataset_eb_tess_labeled_batch_0376.parquet (50000 filas)\n",
      "📝 Procesando batch #378...\n",
      "✅ Batch #378 guardado en dataset_eb_tess_labeled_batch_0377.parquet (50000 filas)\n",
      "📝 Procesando batch #379...\n",
      "✅ Batch #379 guardado en dataset_eb_tess_labeled_batch_0378.parquet (50000 filas)\n",
      "📝 Procesando batch #380...\n",
      "✅ Batch #380 guardado en dataset_eb_tess_labeled_batch_0379.parquet (50000 filas)\n",
      "📝 Procesando batch #381...\n",
      "✅ Batch #381 guardado en dataset_eb_tess_labeled_batch_0380.parquet (50000 filas)\n",
      "📝 Procesando batch #382...\n",
      "✅ Batch #382 guardado en dataset_eb_tess_labeled_batch_0381.parquet (48576 filas)\n",
      "📝 Procesando batch #383...\n",
      "✅ Batch #383 guardado en dataset_eb_tess_labeled_batch_0382.parquet (50000 filas)\n",
      "📝 Procesando batch #384...\n",
      "✅ Batch #384 guardado en dataset_eb_tess_labeled_batch_0383.parquet (50000 filas)\n",
      "📝 Procesando batch #385...\n",
      "✅ Batch #385 guardado en dataset_eb_tess_labeled_batch_0384.parquet (50000 filas)\n",
      "📝 Procesando batch #386...\n",
      "✅ Batch #386 guardado en dataset_eb_tess_labeled_batch_0385.parquet (50000 filas)\n",
      "📝 Procesando batch #387...\n",
      "✅ Batch #387 guardado en dataset_eb_tess_labeled_batch_0386.parquet (50000 filas)\n",
      "📝 Procesando batch #388...\n",
      "✅ Batch #388 guardado en dataset_eb_tess_labeled_batch_0387.parquet (50000 filas)\n",
      "📝 Procesando batch #389...\n",
      "✅ Batch #389 guardado en dataset_eb_tess_labeled_batch_0388.parquet (50000 filas)\n",
      "📝 Procesando batch #390...\n",
      "✅ Batch #390 guardado en dataset_eb_tess_labeled_batch_0389.parquet (50000 filas)\n",
      "📝 Procesando batch #391...\n",
      "✅ Batch #391 guardado en dataset_eb_tess_labeled_batch_0390.parquet (50000 filas)\n",
      "📝 Procesando batch #392...\n",
      "✅ Batch #392 guardado en dataset_eb_tess_labeled_batch_0391.parquet (50000 filas)\n",
      "📝 Procesando batch #393...\n",
      "✅ Batch #393 guardado en dataset_eb_tess_labeled_batch_0392.parquet (50000 filas)\n",
      "📝 Procesando batch #394...\n",
      "✅ Batch #394 guardado en dataset_eb_tess_labeled_batch_0393.parquet (50000 filas)\n",
      "📝 Procesando batch #395...\n",
      "✅ Batch #395 guardado en dataset_eb_tess_labeled_batch_0394.parquet (50000 filas)\n",
      "📝 Procesando batch #396...\n",
      "✅ Batch #396 guardado en dataset_eb_tess_labeled_batch_0395.parquet (50000 filas)\n",
      "📝 Procesando batch #397...\n",
      "✅ Batch #397 guardado en dataset_eb_tess_labeled_batch_0396.parquet (50000 filas)\n",
      "📝 Procesando batch #398...\n",
      "✅ Batch #398 guardado en dataset_eb_tess_labeled_batch_0397.parquet (50000 filas)\n",
      "📝 Procesando batch #399...\n",
      "✅ Batch #399 guardado en dataset_eb_tess_labeled_batch_0398.parquet (50000 filas)\n",
      "📝 Procesando batch #400...\n",
      "✅ Batch #400 guardado en dataset_eb_tess_labeled_batch_0399.parquet (50000 filas)\n",
      "📝 Procesando batch #401...\n",
      "✅ Batch #401 guardado en dataset_eb_tess_labeled_batch_0400.parquet (50000 filas)\n",
      "📝 Procesando batch #402...\n",
      "✅ Batch #402 guardado en dataset_eb_tess_labeled_batch_0401.parquet (50000 filas)\n",
      "📝 Procesando batch #403...\n",
      "✅ Batch #403 guardado en dataset_eb_tess_labeled_batch_0402.parquet (48576 filas)\n",
      "📝 Procesando batch #404...\n",
      "✅ Batch #404 guardado en dataset_eb_tess_labeled_batch_0403.parquet (50000 filas)\n",
      "📝 Procesando batch #405...\n",
      "✅ Batch #405 guardado en dataset_eb_tess_labeled_batch_0404.parquet (50000 filas)\n",
      "📝 Procesando batch #406...\n",
      "✅ Batch #406 guardado en dataset_eb_tess_labeled_batch_0405.parquet (50000 filas)\n",
      "📝 Procesando batch #407...\n",
      "✅ Batch #407 guardado en dataset_eb_tess_labeled_batch_0406.parquet (50000 filas)\n",
      "📝 Procesando batch #408...\n",
      "✅ Batch #408 guardado en dataset_eb_tess_labeled_batch_0407.parquet (50000 filas)\n",
      "📝 Procesando batch #409...\n",
      "✅ Batch #409 guardado en dataset_eb_tess_labeled_batch_0408.parquet (50000 filas)\n",
      "📝 Procesando batch #410...\n",
      "✅ Batch #410 guardado en dataset_eb_tess_labeled_batch_0409.parquet (50000 filas)\n",
      "📝 Procesando batch #411...\n",
      "✅ Batch #411 guardado en dataset_eb_tess_labeled_batch_0410.parquet (50000 filas)\n",
      "📝 Procesando batch #412...\n",
      "✅ Batch #412 guardado en dataset_eb_tess_labeled_batch_0411.parquet (50000 filas)\n",
      "📝 Procesando batch #413...\n",
      "✅ Batch #413 guardado en dataset_eb_tess_labeled_batch_0412.parquet (50000 filas)\n",
      "📝 Procesando batch #414...\n",
      "✅ Batch #414 guardado en dataset_eb_tess_labeled_batch_0413.parquet (50000 filas)\n",
      "📝 Procesando batch #415...\n",
      "✅ Batch #415 guardado en dataset_eb_tess_labeled_batch_0414.parquet (50000 filas)\n",
      "📝 Procesando batch #416...\n",
      "✅ Batch #416 guardado en dataset_eb_tess_labeled_batch_0415.parquet (50000 filas)\n",
      "📝 Procesando batch #417...\n",
      "✅ Batch #417 guardado en dataset_eb_tess_labeled_batch_0416.parquet (50000 filas)\n",
      "📝 Procesando batch #418...\n",
      "✅ Batch #418 guardado en dataset_eb_tess_labeled_batch_0417.parquet (50000 filas)\n",
      "📝 Procesando batch #419...\n",
      "✅ Batch #419 guardado en dataset_eb_tess_labeled_batch_0418.parquet (50000 filas)\n",
      "📝 Procesando batch #420...\n",
      "✅ Batch #420 guardado en dataset_eb_tess_labeled_batch_0419.parquet (50000 filas)\n",
      "📝 Procesando batch #421...\n",
      "✅ Batch #421 guardado en dataset_eb_tess_labeled_batch_0420.parquet (50000 filas)\n",
      "📝 Procesando batch #422...\n",
      "✅ Batch #422 guardado en dataset_eb_tess_labeled_batch_0421.parquet (50000 filas)\n",
      "📝 Procesando batch #423...\n",
      "✅ Batch #423 guardado en dataset_eb_tess_labeled_batch_0422.parquet (50000 filas)\n",
      "📝 Procesando batch #424...\n",
      "✅ Batch #424 guardado en dataset_eb_tess_labeled_batch_0423.parquet (48576 filas)\n",
      "📝 Procesando batch #425...\n",
      "✅ Batch #425 guardado en dataset_eb_tess_labeled_batch_0424.parquet (50000 filas)\n",
      "📝 Procesando batch #426...\n",
      "✅ Batch #426 guardado en dataset_eb_tess_labeled_batch_0425.parquet (50000 filas)\n",
      "📝 Procesando batch #427...\n",
      "✅ Batch #427 guardado en dataset_eb_tess_labeled_batch_0426.parquet (50000 filas)\n",
      "📝 Procesando batch #428...\n",
      "✅ Batch #428 guardado en dataset_eb_tess_labeled_batch_0427.parquet (50000 filas)\n",
      "📝 Procesando batch #429...\n",
      "✅ Batch #429 guardado en dataset_eb_tess_labeled_batch_0428.parquet (50000 filas)\n",
      "📝 Procesando batch #430...\n",
      "✅ Batch #430 guardado en dataset_eb_tess_labeled_batch_0429.parquet (50000 filas)\n",
      "📝 Procesando batch #431...\n",
      "✅ Batch #431 guardado en dataset_eb_tess_labeled_batch_0430.parquet (50000 filas)\n",
      "📝 Procesando batch #432...\n",
      "✅ Batch #432 guardado en dataset_eb_tess_labeled_batch_0431.parquet (50000 filas)\n",
      "📝 Procesando batch #433...\n",
      "✅ Batch #433 guardado en dataset_eb_tess_labeled_batch_0432.parquet (50000 filas)\n",
      "📝 Procesando batch #434...\n",
      "✅ Batch #434 guardado en dataset_eb_tess_labeled_batch_0433.parquet (50000 filas)\n",
      "📝 Procesando batch #435...\n",
      "✅ Batch #435 guardado en dataset_eb_tess_labeled_batch_0434.parquet (50000 filas)\n",
      "📝 Procesando batch #436...\n",
      "✅ Batch #436 guardado en dataset_eb_tess_labeled_batch_0435.parquet (50000 filas)\n",
      "📝 Procesando batch #437...\n",
      "✅ Batch #437 guardado en dataset_eb_tess_labeled_batch_0436.parquet (50000 filas)\n",
      "📝 Procesando batch #438...\n",
      "✅ Batch #438 guardado en dataset_eb_tess_labeled_batch_0437.parquet (50000 filas)\n",
      "📝 Procesando batch #439...\n",
      "✅ Batch #439 guardado en dataset_eb_tess_labeled_batch_0438.parquet (50000 filas)\n",
      "📝 Procesando batch #440...\n",
      "✅ Batch #440 guardado en dataset_eb_tess_labeled_batch_0439.parquet (50000 filas)\n",
      "📝 Procesando batch #441...\n",
      "✅ Batch #441 guardado en dataset_eb_tess_labeled_batch_0440.parquet (50000 filas)\n",
      "📝 Procesando batch #442...\n",
      "✅ Batch #442 guardado en dataset_eb_tess_labeled_batch_0441.parquet (50000 filas)\n",
      "📝 Procesando batch #443...\n",
      "✅ Batch #443 guardado en dataset_eb_tess_labeled_batch_0442.parquet (50000 filas)\n",
      "📝 Procesando batch #444...\n",
      "✅ Batch #444 guardado en dataset_eb_tess_labeled_batch_0443.parquet (50000 filas)\n",
      "📝 Procesando batch #445...\n",
      "✅ Batch #445 guardado en dataset_eb_tess_labeled_batch_0444.parquet (48576 filas)\n",
      "📝 Procesando batch #446...\n",
      "✅ Batch #446 guardado en dataset_eb_tess_labeled_batch_0445.parquet (50000 filas)\n",
      "📝 Procesando batch #447...\n",
      "✅ Batch #447 guardado en dataset_eb_tess_labeled_batch_0446.parquet (50000 filas)\n",
      "📝 Procesando batch #448...\n",
      "✅ Batch #448 guardado en dataset_eb_tess_labeled_batch_0447.parquet (50000 filas)\n",
      "📝 Procesando batch #449...\n",
      "✅ Batch #449 guardado en dataset_eb_tess_labeled_batch_0448.parquet (50000 filas)\n",
      "📝 Procesando batch #450...\n",
      "✅ Batch #450 guardado en dataset_eb_tess_labeled_batch_0449.parquet (50000 filas)\n",
      "📝 Procesando batch #451...\n",
      "✅ Batch #451 guardado en dataset_eb_tess_labeled_batch_0450.parquet (50000 filas)\n",
      "📝 Procesando batch #452...\n",
      "✅ Batch #452 guardado en dataset_eb_tess_labeled_batch_0451.parquet (50000 filas)\n",
      "📝 Procesando batch #453...\n",
      "✅ Batch #453 guardado en dataset_eb_tess_labeled_batch_0452.parquet (50000 filas)\n",
      "📝 Procesando batch #454...\n",
      "✅ Batch #454 guardado en dataset_eb_tess_labeled_batch_0453.parquet (50000 filas)\n",
      "📝 Procesando batch #455...\n",
      "✅ Batch #455 guardado en dataset_eb_tess_labeled_batch_0454.parquet (50000 filas)\n",
      "📝 Procesando batch #456...\n",
      "✅ Batch #456 guardado en dataset_eb_tess_labeled_batch_0455.parquet (50000 filas)\n",
      "📝 Procesando batch #457...\n",
      "✅ Batch #457 guardado en dataset_eb_tess_labeled_batch_0456.parquet (50000 filas)\n",
      "📝 Procesando batch #458...\n",
      "✅ Batch #458 guardado en dataset_eb_tess_labeled_batch_0457.parquet (50000 filas)\n",
      "📝 Procesando batch #459...\n",
      "✅ Batch #459 guardado en dataset_eb_tess_labeled_batch_0458.parquet (50000 filas)\n",
      "📝 Procesando batch #460...\n",
      "✅ Batch #460 guardado en dataset_eb_tess_labeled_batch_0459.parquet (50000 filas)\n",
      "📝 Procesando batch #461...\n",
      "✅ Batch #461 guardado en dataset_eb_tess_labeled_batch_0460.parquet (50000 filas)\n",
      "📝 Procesando batch #462...\n",
      "✅ Batch #462 guardado en dataset_eb_tess_labeled_batch_0461.parquet (50000 filas)\n",
      "📝 Procesando batch #463...\n",
      "✅ Batch #463 guardado en dataset_eb_tess_labeled_batch_0462.parquet (50000 filas)\n",
      "📝 Procesando batch #464...\n",
      "✅ Batch #464 guardado en dataset_eb_tess_labeled_batch_0463.parquet (50000 filas)\n",
      "📝 Procesando batch #465...\n",
      "✅ Batch #465 guardado en dataset_eb_tess_labeled_batch_0464.parquet (50000 filas)\n",
      "📝 Procesando batch #466...\n",
      "✅ Batch #466 guardado en dataset_eb_tess_labeled_batch_0465.parquet (48576 filas)\n",
      "📝 Procesando batch #467...\n",
      "✅ Batch #467 guardado en dataset_eb_tess_labeled_batch_0466.parquet (50000 filas)\n",
      "📝 Procesando batch #468...\n",
      "✅ Batch #468 guardado en dataset_eb_tess_labeled_batch_0467.parquet (50000 filas)\n",
      "📝 Procesando batch #469...\n",
      "✅ Batch #469 guardado en dataset_eb_tess_labeled_batch_0468.parquet (50000 filas)\n",
      "📝 Procesando batch #470...\n",
      "✅ Batch #470 guardado en dataset_eb_tess_labeled_batch_0469.parquet (50000 filas)\n",
      "📝 Procesando batch #471...\n",
      "✅ Batch #471 guardado en dataset_eb_tess_labeled_batch_0470.parquet (50000 filas)\n",
      "📝 Procesando batch #472...\n",
      "✅ Batch #472 guardado en dataset_eb_tess_labeled_batch_0471.parquet (50000 filas)\n",
      "📝 Procesando batch #473...\n",
      "✅ Batch #473 guardado en dataset_eb_tess_labeled_batch_0472.parquet (50000 filas)\n",
      "📝 Procesando batch #474...\n",
      "✅ Batch #474 guardado en dataset_eb_tess_labeled_batch_0473.parquet (50000 filas)\n",
      "📝 Procesando batch #475...\n",
      "✅ Batch #475 guardado en dataset_eb_tess_labeled_batch_0474.parquet (50000 filas)\n",
      "📝 Procesando batch #476...\n",
      "✅ Batch #476 guardado en dataset_eb_tess_labeled_batch_0475.parquet (50000 filas)\n",
      "📝 Procesando batch #477...\n",
      "✅ Batch #477 guardado en dataset_eb_tess_labeled_batch_0476.parquet (50000 filas)\n",
      "📝 Procesando batch #478...\n",
      "✅ Batch #478 guardado en dataset_eb_tess_labeled_batch_0477.parquet (50000 filas)\n",
      "📝 Procesando batch #479...\n",
      "✅ Batch #479 guardado en dataset_eb_tess_labeled_batch_0478.parquet (50000 filas)\n",
      "📝 Procesando batch #480...\n",
      "✅ Batch #480 guardado en dataset_eb_tess_labeled_batch_0479.parquet (50000 filas)\n",
      "📝 Procesando batch #481...\n",
      "✅ Batch #481 guardado en dataset_eb_tess_labeled_batch_0480.parquet (50000 filas)\n",
      "📝 Procesando batch #482...\n",
      "✅ Batch #482 guardado en dataset_eb_tess_labeled_batch_0481.parquet (50000 filas)\n",
      "📝 Procesando batch #483...\n",
      "✅ Batch #483 guardado en dataset_eb_tess_labeled_batch_0482.parquet (50000 filas)\n",
      "📝 Procesando batch #484...\n",
      "✅ Batch #484 guardado en dataset_eb_tess_labeled_batch_0483.parquet (17035 filas)\n",
      "📝 Procesando batch #485...\n",
      "✅ Batch #485 guardado en dataset_eb_tess_labeled_batch_0484.parquet (50000 filas)\n",
      "📝 Procesando batch #486...\n",
      "✅ Batch #486 guardado en dataset_eb_tess_labeled_batch_0485.parquet (50000 filas)\n",
      "📝 Procesando batch #487...\n",
      "✅ Batch #487 guardado en dataset_eb_tess_labeled_batch_0486.parquet (50000 filas)\n",
      "📝 Procesando batch #488...\n",
      "✅ Batch #488 guardado en dataset_eb_tess_labeled_batch_0487.parquet (50000 filas)\n",
      "📝 Procesando batch #489...\n",
      "✅ Batch #489 guardado en dataset_eb_tess_labeled_batch_0488.parquet (50000 filas)\n",
      "📝 Procesando batch #490...\n",
      "✅ Batch #490 guardado en dataset_eb_tess_labeled_batch_0489.parquet (50000 filas)\n",
      "📝 Procesando batch #491...\n",
      "✅ Batch #491 guardado en dataset_eb_tess_labeled_batch_0490.parquet (50000 filas)\n",
      "📝 Procesando batch #492...\n",
      "✅ Batch #492 guardado en dataset_eb_tess_labeled_batch_0491.parquet (50000 filas)\n",
      "📝 Procesando batch #493...\n",
      "✅ Batch #493 guardado en dataset_eb_tess_labeled_batch_0492.parquet (50000 filas)\n",
      "📝 Procesando batch #494...\n",
      "✅ Batch #494 guardado en dataset_eb_tess_labeled_batch_0493.parquet (50000 filas)\n",
      "📝 Procesando batch #495...\n",
      "✅ Batch #495 guardado en dataset_eb_tess_labeled_batch_0494.parquet (50000 filas)\n",
      "📝 Procesando batch #496...\n",
      "✅ Batch #496 guardado en dataset_eb_tess_labeled_batch_0495.parquet (50000 filas)\n",
      "📝 Procesando batch #497...\n",
      "✅ Batch #497 guardado en dataset_eb_tess_labeled_batch_0496.parquet (50000 filas)\n",
      "📝 Procesando batch #498...\n",
      "✅ Batch #498 guardado en dataset_eb_tess_labeled_batch_0497.parquet (50000 filas)\n",
      "📝 Procesando batch #499...\n",
      "✅ Batch #499 guardado en dataset_eb_tess_labeled_batch_0498.parquet (50000 filas)\n",
      "📝 Procesando batch #500...\n",
      "✅ Batch #500 guardado en dataset_eb_tess_labeled_batch_0499.parquet (50000 filas)\n",
      "📝 Procesando batch #501...\n",
      "✅ Batch #501 guardado en dataset_eb_tess_labeled_batch_0500.parquet (50000 filas)\n",
      "📝 Procesando batch #502...\n",
      "✅ Batch #502 guardado en dataset_eb_tess_labeled_batch_0501.parquet (50000 filas)\n",
      "📝 Procesando batch #503...\n",
      "✅ Batch #503 guardado en dataset_eb_tess_labeled_batch_0502.parquet (50000 filas)\n",
      "📝 Procesando batch #504...\n",
      "✅ Batch #504 guardado en dataset_eb_tess_labeled_batch_0503.parquet (50000 filas)\n",
      "📝 Procesando batch #505...\n",
      "✅ Batch #505 guardado en dataset_eb_tess_labeled_batch_0504.parquet (48576 filas)\n",
      "📝 Procesando batch #506...\n",
      "✅ Batch #506 guardado en dataset_eb_tess_labeled_batch_0505.parquet (50000 filas)\n",
      "📝 Procesando batch #507...\n",
      "✅ Batch #507 guardado en dataset_eb_tess_labeled_batch_0506.parquet (50000 filas)\n",
      "📝 Procesando batch #508...\n",
      "✅ Batch #508 guardado en dataset_eb_tess_labeled_batch_0507.parquet (50000 filas)\n",
      "📝 Procesando batch #509...\n",
      "✅ Batch #509 guardado en dataset_eb_tess_labeled_batch_0508.parquet (50000 filas)\n",
      "📝 Procesando batch #510...\n",
      "✅ Batch #510 guardado en dataset_eb_tess_labeled_batch_0509.parquet (50000 filas)\n",
      "📝 Procesando batch #511...\n",
      "✅ Batch #511 guardado en dataset_eb_tess_labeled_batch_0510.parquet (50000 filas)\n",
      "📝 Procesando batch #512...\n",
      "✅ Batch #512 guardado en dataset_eb_tess_labeled_batch_0511.parquet (50000 filas)\n",
      "📝 Procesando batch #513...\n",
      "✅ Batch #513 guardado en dataset_eb_tess_labeled_batch_0512.parquet (50000 filas)\n",
      "📝 Procesando batch #514...\n",
      "✅ Batch #514 guardado en dataset_eb_tess_labeled_batch_0513.parquet (50000 filas)\n",
      "📝 Procesando batch #515...\n",
      "✅ Batch #515 guardado en dataset_eb_tess_labeled_batch_0514.parquet (50000 filas)\n",
      "📝 Procesando batch #516...\n",
      "✅ Batch #516 guardado en dataset_eb_tess_labeled_batch_0515.parquet (50000 filas)\n",
      "📝 Procesando batch #517...\n",
      "✅ Batch #517 guardado en dataset_eb_tess_labeled_batch_0516.parquet (50000 filas)\n",
      "📝 Procesando batch #518...\n",
      "✅ Batch #518 guardado en dataset_eb_tess_labeled_batch_0517.parquet (50000 filas)\n",
      "📝 Procesando batch #519...\n",
      "✅ Batch #519 guardado en dataset_eb_tess_labeled_batch_0518.parquet (50000 filas)\n",
      "📝 Procesando batch #520...\n",
      "✅ Batch #520 guardado en dataset_eb_tess_labeled_batch_0519.parquet (50000 filas)\n",
      "📝 Procesando batch #521...\n",
      "✅ Batch #521 guardado en dataset_eb_tess_labeled_batch_0520.parquet (50000 filas)\n",
      "📝 Procesando batch #522...\n",
      "✅ Batch #522 guardado en dataset_eb_tess_labeled_batch_0521.parquet (50000 filas)\n",
      "📝 Procesando batch #523...\n",
      "✅ Batch #523 guardado en dataset_eb_tess_labeled_batch_0522.parquet (50000 filas)\n",
      "📝 Procesando batch #524...\n",
      "✅ Batch #524 guardado en dataset_eb_tess_labeled_batch_0523.parquet (50000 filas)\n",
      "📝 Procesando batch #525...\n",
      "✅ Batch #525 guardado en dataset_eb_tess_labeled_batch_0524.parquet (50000 filas)\n",
      "📝 Procesando batch #526...\n",
      "✅ Batch #526 guardado en dataset_eb_tess_labeled_batch_0525.parquet (48576 filas)\n",
      "📝 Procesando batch #527...\n",
      "✅ Batch #527 guardado en dataset_eb_tess_labeled_batch_0526.parquet (50000 filas)\n",
      "📝 Procesando batch #528...\n",
      "✅ Batch #528 guardado en dataset_eb_tess_labeled_batch_0527.parquet (50000 filas)\n",
      "📝 Procesando batch #529...\n",
      "✅ Batch #529 guardado en dataset_eb_tess_labeled_batch_0528.parquet (50000 filas)\n",
      "📝 Procesando batch #530...\n",
      "✅ Batch #530 guardado en dataset_eb_tess_labeled_batch_0529.parquet (50000 filas)\n",
      "📝 Procesando batch #531...\n",
      "✅ Batch #531 guardado en dataset_eb_tess_labeled_batch_0530.parquet (50000 filas)\n",
      "📝 Procesando batch #532...\n",
      "✅ Batch #532 guardado en dataset_eb_tess_labeled_batch_0531.parquet (50000 filas)\n",
      "📝 Procesando batch #533...\n",
      "✅ Batch #533 guardado en dataset_eb_tess_labeled_batch_0532.parquet (50000 filas)\n",
      "📝 Procesando batch #534...\n",
      "✅ Batch #534 guardado en dataset_eb_tess_labeled_batch_0533.parquet (50000 filas)\n",
      "📝 Procesando batch #535...\n",
      "✅ Batch #535 guardado en dataset_eb_tess_labeled_batch_0534.parquet (50000 filas)\n",
      "📝 Procesando batch #536...\n",
      "✅ Batch #536 guardado en dataset_eb_tess_labeled_batch_0535.parquet (50000 filas)\n",
      "📝 Procesando batch #537...\n",
      "✅ Batch #537 guardado en dataset_eb_tess_labeled_batch_0536.parquet (50000 filas)\n",
      "📝 Procesando batch #538...\n",
      "✅ Batch #538 guardado en dataset_eb_tess_labeled_batch_0537.parquet (50000 filas)\n",
      "📝 Procesando batch #539...\n",
      "✅ Batch #539 guardado en dataset_eb_tess_labeled_batch_0538.parquet (50000 filas)\n",
      "📝 Procesando batch #540...\n",
      "✅ Batch #540 guardado en dataset_eb_tess_labeled_batch_0539.parquet (50000 filas)\n",
      "📝 Procesando batch #541...\n",
      "✅ Batch #541 guardado en dataset_eb_tess_labeled_batch_0540.parquet (50000 filas)\n",
      "📝 Procesando batch #542...\n",
      "✅ Batch #542 guardado en dataset_eb_tess_labeled_batch_0541.parquet (50000 filas)\n",
      "📝 Procesando batch #543...\n",
      "✅ Batch #543 guardado en dataset_eb_tess_labeled_batch_0542.parquet (50000 filas)\n",
      "📝 Procesando batch #544...\n",
      "✅ Batch #544 guardado en dataset_eb_tess_labeled_batch_0543.parquet (50000 filas)\n",
      "📝 Procesando batch #545...\n",
      "✅ Batch #545 guardado en dataset_eb_tess_labeled_batch_0544.parquet (50000 filas)\n",
      "📝 Procesando batch #546...\n",
      "✅ Batch #546 guardado en dataset_eb_tess_labeled_batch_0545.parquet (50000 filas)\n",
      "📝 Procesando batch #547...\n",
      "✅ Batch #547 guardado en dataset_eb_tess_labeled_batch_0546.parquet (48576 filas)\n",
      "📝 Procesando batch #548...\n",
      "✅ Batch #548 guardado en dataset_eb_tess_labeled_batch_0547.parquet (50000 filas)\n",
      "📝 Procesando batch #549...\n",
      "✅ Batch #549 guardado en dataset_eb_tess_labeled_batch_0548.parquet (50000 filas)\n",
      "📝 Procesando batch #550...\n",
      "✅ Batch #550 guardado en dataset_eb_tess_labeled_batch_0549.parquet (50000 filas)\n",
      "📝 Procesando batch #551...\n",
      "✅ Batch #551 guardado en dataset_eb_tess_labeled_batch_0550.parquet (50000 filas)\n",
      "📝 Procesando batch #552...\n",
      "✅ Batch #552 guardado en dataset_eb_tess_labeled_batch_0551.parquet (50000 filas)\n",
      "📝 Procesando batch #553...\n",
      "✅ Batch #553 guardado en dataset_eb_tess_labeled_batch_0552.parquet (50000 filas)\n",
      "📝 Procesando batch #554...\n",
      "✅ Batch #554 guardado en dataset_eb_tess_labeled_batch_0553.parquet (50000 filas)\n",
      "📝 Procesando batch #555...\n",
      "✅ Batch #555 guardado en dataset_eb_tess_labeled_batch_0554.parquet (50000 filas)\n",
      "📝 Procesando batch #556...\n",
      "✅ Batch #556 guardado en dataset_eb_tess_labeled_batch_0555.parquet (50000 filas)\n",
      "📝 Procesando batch #557...\n",
      "✅ Batch #557 guardado en dataset_eb_tess_labeled_batch_0556.parquet (50000 filas)\n",
      "📝 Procesando batch #558...\n",
      "✅ Batch #558 guardado en dataset_eb_tess_labeled_batch_0557.parquet (50000 filas)\n",
      "📝 Procesando batch #559...\n",
      "✅ Batch #559 guardado en dataset_eb_tess_labeled_batch_0558.parquet (50000 filas)\n",
      "📝 Procesando batch #560...\n",
      "✅ Batch #560 guardado en dataset_eb_tess_labeled_batch_0559.parquet (50000 filas)\n",
      "📝 Procesando batch #561...\n",
      "✅ Batch #561 guardado en dataset_eb_tess_labeled_batch_0560.parquet (50000 filas)\n",
      "📝 Procesando batch #562...\n",
      "✅ Batch #562 guardado en dataset_eb_tess_labeled_batch_0561.parquet (50000 filas)\n",
      "📝 Procesando batch #563...\n",
      "✅ Batch #563 guardado en dataset_eb_tess_labeled_batch_0562.parquet (50000 filas)\n",
      "📝 Procesando batch #564...\n",
      "✅ Batch #564 guardado en dataset_eb_tess_labeled_batch_0563.parquet (50000 filas)\n",
      "📝 Procesando batch #565...\n",
      "✅ Batch #565 guardado en dataset_eb_tess_labeled_batch_0564.parquet (50000 filas)\n",
      "📝 Procesando batch #566...\n",
      "✅ Batch #566 guardado en dataset_eb_tess_labeled_batch_0565.parquet (50000 filas)\n",
      "📝 Procesando batch #567...\n",
      "✅ Batch #567 guardado en dataset_eb_tess_labeled_batch_0566.parquet (50000 filas)\n",
      "📝 Procesando batch #568...\n",
      "✅ Batch #568 guardado en dataset_eb_tess_labeled_batch_0567.parquet (48576 filas)\n",
      "📝 Procesando batch #569...\n",
      "✅ Batch #569 guardado en dataset_eb_tess_labeled_batch_0568.parquet (50000 filas)\n",
      "📝 Procesando batch #570...\n",
      "✅ Batch #570 guardado en dataset_eb_tess_labeled_batch_0569.parquet (50000 filas)\n",
      "📝 Procesando batch #571...\n",
      "✅ Batch #571 guardado en dataset_eb_tess_labeled_batch_0570.parquet (50000 filas)\n",
      "📝 Procesando batch #572...\n",
      "✅ Batch #572 guardado en dataset_eb_tess_labeled_batch_0571.parquet (50000 filas)\n",
      "📝 Procesando batch #573...\n",
      "✅ Batch #573 guardado en dataset_eb_tess_labeled_batch_0572.parquet (50000 filas)\n",
      "📝 Procesando batch #574...\n",
      "✅ Batch #574 guardado en dataset_eb_tess_labeled_batch_0573.parquet (50000 filas)\n",
      "📝 Procesando batch #575...\n",
      "✅ Batch #575 guardado en dataset_eb_tess_labeled_batch_0574.parquet (50000 filas)\n",
      "📝 Procesando batch #576...\n",
      "✅ Batch #576 guardado en dataset_eb_tess_labeled_batch_0575.parquet (50000 filas)\n",
      "📝 Procesando batch #577...\n",
      "✅ Batch #577 guardado en dataset_eb_tess_labeled_batch_0576.parquet (50000 filas)\n",
      "📝 Procesando batch #578...\n",
      "✅ Batch #578 guardado en dataset_eb_tess_labeled_batch_0577.parquet (50000 filas)\n",
      "📝 Procesando batch #579...\n",
      "✅ Batch #579 guardado en dataset_eb_tess_labeled_batch_0578.parquet (50000 filas)\n",
      "📝 Procesando batch #580...\n",
      "✅ Batch #580 guardado en dataset_eb_tess_labeled_batch_0579.parquet (50000 filas)\n",
      "📝 Procesando batch #581...\n",
      "✅ Batch #581 guardado en dataset_eb_tess_labeled_batch_0580.parquet (50000 filas)\n",
      "📝 Procesando batch #582...\n",
      "✅ Batch #582 guardado en dataset_eb_tess_labeled_batch_0581.parquet (50000 filas)\n",
      "📝 Procesando batch #583...\n",
      "✅ Batch #583 guardado en dataset_eb_tess_labeled_batch_0582.parquet (50000 filas)\n",
      "📝 Procesando batch #584...\n",
      "✅ Batch #584 guardado en dataset_eb_tess_labeled_batch_0583.parquet (50000 filas)\n",
      "📝 Procesando batch #585...\n",
      "✅ Batch #585 guardado en dataset_eb_tess_labeled_batch_0584.parquet (50000 filas)\n",
      "📝 Procesando batch #586...\n",
      "✅ Batch #586 guardado en dataset_eb_tess_labeled_batch_0585.parquet (50000 filas)\n",
      "📝 Procesando batch #587...\n",
      "✅ Batch #587 guardado en dataset_eb_tess_labeled_batch_0586.parquet (50000 filas)\n",
      "📝 Procesando batch #588...\n",
      "✅ Batch #588 guardado en dataset_eb_tess_labeled_batch_0587.parquet (50000 filas)\n",
      "📝 Procesando batch #589...\n",
      "✅ Batch #589 guardado en dataset_eb_tess_labeled_batch_0588.parquet (48576 filas)\n",
      "📝 Procesando batch #590...\n",
      "✅ Batch #590 guardado en dataset_eb_tess_labeled_batch_0589.parquet (50000 filas)\n",
      "📝 Procesando batch #591...\n",
      "✅ Batch #591 guardado en dataset_eb_tess_labeled_batch_0590.parquet (50000 filas)\n",
      "📝 Procesando batch #592...\n",
      "✅ Batch #592 guardado en dataset_eb_tess_labeled_batch_0591.parquet (50000 filas)\n",
      "📝 Procesando batch #593...\n",
      "✅ Batch #593 guardado en dataset_eb_tess_labeled_batch_0592.parquet (50000 filas)\n",
      "📝 Procesando batch #594...\n",
      "✅ Batch #594 guardado en dataset_eb_tess_labeled_batch_0593.parquet (50000 filas)\n",
      "📝 Procesando batch #595...\n",
      "✅ Batch #595 guardado en dataset_eb_tess_labeled_batch_0594.parquet (50000 filas)\n",
      "📝 Procesando batch #596...\n",
      "✅ Batch #596 guardado en dataset_eb_tess_labeled_batch_0595.parquet (50000 filas)\n",
      "📝 Procesando batch #597...\n",
      "✅ Batch #597 guardado en dataset_eb_tess_labeled_batch_0596.parquet (50000 filas)\n",
      "📝 Procesando batch #598...\n",
      "✅ Batch #598 guardado en dataset_eb_tess_labeled_batch_0597.parquet (50000 filas)\n",
      "📝 Procesando batch #599...\n",
      "✅ Batch #599 guardado en dataset_eb_tess_labeled_batch_0598.parquet (50000 filas)\n",
      "📝 Procesando batch #600...\n",
      "✅ Batch #600 guardado en dataset_eb_tess_labeled_batch_0599.parquet (50000 filas)\n",
      "📝 Procesando batch #601...\n",
      "✅ Batch #601 guardado en dataset_eb_tess_labeled_batch_0600.parquet (50000 filas)\n",
      "📝 Procesando batch #602...\n",
      "✅ Batch #602 guardado en dataset_eb_tess_labeled_batch_0601.parquet (50000 filas)\n",
      "📝 Procesando batch #603...\n",
      "✅ Batch #603 guardado en dataset_eb_tess_labeled_batch_0602.parquet (50000 filas)\n",
      "📝 Procesando batch #604...\n",
      "✅ Batch #604 guardado en dataset_eb_tess_labeled_batch_0603.parquet (50000 filas)\n",
      "📝 Procesando batch #605...\n",
      "✅ Batch #605 guardado en dataset_eb_tess_labeled_batch_0604.parquet (50000 filas)\n",
      "📝 Procesando batch #606...\n",
      "✅ Batch #606 guardado en dataset_eb_tess_labeled_batch_0605.parquet (50000 filas)\n",
      "📝 Procesando batch #607...\n",
      "✅ Batch #607 guardado en dataset_eb_tess_labeled_batch_0606.parquet (50000 filas)\n",
      "📝 Procesando batch #608...\n",
      "✅ Batch #608 guardado en dataset_eb_tess_labeled_batch_0607.parquet (50000 filas)\n",
      "📝 Procesando batch #609...\n",
      "✅ Batch #609 guardado en dataset_eb_tess_labeled_batch_0608.parquet (50000 filas)\n",
      "📝 Procesando batch #610...\n",
      "✅ Batch #610 guardado en dataset_eb_tess_labeled_batch_0609.parquet (48576 filas)\n",
      "📝 Procesando batch #611...\n",
      "✅ Batch #611 guardado en dataset_eb_tess_labeled_batch_0610.parquet (50000 filas)\n",
      "📝 Procesando batch #612...\n",
      "✅ Batch #612 guardado en dataset_eb_tess_labeled_batch_0611.parquet (50000 filas)\n",
      "📝 Procesando batch #613...\n",
      "✅ Batch #613 guardado en dataset_eb_tess_labeled_batch_0612.parquet (50000 filas)\n",
      "📝 Procesando batch #614...\n",
      "✅ Batch #614 guardado en dataset_eb_tess_labeled_batch_0613.parquet (50000 filas)\n",
      "📝 Procesando batch #615...\n",
      "✅ Batch #615 guardado en dataset_eb_tess_labeled_batch_0614.parquet (50000 filas)\n",
      "📝 Procesando batch #616...\n",
      "✅ Batch #616 guardado en dataset_eb_tess_labeled_batch_0615.parquet (50000 filas)\n",
      "📝 Procesando batch #617...\n",
      "✅ Batch #617 guardado en dataset_eb_tess_labeled_batch_0616.parquet (50000 filas)\n",
      "📝 Procesando batch #618...\n",
      "✅ Batch #618 guardado en dataset_eb_tess_labeled_batch_0617.parquet (50000 filas)\n",
      "📝 Procesando batch #619...\n",
      "✅ Batch #619 guardado en dataset_eb_tess_labeled_batch_0618.parquet (50000 filas)\n",
      "📝 Procesando batch #620...\n",
      "✅ Batch #620 guardado en dataset_eb_tess_labeled_batch_0619.parquet (50000 filas)\n",
      "📝 Procesando batch #621...\n",
      "✅ Batch #621 guardado en dataset_eb_tess_labeled_batch_0620.parquet (50000 filas)\n",
      "📝 Procesando batch #622...\n",
      "✅ Batch #622 guardado en dataset_eb_tess_labeled_batch_0621.parquet (50000 filas)\n",
      "📝 Procesando batch #623...\n",
      "✅ Batch #623 guardado en dataset_eb_tess_labeled_batch_0622.parquet (50000 filas)\n",
      "📝 Procesando batch #624...\n",
      "✅ Batch #624 guardado en dataset_eb_tess_labeled_batch_0623.parquet (50000 filas)\n",
      "📝 Procesando batch #625...\n",
      "✅ Batch #625 guardado en dataset_eb_tess_labeled_batch_0624.parquet (50000 filas)\n",
      "📝 Procesando batch #626...\n",
      "✅ Batch #626 guardado en dataset_eb_tess_labeled_batch_0625.parquet (50000 filas)\n",
      "📝 Procesando batch #627...\n",
      "✅ Batch #627 guardado en dataset_eb_tess_labeled_batch_0626.parquet (50000 filas)\n",
      "📝 Procesando batch #628...\n",
      "✅ Batch #628 guardado en dataset_eb_tess_labeled_batch_0627.parquet (50000 filas)\n",
      "📝 Procesando batch #629...\n",
      "✅ Batch #629 guardado en dataset_eb_tess_labeled_batch_0628.parquet (50000 filas)\n",
      "📝 Procesando batch #630...\n",
      "✅ Batch #630 guardado en dataset_eb_tess_labeled_batch_0629.parquet (50000 filas)\n",
      "📝 Procesando batch #631...\n",
      "✅ Batch #631 guardado en dataset_eb_tess_labeled_batch_0630.parquet (48576 filas)\n",
      "📝 Procesando batch #632...\n",
      "✅ Batch #632 guardado en dataset_eb_tess_labeled_batch_0631.parquet (50000 filas)\n",
      "📝 Procesando batch #633...\n",
      "✅ Batch #633 guardado en dataset_eb_tess_labeled_batch_0632.parquet (50000 filas)\n",
      "📝 Procesando batch #634...\n",
      "✅ Batch #634 guardado en dataset_eb_tess_labeled_batch_0633.parquet (50000 filas)\n",
      "📝 Procesando batch #635...\n",
      "✅ Batch #635 guardado en dataset_eb_tess_labeled_batch_0634.parquet (50000 filas)\n",
      "📝 Procesando batch #636...\n",
      "✅ Batch #636 guardado en dataset_eb_tess_labeled_batch_0635.parquet (50000 filas)\n",
      "📝 Procesando batch #637...\n",
      "✅ Batch #637 guardado en dataset_eb_tess_labeled_batch_0636.parquet (50000 filas)\n",
      "📝 Procesando batch #638...\n",
      "✅ Batch #638 guardado en dataset_eb_tess_labeled_batch_0637.parquet (50000 filas)\n",
      "📝 Procesando batch #639...\n",
      "✅ Batch #639 guardado en dataset_eb_tess_labeled_batch_0638.parquet (50000 filas)\n",
      "📝 Procesando batch #640...\n",
      "✅ Batch #640 guardado en dataset_eb_tess_labeled_batch_0639.parquet (50000 filas)\n",
      "📝 Procesando batch #641...\n",
      "✅ Batch #641 guardado en dataset_eb_tess_labeled_batch_0640.parquet (50000 filas)\n",
      "📝 Procesando batch #642...\n",
      "✅ Batch #642 guardado en dataset_eb_tess_labeled_batch_0641.parquet (50000 filas)\n",
      "📝 Procesando batch #643...\n",
      "✅ Batch #643 guardado en dataset_eb_tess_labeled_batch_0642.parquet (50000 filas)\n",
      "📝 Procesando batch #644...\n",
      "✅ Batch #644 guardado en dataset_eb_tess_labeled_batch_0643.parquet (50000 filas)\n",
      "📝 Procesando batch #645...\n",
      "✅ Batch #645 guardado en dataset_eb_tess_labeled_batch_0644.parquet (50000 filas)\n",
      "📝 Procesando batch #646...\n",
      "✅ Batch #646 guardado en dataset_eb_tess_labeled_batch_0645.parquet (50000 filas)\n",
      "📝 Procesando batch #647...\n",
      "✅ Batch #647 guardado en dataset_eb_tess_labeled_batch_0646.parquet (50000 filas)\n",
      "📝 Procesando batch #648...\n",
      "✅ Batch #648 guardado en dataset_eb_tess_labeled_batch_0647.parquet (50000 filas)\n",
      "📝 Procesando batch #649...\n",
      "✅ Batch #649 guardado en dataset_eb_tess_labeled_batch_0648.parquet (28765 filas)\n",
      "📝 Procesando batch #650...\n",
      "✅ Batch #650 guardado en dataset_eb_tess_labeled_batch_0649.parquet (50000 filas)\n",
      "📝 Procesando batch #651...\n",
      "✅ Batch #651 guardado en dataset_eb_tess_labeled_batch_0650.parquet (50000 filas)\n",
      "📝 Procesando batch #652...\n",
      "✅ Batch #652 guardado en dataset_eb_tess_labeled_batch_0651.parquet (50000 filas)\n",
      "📝 Procesando batch #653...\n",
      "✅ Batch #653 guardado en dataset_eb_tess_labeled_batch_0652.parquet (50000 filas)\n",
      "📝 Procesando batch #654...\n",
      "✅ Batch #654 guardado en dataset_eb_tess_labeled_batch_0653.parquet (50000 filas)\n",
      "📝 Procesando batch #655...\n",
      "✅ Batch #655 guardado en dataset_eb_tess_labeled_batch_0654.parquet (50000 filas)\n",
      "📝 Procesando batch #656...\n",
      "✅ Batch #656 guardado en dataset_eb_tess_labeled_batch_0655.parquet (50000 filas)\n",
      "📝 Procesando batch #657...\n",
      "✅ Batch #657 guardado en dataset_eb_tess_labeled_batch_0656.parquet (50000 filas)\n",
      "📝 Procesando batch #658...\n",
      "✅ Batch #658 guardado en dataset_eb_tess_labeled_batch_0657.parquet (50000 filas)\n",
      "📝 Procesando batch #659...\n",
      "✅ Batch #659 guardado en dataset_eb_tess_labeled_batch_0658.parquet (50000 filas)\n",
      "📝 Procesando batch #660...\n",
      "✅ Batch #660 guardado en dataset_eb_tess_labeled_batch_0659.parquet (50000 filas)\n",
      "📝 Procesando batch #661...\n",
      "✅ Batch #661 guardado en dataset_eb_tess_labeled_batch_0660.parquet (50000 filas)\n",
      "📝 Procesando batch #662...\n",
      "✅ Batch #662 guardado en dataset_eb_tess_labeled_batch_0661.parquet (50000 filas)\n",
      "📝 Procesando batch #663...\n",
      "✅ Batch #663 guardado en dataset_eb_tess_labeled_batch_0662.parquet (50000 filas)\n",
      "📝 Procesando batch #664...\n",
      "✅ Batch #664 guardado en dataset_eb_tess_labeled_batch_0663.parquet (50000 filas)\n",
      "📝 Procesando batch #665...\n",
      "✅ Batch #665 guardado en dataset_eb_tess_labeled_batch_0664.parquet (50000 filas)\n",
      "📝 Procesando batch #666...\n",
      "✅ Batch #666 guardado en dataset_eb_tess_labeled_batch_0665.parquet (50000 filas)\n",
      "📝 Procesando batch #667...\n",
      "✅ Batch #667 guardado en dataset_eb_tess_labeled_batch_0666.parquet (50000 filas)\n",
      "📝 Procesando batch #668...\n",
      "✅ Batch #668 guardado en dataset_eb_tess_labeled_batch_0667.parquet (50000 filas)\n",
      "📝 Procesando batch #669...\n",
      "✅ Batch #669 guardado en dataset_eb_tess_labeled_batch_0668.parquet (50000 filas)\n",
      "📝 Procesando batch #670...\n",
      "✅ Batch #670 guardado en dataset_eb_tess_labeled_batch_0669.parquet (48576 filas)\n",
      "📝 Procesando batch #671...\n",
      "✅ Batch #671 guardado en dataset_eb_tess_labeled_batch_0670.parquet (50000 filas)\n",
      "📝 Procesando batch #672...\n",
      "✅ Batch #672 guardado en dataset_eb_tess_labeled_batch_0671.parquet (50000 filas)\n",
      "📝 Procesando batch #673...\n",
      "✅ Batch #673 guardado en dataset_eb_tess_labeled_batch_0672.parquet (50000 filas)\n",
      "📝 Procesando batch #674...\n",
      "✅ Batch #674 guardado en dataset_eb_tess_labeled_batch_0673.parquet (50000 filas)\n",
      "📝 Procesando batch #675...\n",
      "✅ Batch #675 guardado en dataset_eb_tess_labeled_batch_0674.parquet (50000 filas)\n",
      "📝 Procesando batch #676...\n",
      "✅ Batch #676 guardado en dataset_eb_tess_labeled_batch_0675.parquet (50000 filas)\n",
      "📝 Procesando batch #677...\n",
      "✅ Batch #677 guardado en dataset_eb_tess_labeled_batch_0676.parquet (50000 filas)\n",
      "📝 Procesando batch #678...\n",
      "✅ Batch #678 guardado en dataset_eb_tess_labeled_batch_0677.parquet (50000 filas)\n",
      "📝 Procesando batch #679...\n",
      "✅ Batch #679 guardado en dataset_eb_tess_labeled_batch_0678.parquet (50000 filas)\n",
      "📝 Procesando batch #680...\n",
      "✅ Batch #680 guardado en dataset_eb_tess_labeled_batch_0679.parquet (50000 filas)\n",
      "📝 Procesando batch #681...\n",
      "✅ Batch #681 guardado en dataset_eb_tess_labeled_batch_0680.parquet (50000 filas)\n",
      "📝 Procesando batch #682...\n",
      "✅ Batch #682 guardado en dataset_eb_tess_labeled_batch_0681.parquet (50000 filas)\n",
      "📝 Procesando batch #683...\n",
      "✅ Batch #683 guardado en dataset_eb_tess_labeled_batch_0682.parquet (50000 filas)\n",
      "📝 Procesando batch #684...\n",
      "✅ Batch #684 guardado en dataset_eb_tess_labeled_batch_0683.parquet (50000 filas)\n",
      "📝 Procesando batch #685...\n",
      "✅ Batch #685 guardado en dataset_eb_tess_labeled_batch_0684.parquet (50000 filas)\n",
      "📝 Procesando batch #686...\n",
      "✅ Batch #686 guardado en dataset_eb_tess_labeled_batch_0685.parquet (50000 filas)\n",
      "📝 Procesando batch #687...\n",
      "✅ Batch #687 guardado en dataset_eb_tess_labeled_batch_0686.parquet (50000 filas)\n",
      "📝 Procesando batch #688...\n",
      "✅ Batch #688 guardado en dataset_eb_tess_labeled_batch_0687.parquet (50000 filas)\n",
      "📝 Procesando batch #689...\n",
      "✅ Batch #689 guardado en dataset_eb_tess_labeled_batch_0688.parquet (50000 filas)\n",
      "📝 Procesando batch #690...\n",
      "✅ Batch #690 guardado en dataset_eb_tess_labeled_batch_0689.parquet (50000 filas)\n",
      "📝 Procesando batch #691...\n",
      "✅ Batch #691 guardado en dataset_eb_tess_labeled_batch_0690.parquet (48576 filas)\n",
      "📝 Procesando batch #692...\n",
      "✅ Batch #692 guardado en dataset_eb_tess_labeled_batch_0691.parquet (50000 filas)\n",
      "📝 Procesando batch #693...\n",
      "✅ Batch #693 guardado en dataset_eb_tess_labeled_batch_0692.parquet (50000 filas)\n",
      "📝 Procesando batch #694...\n",
      "✅ Batch #694 guardado en dataset_eb_tess_labeled_batch_0693.parquet (50000 filas)\n",
      "📝 Procesando batch #695...\n",
      "✅ Batch #695 guardado en dataset_eb_tess_labeled_batch_0694.parquet (50000 filas)\n",
      "📝 Procesando batch #696...\n",
      "✅ Batch #696 guardado en dataset_eb_tess_labeled_batch_0695.parquet (50000 filas)\n",
      "📝 Procesando batch #697...\n",
      "✅ Batch #697 guardado en dataset_eb_tess_labeled_batch_0696.parquet (50000 filas)\n",
      "📝 Procesando batch #698...\n",
      "✅ Batch #698 guardado en dataset_eb_tess_labeled_batch_0697.parquet (50000 filas)\n",
      "📝 Procesando batch #699...\n",
      "✅ Batch #699 guardado en dataset_eb_tess_labeled_batch_0698.parquet (50000 filas)\n",
      "📝 Procesando batch #700...\n",
      "✅ Batch #700 guardado en dataset_eb_tess_labeled_batch_0699.parquet (50000 filas)\n",
      "📝 Procesando batch #701...\n",
      "✅ Batch #701 guardado en dataset_eb_tess_labeled_batch_0700.parquet (50000 filas)\n",
      "📝 Procesando batch #702...\n",
      "✅ Batch #702 guardado en dataset_eb_tess_labeled_batch_0701.parquet (50000 filas)\n",
      "📝 Procesando batch #703...\n",
      "✅ Batch #703 guardado en dataset_eb_tess_labeled_batch_0702.parquet (50000 filas)\n",
      "📝 Procesando batch #704...\n",
      "✅ Batch #704 guardado en dataset_eb_tess_labeled_batch_0703.parquet (50000 filas)\n",
      "📝 Procesando batch #705...\n",
      "✅ Batch #705 guardado en dataset_eb_tess_labeled_batch_0704.parquet (50000 filas)\n",
      "📝 Procesando batch #706...\n",
      "✅ Batch #706 guardado en dataset_eb_tess_labeled_batch_0705.parquet (50000 filas)\n",
      "📝 Procesando batch #707...\n",
      "✅ Batch #707 guardado en dataset_eb_tess_labeled_batch_0706.parquet (50000 filas)\n",
      "📝 Procesando batch #708...\n",
      "✅ Batch #708 guardado en dataset_eb_tess_labeled_batch_0707.parquet (50000 filas)\n",
      "📝 Procesando batch #709...\n",
      "✅ Batch #709 guardado en dataset_eb_tess_labeled_batch_0708.parquet (50000 filas)\n",
      "📝 Procesando batch #710...\n",
      "✅ Batch #710 guardado en dataset_eb_tess_labeled_batch_0709.parquet (50000 filas)\n",
      "📝 Procesando batch #711...\n",
      "✅ Batch #711 guardado en dataset_eb_tess_labeled_batch_0710.parquet (50000 filas)\n",
      "📝 Procesando batch #712...\n",
      "✅ Batch #712 guardado en dataset_eb_tess_labeled_batch_0711.parquet (48576 filas)\n",
      "📝 Procesando batch #713...\n",
      "✅ Batch #713 guardado en dataset_eb_tess_labeled_batch_0712.parquet (50000 filas)\n",
      "📝 Procesando batch #714...\n",
      "✅ Batch #714 guardado en dataset_eb_tess_labeled_batch_0713.parquet (50000 filas)\n",
      "📝 Procesando batch #715...\n",
      "✅ Batch #715 guardado en dataset_eb_tess_labeled_batch_0714.parquet (50000 filas)\n",
      "📝 Procesando batch #716...\n",
      "✅ Batch #716 guardado en dataset_eb_tess_labeled_batch_0715.parquet (50000 filas)\n",
      "📝 Procesando batch #717...\n",
      "✅ Batch #717 guardado en dataset_eb_tess_labeled_batch_0716.parquet (50000 filas)\n",
      "📝 Procesando batch #718...\n",
      "✅ Batch #718 guardado en dataset_eb_tess_labeled_batch_0717.parquet (50000 filas)\n",
      "📝 Procesando batch #719...\n",
      "✅ Batch #719 guardado en dataset_eb_tess_labeled_batch_0718.parquet (50000 filas)\n",
      "📝 Procesando batch #720...\n",
      "✅ Batch #720 guardado en dataset_eb_tess_labeled_batch_0719.parquet (50000 filas)\n",
      "📝 Procesando batch #721...\n",
      "✅ Batch #721 guardado en dataset_eb_tess_labeled_batch_0720.parquet (50000 filas)\n",
      "📝 Procesando batch #722...\n",
      "✅ Batch #722 guardado en dataset_eb_tess_labeled_batch_0721.parquet (50000 filas)\n",
      "📝 Procesando batch #723...\n",
      "✅ Batch #723 guardado en dataset_eb_tess_labeled_batch_0722.parquet (50000 filas)\n",
      "📝 Procesando batch #724...\n",
      "✅ Batch #724 guardado en dataset_eb_tess_labeled_batch_0723.parquet (50000 filas)\n",
      "📝 Procesando batch #725...\n",
      "✅ Batch #725 guardado en dataset_eb_tess_labeled_batch_0724.parquet (50000 filas)\n",
      "📝 Procesando batch #726...\n",
      "✅ Batch #726 guardado en dataset_eb_tess_labeled_batch_0725.parquet (50000 filas)\n",
      "📝 Procesando batch #727...\n",
      "✅ Batch #727 guardado en dataset_eb_tess_labeled_batch_0726.parquet (50000 filas)\n",
      "📝 Procesando batch #728...\n",
      "✅ Batch #728 guardado en dataset_eb_tess_labeled_batch_0727.parquet (50000 filas)\n",
      "📝 Procesando batch #729...\n",
      "✅ Batch #729 guardado en dataset_eb_tess_labeled_batch_0728.parquet (50000 filas)\n",
      "📝 Procesando batch #730...\n",
      "✅ Batch #730 guardado en dataset_eb_tess_labeled_batch_0729.parquet (50000 filas)\n",
      "📝 Procesando batch #731...\n",
      "✅ Batch #731 guardado en dataset_eb_tess_labeled_batch_0730.parquet (50000 filas)\n",
      "📝 Procesando batch #732...\n",
      "✅ Batch #732 guardado en dataset_eb_tess_labeled_batch_0731.parquet (50000 filas)\n",
      "📝 Procesando batch #733...\n",
      "✅ Batch #733 guardado en dataset_eb_tess_labeled_batch_0732.parquet (48576 filas)\n",
      "📝 Procesando batch #734...\n",
      "✅ Batch #734 guardado en dataset_eb_tess_labeled_batch_0733.parquet (50000 filas)\n",
      "📝 Procesando batch #735...\n",
      "✅ Batch #735 guardado en dataset_eb_tess_labeled_batch_0734.parquet (50000 filas)\n",
      "📝 Procesando batch #736...\n",
      "✅ Batch #736 guardado en dataset_eb_tess_labeled_batch_0735.parquet (50000 filas)\n",
      "📝 Procesando batch #737...\n",
      "✅ Batch #737 guardado en dataset_eb_tess_labeled_batch_0736.parquet (50000 filas)\n",
      "📝 Procesando batch #738...\n",
      "✅ Batch #738 guardado en dataset_eb_tess_labeled_batch_0737.parquet (50000 filas)\n",
      "📝 Procesando batch #739...\n",
      "✅ Batch #739 guardado en dataset_eb_tess_labeled_batch_0738.parquet (50000 filas)\n",
      "📝 Procesando batch #740...\n",
      "✅ Batch #740 guardado en dataset_eb_tess_labeled_batch_0739.parquet (50000 filas)\n",
      "📝 Procesando batch #741...\n",
      "✅ Batch #741 guardado en dataset_eb_tess_labeled_batch_0740.parquet (50000 filas)\n",
      "📝 Procesando batch #742...\n",
      "✅ Batch #742 guardado en dataset_eb_tess_labeled_batch_0741.parquet (50000 filas)\n",
      "📝 Procesando batch #743...\n",
      "✅ Batch #743 guardado en dataset_eb_tess_labeled_batch_0742.parquet (50000 filas)\n",
      "📝 Procesando batch #744...\n",
      "✅ Batch #744 guardado en dataset_eb_tess_labeled_batch_0743.parquet (50000 filas)\n",
      "📝 Procesando batch #745...\n",
      "✅ Batch #745 guardado en dataset_eb_tess_labeled_batch_0744.parquet (50000 filas)\n",
      "📝 Procesando batch #746...\n",
      "✅ Batch #746 guardado en dataset_eb_tess_labeled_batch_0745.parquet (50000 filas)\n",
      "📝 Procesando batch #747...\n",
      "✅ Batch #747 guardado en dataset_eb_tess_labeled_batch_0746.parquet (50000 filas)\n",
      "📝 Procesando batch #748...\n",
      "✅ Batch #748 guardado en dataset_eb_tess_labeled_batch_0747.parquet (50000 filas)\n",
      "📝 Procesando batch #749...\n",
      "✅ Batch #749 guardado en dataset_eb_tess_labeled_batch_0748.parquet (50000 filas)\n",
      "📝 Procesando batch #750...\n",
      "✅ Batch #750 guardado en dataset_eb_tess_labeled_batch_0749.parquet (50000 filas)\n",
      "📝 Procesando batch #751...\n",
      "✅ Batch #751 guardado en dataset_eb_tess_labeled_batch_0750.parquet (50000 filas)\n",
      "📝 Procesando batch #752...\n",
      "✅ Batch #752 guardado en dataset_eb_tess_labeled_batch_0751.parquet (50000 filas)\n",
      "📝 Procesando batch #753...\n",
      "✅ Batch #753 guardado en dataset_eb_tess_labeled_batch_0752.parquet (50000 filas)\n",
      "📝 Procesando batch #754...\n",
      "✅ Batch #754 guardado en dataset_eb_tess_labeled_batch_0753.parquet (48576 filas)\n",
      "📝 Procesando batch #755...\n",
      "✅ Batch #755 guardado en dataset_eb_tess_labeled_batch_0754.parquet (50000 filas)\n",
      "📝 Procesando batch #756...\n",
      "✅ Batch #756 guardado en dataset_eb_tess_labeled_batch_0755.parquet (50000 filas)\n",
      "📝 Procesando batch #757...\n",
      "✅ Batch #757 guardado en dataset_eb_tess_labeled_batch_0756.parquet (50000 filas)\n",
      "📝 Procesando batch #758...\n",
      "✅ Batch #758 guardado en dataset_eb_tess_labeled_batch_0757.parquet (50000 filas)\n",
      "📝 Procesando batch #759...\n",
      "✅ Batch #759 guardado en dataset_eb_tess_labeled_batch_0758.parquet (50000 filas)\n",
      "📝 Procesando batch #760...\n",
      "✅ Batch #760 guardado en dataset_eb_tess_labeled_batch_0759.parquet (50000 filas)\n",
      "📝 Procesando batch #761...\n",
      "✅ Batch #761 guardado en dataset_eb_tess_labeled_batch_0760.parquet (50000 filas)\n",
      "📝 Procesando batch #762...\n",
      "✅ Batch #762 guardado en dataset_eb_tess_labeled_batch_0761.parquet (50000 filas)\n",
      "📝 Procesando batch #763...\n",
      "✅ Batch #763 guardado en dataset_eb_tess_labeled_batch_0762.parquet (50000 filas)\n",
      "📝 Procesando batch #764...\n",
      "✅ Batch #764 guardado en dataset_eb_tess_labeled_batch_0763.parquet (50000 filas)\n",
      "📝 Procesando batch #765...\n",
      "✅ Batch #765 guardado en dataset_eb_tess_labeled_batch_0764.parquet (50000 filas)\n",
      "📝 Procesando batch #766...\n",
      "✅ Batch #766 guardado en dataset_eb_tess_labeled_batch_0765.parquet (50000 filas)\n",
      "📝 Procesando batch #767...\n",
      "✅ Batch #767 guardado en dataset_eb_tess_labeled_batch_0766.parquet (50000 filas)\n",
      "📝 Procesando batch #768...\n",
      "✅ Batch #768 guardado en dataset_eb_tess_labeled_batch_0767.parquet (50000 filas)\n",
      "📝 Procesando batch #769...\n",
      "✅ Batch #769 guardado en dataset_eb_tess_labeled_batch_0768.parquet (50000 filas)\n",
      "📝 Procesando batch #770...\n",
      "✅ Batch #770 guardado en dataset_eb_tess_labeled_batch_0769.parquet (50000 filas)\n",
      "📝 Procesando batch #771...\n",
      "✅ Batch #771 guardado en dataset_eb_tess_labeled_batch_0770.parquet (50000 filas)\n",
      "📝 Procesando batch #772...\n",
      "✅ Batch #772 guardado en dataset_eb_tess_labeled_batch_0771.parquet (50000 filas)\n",
      "📝 Procesando batch #773...\n",
      "✅ Batch #773 guardado en dataset_eb_tess_labeled_batch_0772.parquet (50000 filas)\n",
      "📝 Procesando batch #774...\n",
      "✅ Batch #774 guardado en dataset_eb_tess_labeled_batch_0773.parquet (50000 filas)\n",
      "📝 Procesando batch #775...\n",
      "✅ Batch #775 guardado en dataset_eb_tess_labeled_batch_0774.parquet (48576 filas)\n",
      "📝 Procesando batch #776...\n",
      "✅ Batch #776 guardado en dataset_eb_tess_labeled_batch_0775.parquet (50000 filas)\n",
      "📝 Procesando batch #777...\n",
      "✅ Batch #777 guardado en dataset_eb_tess_labeled_batch_0776.parquet (50000 filas)\n",
      "📝 Procesando batch #778...\n",
      "✅ Batch #778 guardado en dataset_eb_tess_labeled_batch_0777.parquet (50000 filas)\n",
      "📝 Procesando batch #779...\n",
      "✅ Batch #779 guardado en dataset_eb_tess_labeled_batch_0778.parquet (50000 filas)\n",
      "📝 Procesando batch #780...\n",
      "✅ Batch #780 guardado en dataset_eb_tess_labeled_batch_0779.parquet (50000 filas)\n",
      "📝 Procesando batch #781...\n",
      "✅ Batch #781 guardado en dataset_eb_tess_labeled_batch_0780.parquet (50000 filas)\n",
      "📝 Procesando batch #782...\n",
      "✅ Batch #782 guardado en dataset_eb_tess_labeled_batch_0781.parquet (50000 filas)\n",
      "📝 Procesando batch #783...\n",
      "✅ Batch #783 guardado en dataset_eb_tess_labeled_batch_0782.parquet (50000 filas)\n",
      "📝 Procesando batch #784...\n",
      "✅ Batch #784 guardado en dataset_eb_tess_labeled_batch_0783.parquet (50000 filas)\n",
      "📝 Procesando batch #785...\n",
      "✅ Batch #785 guardado en dataset_eb_tess_labeled_batch_0784.parquet (50000 filas)\n",
      "📝 Procesando batch #786...\n",
      "✅ Batch #786 guardado en dataset_eb_tess_labeled_batch_0785.parquet (50000 filas)\n",
      "📝 Procesando batch #787...\n",
      "✅ Batch #787 guardado en dataset_eb_tess_labeled_batch_0786.parquet (50000 filas)\n",
      "📝 Procesando batch #788...\n",
      "✅ Batch #788 guardado en dataset_eb_tess_labeled_batch_0787.parquet (50000 filas)\n",
      "📝 Procesando batch #789...\n",
      "✅ Batch #789 guardado en dataset_eb_tess_labeled_batch_0788.parquet (50000 filas)\n",
      "📝 Procesando batch #790...\n",
      "✅ Batch #790 guardado en dataset_eb_tess_labeled_batch_0789.parquet (50000 filas)\n",
      "📝 Procesando batch #791...\n",
      "✅ Batch #791 guardado en dataset_eb_tess_labeled_batch_0790.parquet (50000 filas)\n",
      "📝 Procesando batch #792...\n",
      "✅ Batch #792 guardado en dataset_eb_tess_labeled_batch_0791.parquet (50000 filas)\n",
      "📝 Procesando batch #793...\n",
      "✅ Batch #793 guardado en dataset_eb_tess_labeled_batch_0792.parquet (50000 filas)\n",
      "📝 Procesando batch #794...\n",
      "✅ Batch #794 guardado en dataset_eb_tess_labeled_batch_0793.parquet (50000 filas)\n",
      "📝 Procesando batch #795...\n",
      "✅ Batch #795 guardado en dataset_eb_tess_labeled_batch_0794.parquet (50000 filas)\n",
      "📝 Procesando batch #796...\n",
      "✅ Batch #796 guardado en dataset_eb_tess_labeled_batch_0795.parquet (48576 filas)\n",
      "📝 Procesando batch #797...\n",
      "✅ Batch #797 guardado en dataset_eb_tess_labeled_batch_0796.parquet (50000 filas)\n",
      "📝 Procesando batch #798...\n",
      "✅ Batch #798 guardado en dataset_eb_tess_labeled_batch_0797.parquet (50000 filas)\n",
      "📝 Procesando batch #799...\n",
      "✅ Batch #799 guardado en dataset_eb_tess_labeled_batch_0798.parquet (50000 filas)\n",
      "📝 Procesando batch #800...\n",
      "✅ Batch #800 guardado en dataset_eb_tess_labeled_batch_0799.parquet (28289 filas)\n",
      "📝 Procesando batch #801...\n",
      "✅ Batch #801 guardado en dataset_eb_tess_labeled_batch_0800.parquet (50000 filas)\n",
      "📝 Procesando batch #802...\n",
      "✅ Batch #802 guardado en dataset_eb_tess_labeled_batch_0801.parquet (50000 filas)\n",
      "📝 Procesando batch #803...\n",
      "✅ Batch #803 guardado en dataset_eb_tess_labeled_batch_0802.parquet (50000 filas)\n",
      "📝 Procesando batch #804...\n",
      "✅ Batch #804 guardado en dataset_eb_tess_labeled_batch_0803.parquet (50000 filas)\n",
      "📝 Procesando batch #805...\n",
      "✅ Batch #805 guardado en dataset_eb_tess_labeled_batch_0804.parquet (50000 filas)\n",
      "📝 Procesando batch #806...\n",
      "✅ Batch #806 guardado en dataset_eb_tess_labeled_batch_0805.parquet (50000 filas)\n",
      "📝 Procesando batch #807...\n",
      "✅ Batch #807 guardado en dataset_eb_tess_labeled_batch_0806.parquet (50000 filas)\n",
      "📝 Procesando batch #808...\n",
      "✅ Batch #808 guardado en dataset_eb_tess_labeled_batch_0807.parquet (50000 filas)\n",
      "📝 Procesando batch #809...\n",
      "✅ Batch #809 guardado en dataset_eb_tess_labeled_batch_0808.parquet (50000 filas)\n",
      "📝 Procesando batch #810...\n",
      "✅ Batch #810 guardado en dataset_eb_tess_labeled_batch_0809.parquet (50000 filas)\n",
      "📝 Procesando batch #811...\n",
      "✅ Batch #811 guardado en dataset_eb_tess_labeled_batch_0810.parquet (50000 filas)\n",
      "📝 Procesando batch #812...\n",
      "✅ Batch #812 guardado en dataset_eb_tess_labeled_batch_0811.parquet (50000 filas)\n",
      "📝 Procesando batch #813...\n",
      "✅ Batch #813 guardado en dataset_eb_tess_labeled_batch_0812.parquet (50000 filas)\n",
      "📝 Procesando batch #814...\n",
      "✅ Batch #814 guardado en dataset_eb_tess_labeled_batch_0813.parquet (50000 filas)\n",
      "📝 Procesando batch #815...\n",
      "✅ Batch #815 guardado en dataset_eb_tess_labeled_batch_0814.parquet (50000 filas)\n",
      "📝 Procesando batch #816...\n",
      "✅ Batch #816 guardado en dataset_eb_tess_labeled_batch_0815.parquet (50000 filas)\n",
      "📝 Procesando batch #817...\n",
      "✅ Batch #817 guardado en dataset_eb_tess_labeled_batch_0816.parquet (50000 filas)\n",
      "📝 Procesando batch #818...\n",
      "✅ Batch #818 guardado en dataset_eb_tess_labeled_batch_0817.parquet (50000 filas)\n",
      "📝 Procesando batch #819...\n",
      "✅ Batch #819 guardado en dataset_eb_tess_labeled_batch_0818.parquet (50000 filas)\n",
      "📝 Procesando batch #820...\n",
      "✅ Batch #820 guardado en dataset_eb_tess_labeled_batch_0819.parquet (50000 filas)\n",
      "📝 Procesando batch #821...\n",
      "✅ Batch #821 guardado en dataset_eb_tess_labeled_batch_0820.parquet (48576 filas)\n",
      "📝 Procesando batch #822...\n",
      "✅ Batch #822 guardado en dataset_eb_tess_labeled_batch_0821.parquet (50000 filas)\n",
      "📝 Procesando batch #823...\n",
      "✅ Batch #823 guardado en dataset_eb_tess_labeled_batch_0822.parquet (50000 filas)\n",
      "📝 Procesando batch #824...\n",
      "✅ Batch #824 guardado en dataset_eb_tess_labeled_batch_0823.parquet (50000 filas)\n",
      "📝 Procesando batch #825...\n",
      "✅ Batch #825 guardado en dataset_eb_tess_labeled_batch_0824.parquet (50000 filas)\n",
      "📝 Procesando batch #826...\n",
      "✅ Batch #826 guardado en dataset_eb_tess_labeled_batch_0825.parquet (50000 filas)\n",
      "📝 Procesando batch #827...\n",
      "✅ Batch #827 guardado en dataset_eb_tess_labeled_batch_0826.parquet (50000 filas)\n",
      "📝 Procesando batch #828...\n",
      "✅ Batch #828 guardado en dataset_eb_tess_labeled_batch_0827.parquet (50000 filas)\n",
      "📝 Procesando batch #829...\n",
      "✅ Batch #829 guardado en dataset_eb_tess_labeled_batch_0828.parquet (50000 filas)\n",
      "📝 Procesando batch #830...\n",
      "✅ Batch #830 guardado en dataset_eb_tess_labeled_batch_0829.parquet (50000 filas)\n",
      "📝 Procesando batch #831...\n",
      "✅ Batch #831 guardado en dataset_eb_tess_labeled_batch_0830.parquet (50000 filas)\n",
      "📝 Procesando batch #832...\n",
      "✅ Batch #832 guardado en dataset_eb_tess_labeled_batch_0831.parquet (50000 filas)\n",
      "📝 Procesando batch #833...\n",
      "✅ Batch #833 guardado en dataset_eb_tess_labeled_batch_0832.parquet (50000 filas)\n",
      "📝 Procesando batch #834...\n",
      "✅ Batch #834 guardado en dataset_eb_tess_labeled_batch_0833.parquet (50000 filas)\n",
      "📝 Procesando batch #835...\n",
      "✅ Batch #835 guardado en dataset_eb_tess_labeled_batch_0834.parquet (50000 filas)\n",
      "📝 Procesando batch #836...\n",
      "✅ Batch #836 guardado en dataset_eb_tess_labeled_batch_0835.parquet (50000 filas)\n",
      "📝 Procesando batch #837...\n",
      "✅ Batch #837 guardado en dataset_eb_tess_labeled_batch_0836.parquet (50000 filas)\n",
      "📝 Procesando batch #838...\n",
      "✅ Batch #838 guardado en dataset_eb_tess_labeled_batch_0837.parquet (50000 filas)\n",
      "📝 Procesando batch #839...\n",
      "✅ Batch #839 guardado en dataset_eb_tess_labeled_batch_0838.parquet (50000 filas)\n",
      "📝 Procesando batch #840...\n",
      "✅ Batch #840 guardado en dataset_eb_tess_labeled_batch_0839.parquet (50000 filas)\n",
      "📝 Procesando batch #841...\n",
      "✅ Batch #841 guardado en dataset_eb_tess_labeled_batch_0840.parquet (50000 filas)\n",
      "📝 Procesando batch #842...\n",
      "✅ Batch #842 guardado en dataset_eb_tess_labeled_batch_0841.parquet (48576 filas)\n",
      "📝 Procesando batch #843...\n",
      "✅ Batch #843 guardado en dataset_eb_tess_labeled_batch_0842.parquet (50000 filas)\n",
      "📝 Procesando batch #844...\n",
      "✅ Batch #844 guardado en dataset_eb_tess_labeled_batch_0843.parquet (50000 filas)\n",
      "📝 Procesando batch #845...\n",
      "✅ Batch #845 guardado en dataset_eb_tess_labeled_batch_0844.parquet (50000 filas)\n",
      "📝 Procesando batch #846...\n",
      "✅ Batch #846 guardado en dataset_eb_tess_labeled_batch_0845.parquet (50000 filas)\n",
      "📝 Procesando batch #847...\n",
      "✅ Batch #847 guardado en dataset_eb_tess_labeled_batch_0846.parquet (50000 filas)\n",
      "📝 Procesando batch #848...\n",
      "✅ Batch #848 guardado en dataset_eb_tess_labeled_batch_0847.parquet (50000 filas)\n",
      "📝 Procesando batch #849...\n",
      "✅ Batch #849 guardado en dataset_eb_tess_labeled_batch_0848.parquet (50000 filas)\n",
      "📝 Procesando batch #850...\n",
      "✅ Batch #850 guardado en dataset_eb_tess_labeled_batch_0849.parquet (50000 filas)\n",
      "📝 Procesando batch #851...\n",
      "✅ Batch #851 guardado en dataset_eb_tess_labeled_batch_0850.parquet (50000 filas)\n",
      "📝 Procesando batch #852...\n",
      "✅ Batch #852 guardado en dataset_eb_tess_labeled_batch_0851.parquet (50000 filas)\n",
      "📝 Procesando batch #853...\n",
      "✅ Batch #853 guardado en dataset_eb_tess_labeled_batch_0852.parquet (50000 filas)\n",
      "📝 Procesando batch #854...\n",
      "✅ Batch #854 guardado en dataset_eb_tess_labeled_batch_0853.parquet (50000 filas)\n",
      "📝 Procesando batch #855...\n",
      "✅ Batch #855 guardado en dataset_eb_tess_labeled_batch_0854.parquet (50000 filas)\n",
      "📝 Procesando batch #856...\n",
      "✅ Batch #856 guardado en dataset_eb_tess_labeled_batch_0855.parquet (50000 filas)\n",
      "📝 Procesando batch #857...\n",
      "✅ Batch #857 guardado en dataset_eb_tess_labeled_batch_0856.parquet (50000 filas)\n",
      "📝 Procesando batch #858...\n",
      "✅ Batch #858 guardado en dataset_eb_tess_labeled_batch_0857.parquet (50000 filas)\n",
      "📝 Procesando batch #859...\n",
      "✅ Batch #859 guardado en dataset_eb_tess_labeled_batch_0858.parquet (50000 filas)\n",
      "📝 Procesando batch #860...\n",
      "✅ Batch #860 guardado en dataset_eb_tess_labeled_batch_0859.parquet (50000 filas)\n",
      "📝 Procesando batch #861...\n",
      "✅ Batch #861 guardado en dataset_eb_tess_labeled_batch_0860.parquet (50000 filas)\n",
      "📝 Procesando batch #862...\n",
      "✅ Batch #862 guardado en dataset_eb_tess_labeled_batch_0861.parquet (50000 filas)\n",
      "📝 Procesando batch #863...\n",
      "✅ Batch #863 guardado en dataset_eb_tess_labeled_batch_0862.parquet (48576 filas)\n",
      "📝 Procesando batch #864...\n",
      "✅ Batch #864 guardado en dataset_eb_tess_labeled_batch_0863.parquet (50000 filas)\n",
      "📝 Procesando batch #865...\n",
      "✅ Batch #865 guardado en dataset_eb_tess_labeled_batch_0864.parquet (50000 filas)\n",
      "📝 Procesando batch #866...\n",
      "✅ Batch #866 guardado en dataset_eb_tess_labeled_batch_0865.parquet (50000 filas)\n",
      "📝 Procesando batch #867...\n",
      "✅ Batch #867 guardado en dataset_eb_tess_labeled_batch_0866.parquet (50000 filas)\n",
      "📝 Procesando batch #868...\n",
      "✅ Batch #868 guardado en dataset_eb_tess_labeled_batch_0867.parquet (50000 filas)\n",
      "📝 Procesando batch #869...\n",
      "✅ Batch #869 guardado en dataset_eb_tess_labeled_batch_0868.parquet (50000 filas)\n",
      "📝 Procesando batch #870...\n",
      "✅ Batch #870 guardado en dataset_eb_tess_labeled_batch_0869.parquet (50000 filas)\n",
      "📝 Procesando batch #871...\n",
      "✅ Batch #871 guardado en dataset_eb_tess_labeled_batch_0870.parquet (50000 filas)\n",
      "📝 Procesando batch #872...\n",
      "✅ Batch #872 guardado en dataset_eb_tess_labeled_batch_0871.parquet (50000 filas)\n",
      "📝 Procesando batch #873...\n",
      "✅ Batch #873 guardado en dataset_eb_tess_labeled_batch_0872.parquet (50000 filas)\n",
      "📝 Procesando batch #874...\n",
      "✅ Batch #874 guardado en dataset_eb_tess_labeled_batch_0873.parquet (50000 filas)\n",
      "📝 Procesando batch #875...\n",
      "✅ Batch #875 guardado en dataset_eb_tess_labeled_batch_0874.parquet (50000 filas)\n",
      "📝 Procesando batch #876...\n",
      "✅ Batch #876 guardado en dataset_eb_tess_labeled_batch_0875.parquet (50000 filas)\n",
      "📝 Procesando batch #877...\n",
      "✅ Batch #877 guardado en dataset_eb_tess_labeled_batch_0876.parquet (50000 filas)\n",
      "📝 Procesando batch #878...\n",
      "✅ Batch #878 guardado en dataset_eb_tess_labeled_batch_0877.parquet (50000 filas)\n",
      "📝 Procesando batch #879...\n",
      "✅ Batch #879 guardado en dataset_eb_tess_labeled_batch_0878.parquet (50000 filas)\n",
      "📝 Procesando batch #880...\n",
      "✅ Batch #880 guardado en dataset_eb_tess_labeled_batch_0879.parquet (50000 filas)\n",
      "📝 Procesando batch #881...\n",
      "✅ Batch #881 guardado en dataset_eb_tess_labeled_batch_0880.parquet (50000 filas)\n",
      "📝 Procesando batch #882...\n",
      "✅ Batch #882 guardado en dataset_eb_tess_labeled_batch_0881.parquet (50000 filas)\n",
      "📝 Procesando batch #883...\n",
      "✅ Batch #883 guardado en dataset_eb_tess_labeled_batch_0882.parquet (50000 filas)\n",
      "📝 Procesando batch #884...\n",
      "✅ Batch #884 guardado en dataset_eb_tess_labeled_batch_0883.parquet (48576 filas)\n",
      "📝 Procesando batch #885...\n",
      "✅ Batch #885 guardado en dataset_eb_tess_labeled_batch_0884.parquet (50000 filas)\n",
      "📝 Procesando batch #886...\n",
      "✅ Batch #886 guardado en dataset_eb_tess_labeled_batch_0885.parquet (50000 filas)\n",
      "📝 Procesando batch #887...\n",
      "✅ Batch #887 guardado en dataset_eb_tess_labeled_batch_0886.parquet (50000 filas)\n",
      "📝 Procesando batch #888...\n",
      "✅ Batch #888 guardado en dataset_eb_tess_labeled_batch_0887.parquet (50000 filas)\n",
      "📝 Procesando batch #889...\n",
      "✅ Batch #889 guardado en dataset_eb_tess_labeled_batch_0888.parquet (50000 filas)\n",
      "📝 Procesando batch #890...\n",
      "✅ Batch #890 guardado en dataset_eb_tess_labeled_batch_0889.parquet (50000 filas)\n",
      "📝 Procesando batch #891...\n",
      "✅ Batch #891 guardado en dataset_eb_tess_labeled_batch_0890.parquet (50000 filas)\n",
      "📝 Procesando batch #892...\n",
      "✅ Batch #892 guardado en dataset_eb_tess_labeled_batch_0891.parquet (50000 filas)\n",
      "📝 Procesando batch #893...\n",
      "✅ Batch #893 guardado en dataset_eb_tess_labeled_batch_0892.parquet (50000 filas)\n",
      "📝 Procesando batch #894...\n",
      "✅ Batch #894 guardado en dataset_eb_tess_labeled_batch_0893.parquet (50000 filas)\n",
      "📝 Procesando batch #895...\n",
      "✅ Batch #895 guardado en dataset_eb_tess_labeled_batch_0894.parquet (50000 filas)\n",
      "📝 Procesando batch #896...\n",
      "✅ Batch #896 guardado en dataset_eb_tess_labeled_batch_0895.parquet (50000 filas)\n",
      "📝 Procesando batch #897...\n",
      "✅ Batch #897 guardado en dataset_eb_tess_labeled_batch_0896.parquet (50000 filas)\n",
      "📝 Procesando batch #898...\n",
      "✅ Batch #898 guardado en dataset_eb_tess_labeled_batch_0897.parquet (50000 filas)\n",
      "📝 Procesando batch #899...\n",
      "✅ Batch #899 guardado en dataset_eb_tess_labeled_batch_0898.parquet (50000 filas)\n",
      "📝 Procesando batch #900...\n",
      "✅ Batch #900 guardado en dataset_eb_tess_labeled_batch_0899.parquet (50000 filas)\n",
      "📝 Procesando batch #901...\n",
      "✅ Batch #901 guardado en dataset_eb_tess_labeled_batch_0900.parquet (50000 filas)\n",
      "📝 Procesando batch #902...\n",
      "✅ Batch #902 guardado en dataset_eb_tess_labeled_batch_0901.parquet (50000 filas)\n",
      "📝 Procesando batch #903...\n",
      "✅ Batch #903 guardado en dataset_eb_tess_labeled_batch_0902.parquet (50000 filas)\n",
      "📝 Procesando batch #904...\n",
      "✅ Batch #904 guardado en dataset_eb_tess_labeled_batch_0903.parquet (50000 filas)\n",
      "📝 Procesando batch #905...\n",
      "✅ Batch #905 guardado en dataset_eb_tess_labeled_batch_0904.parquet (48576 filas)\n",
      "📝 Procesando batch #906...\n",
      "✅ Batch #906 guardado en dataset_eb_tess_labeled_batch_0905.parquet (50000 filas)\n",
      "📝 Procesando batch #907...\n",
      "✅ Batch #907 guardado en dataset_eb_tess_labeled_batch_0906.parquet (50000 filas)\n",
      "📝 Procesando batch #908...\n",
      "✅ Batch #908 guardado en dataset_eb_tess_labeled_batch_0907.parquet (50000 filas)\n",
      "📝 Procesando batch #909...\n",
      "✅ Batch #909 guardado en dataset_eb_tess_labeled_batch_0908.parquet (50000 filas)\n",
      "📝 Procesando batch #910...\n",
      "✅ Batch #910 guardado en dataset_eb_tess_labeled_batch_0909.parquet (50000 filas)\n",
      "📝 Procesando batch #911...\n",
      "✅ Batch #911 guardado en dataset_eb_tess_labeled_batch_0910.parquet (50000 filas)\n",
      "📝 Procesando batch #912...\n",
      "✅ Batch #912 guardado en dataset_eb_tess_labeled_batch_0911.parquet (50000 filas)\n",
      "📝 Procesando batch #913...\n",
      "✅ Batch #913 guardado en dataset_eb_tess_labeled_batch_0912.parquet (50000 filas)\n",
      "📝 Procesando batch #914...\n",
      "✅ Batch #914 guardado en dataset_eb_tess_labeled_batch_0913.parquet (50000 filas)\n",
      "📝 Procesando batch #915...\n",
      "✅ Batch #915 guardado en dataset_eb_tess_labeled_batch_0914.parquet (50000 filas)\n",
      "📝 Procesando batch #916...\n",
      "✅ Batch #916 guardado en dataset_eb_tess_labeled_batch_0915.parquet (50000 filas)\n",
      "📝 Procesando batch #917...\n",
      "✅ Batch #917 guardado en dataset_eb_tess_labeled_batch_0916.parquet (50000 filas)\n",
      "📝 Procesando batch #918...\n",
      "✅ Batch #918 guardado en dataset_eb_tess_labeled_batch_0917.parquet (50000 filas)\n",
      "📝 Procesando batch #919...\n",
      "✅ Batch #919 guardado en dataset_eb_tess_labeled_batch_0918.parquet (50000 filas)\n",
      "📝 Procesando batch #920...\n",
      "✅ Batch #920 guardado en dataset_eb_tess_labeled_batch_0919.parquet (50000 filas)\n",
      "📝 Procesando batch #921...\n",
      "✅ Batch #921 guardado en dataset_eb_tess_labeled_batch_0920.parquet (50000 filas)\n",
      "📝 Procesando batch #922...\n",
      "✅ Batch #922 guardado en dataset_eb_tess_labeled_batch_0921.parquet (50000 filas)\n",
      "📝 Procesando batch #923...\n",
      "✅ Batch #923 guardado en dataset_eb_tess_labeled_batch_0922.parquet (50000 filas)\n",
      "📝 Procesando batch #924...\n",
      "✅ Batch #924 guardado en dataset_eb_tess_labeled_batch_0923.parquet (50000 filas)\n",
      "📝 Procesando batch #925...\n",
      "✅ Batch #925 guardado en dataset_eb_tess_labeled_batch_0924.parquet (50000 filas)\n",
      "📝 Procesando batch #926...\n",
      "✅ Batch #926 guardado en dataset_eb_tess_labeled_batch_0925.parquet (48576 filas)\n",
      "📝 Procesando batch #927...\n",
      "✅ Batch #927 guardado en dataset_eb_tess_labeled_batch_0926.parquet (50000 filas)\n",
      "📝 Procesando batch #928...\n",
      "✅ Batch #928 guardado en dataset_eb_tess_labeled_batch_0927.parquet (50000 filas)\n",
      "📝 Procesando batch #929...\n",
      "✅ Batch #929 guardado en dataset_eb_tess_labeled_batch_0928.parquet (50000 filas)\n",
      "📝 Procesando batch #930...\n",
      "✅ Batch #930 guardado en dataset_eb_tess_labeled_batch_0929.parquet (50000 filas)\n",
      "📝 Procesando batch #931...\n",
      "✅ Batch #931 guardado en dataset_eb_tess_labeled_batch_0930.parquet (50000 filas)\n",
      "📝 Procesando batch #932...\n",
      "✅ Batch #932 guardado en dataset_eb_tess_labeled_batch_0931.parquet (50000 filas)\n",
      "📝 Procesando batch #933...\n",
      "✅ Batch #933 guardado en dataset_eb_tess_labeled_batch_0932.parquet (50000 filas)\n",
      "📝 Procesando batch #934...\n",
      "✅ Batch #934 guardado en dataset_eb_tess_labeled_batch_0933.parquet (50000 filas)\n",
      "📝 Procesando batch #935...\n",
      "✅ Batch #935 guardado en dataset_eb_tess_labeled_batch_0934.parquet (50000 filas)\n",
      "📝 Procesando batch #936...\n",
      "✅ Batch #936 guardado en dataset_eb_tess_labeled_batch_0935.parquet (50000 filas)\n",
      "📝 Procesando batch #937...\n",
      "✅ Batch #937 guardado en dataset_eb_tess_labeled_batch_0936.parquet (50000 filas)\n",
      "📝 Procesando batch #938...\n",
      "✅ Batch #938 guardado en dataset_eb_tess_labeled_batch_0937.parquet (50000 filas)\n",
      "📝 Procesando batch #939...\n",
      "✅ Batch #939 guardado en dataset_eb_tess_labeled_batch_0938.parquet (50000 filas)\n",
      "📝 Procesando batch #940...\n",
      "✅ Batch #940 guardado en dataset_eb_tess_labeled_batch_0939.parquet (50000 filas)\n",
      "📝 Procesando batch #941...\n",
      "✅ Batch #941 guardado en dataset_eb_tess_labeled_batch_0940.parquet (50000 filas)\n",
      "📝 Procesando batch #942...\n",
      "✅ Batch #942 guardado en dataset_eb_tess_labeled_batch_0941.parquet (50000 filas)\n",
      "📝 Procesando batch #943...\n",
      "✅ Batch #943 guardado en dataset_eb_tess_labeled_batch_0942.parquet (50000 filas)\n",
      "📝 Procesando batch #944...\n",
      "✅ Batch #944 guardado en dataset_eb_tess_labeled_batch_0943.parquet (50000 filas)\n",
      "📝 Procesando batch #945...\n",
      "✅ Batch #945 guardado en dataset_eb_tess_labeled_batch_0944.parquet (50000 filas)\n",
      "📝 Procesando batch #946...\n",
      "✅ Batch #946 guardado en dataset_eb_tess_labeled_batch_0945.parquet (50000 filas)\n",
      "📝 Procesando batch #947...\n",
      "✅ Batch #947 guardado en dataset_eb_tess_labeled_batch_0946.parquet (48576 filas)\n",
      "📝 Procesando batch #948...\n",
      "✅ Batch #948 guardado en dataset_eb_tess_labeled_batch_0947.parquet (49964 filas)\n",
      "📝 Procesando batch #949...\n",
      "✅ Batch #949 guardado en dataset_eb_tess_labeled_batch_0948.parquet (50000 filas)\n",
      "📝 Procesando batch #950...\n",
      "✅ Batch #950 guardado en dataset_eb_tess_labeled_batch_0949.parquet (50000 filas)\n",
      "📝 Procesando batch #951...\n",
      "✅ Batch #951 guardado en dataset_eb_tess_labeled_batch_0950.parquet (50000 filas)\n",
      "📝 Procesando batch #952...\n",
      "✅ Batch #952 guardado en dataset_eb_tess_labeled_batch_0951.parquet (50000 filas)\n",
      "📝 Procesando batch #953...\n",
      "✅ Batch #953 guardado en dataset_eb_tess_labeled_batch_0952.parquet (50000 filas)\n",
      "📝 Procesando batch #954...\n",
      "✅ Batch #954 guardado en dataset_eb_tess_labeled_batch_0953.parquet (50000 filas)\n",
      "📝 Procesando batch #955...\n",
      "✅ Batch #955 guardado en dataset_eb_tess_labeled_batch_0954.parquet (50000 filas)\n",
      "📝 Procesando batch #956...\n",
      "✅ Batch #956 guardado en dataset_eb_tess_labeled_batch_0955.parquet (50000 filas)\n",
      "📝 Procesando batch #957...\n",
      "✅ Batch #957 guardado en dataset_eb_tess_labeled_batch_0956.parquet (50000 filas)\n",
      "📝 Procesando batch #958...\n",
      "✅ Batch #958 guardado en dataset_eb_tess_labeled_batch_0957.parquet (50000 filas)\n",
      "📝 Procesando batch #959...\n",
      "✅ Batch #959 guardado en dataset_eb_tess_labeled_batch_0958.parquet (50000 filas)\n",
      "📝 Procesando batch #960...\n",
      "✅ Batch #960 guardado en dataset_eb_tess_labeled_batch_0959.parquet (50000 filas)\n",
      "📝 Procesando batch #961...\n",
      "✅ Batch #961 guardado en dataset_eb_tess_labeled_batch_0960.parquet (50000 filas)\n",
      "📝 Procesando batch #962...\n",
      "✅ Batch #962 guardado en dataset_eb_tess_labeled_batch_0961.parquet (50000 filas)\n",
      "📝 Procesando batch #963...\n",
      "✅ Batch #963 guardado en dataset_eb_tess_labeled_batch_0962.parquet (50000 filas)\n",
      "📝 Procesando batch #964...\n",
      "✅ Batch #964 guardado en dataset_eb_tess_labeled_batch_0963.parquet (50000 filas)\n",
      "📝 Procesando batch #965...\n",
      "✅ Batch #965 guardado en dataset_eb_tess_labeled_batch_0964.parquet (50000 filas)\n",
      "📝 Procesando batch #966...\n",
      "✅ Batch #966 guardado en dataset_eb_tess_labeled_batch_0965.parquet (50000 filas)\n",
      "📝 Procesando batch #967...\n",
      "✅ Batch #967 guardado en dataset_eb_tess_labeled_batch_0966.parquet (50000 filas)\n",
      "📝 Procesando batch #968...\n",
      "✅ Batch #968 guardado en dataset_eb_tess_labeled_batch_0967.parquet (50000 filas)\n",
      "📝 Procesando batch #969...\n",
      "✅ Batch #969 guardado en dataset_eb_tess_labeled_batch_0968.parquet (48576 filas)\n",
      "📝 Procesando batch #970...\n",
      "✅ Batch #970 guardado en dataset_eb_tess_labeled_batch_0969.parquet (50000 filas)\n",
      "📝 Procesando batch #971...\n",
      "✅ Batch #971 guardado en dataset_eb_tess_labeled_batch_0970.parquet (50000 filas)\n",
      "📝 Procesando batch #972...\n",
      "✅ Batch #972 guardado en dataset_eb_tess_labeled_batch_0971.parquet (50000 filas)\n",
      "📝 Procesando batch #973...\n",
      "✅ Batch #973 guardado en dataset_eb_tess_labeled_batch_0972.parquet (50000 filas)\n",
      "📝 Procesando batch #974...\n",
      "✅ Batch #974 guardado en dataset_eb_tess_labeled_batch_0973.parquet (50000 filas)\n",
      "📝 Procesando batch #975...\n",
      "✅ Batch #975 guardado en dataset_eb_tess_labeled_batch_0974.parquet (50000 filas)\n",
      "📝 Procesando batch #976...\n",
      "✅ Batch #976 guardado en dataset_eb_tess_labeled_batch_0975.parquet (50000 filas)\n",
      "📝 Procesando batch #977...\n",
      "✅ Batch #977 guardado en dataset_eb_tess_labeled_batch_0976.parquet (50000 filas)\n",
      "📝 Procesando batch #978...\n",
      "✅ Batch #978 guardado en dataset_eb_tess_labeled_batch_0977.parquet (50000 filas)\n",
      "📝 Procesando batch #979...\n",
      "✅ Batch #979 guardado en dataset_eb_tess_labeled_batch_0978.parquet (50000 filas)\n",
      "📝 Procesando batch #980...\n",
      "✅ Batch #980 guardado en dataset_eb_tess_labeled_batch_0979.parquet (50000 filas)\n",
      "📝 Procesando batch #981...\n",
      "✅ Batch #981 guardado en dataset_eb_tess_labeled_batch_0980.parquet (50000 filas)\n",
      "📝 Procesando batch #982...\n",
      "✅ Batch #982 guardado en dataset_eb_tess_labeled_batch_0981.parquet (50000 filas)\n",
      "📝 Procesando batch #983...\n",
      "✅ Batch #983 guardado en dataset_eb_tess_labeled_batch_0982.parquet (50000 filas)\n",
      "📝 Procesando batch #984...\n",
      "✅ Batch #984 guardado en dataset_eb_tess_labeled_batch_0983.parquet (50000 filas)\n",
      "📝 Procesando batch #985...\n",
      "✅ Batch #985 guardado en dataset_eb_tess_labeled_batch_0984.parquet (50000 filas)\n",
      "📝 Procesando batch #986...\n",
      "✅ Batch #986 guardado en dataset_eb_tess_labeled_batch_0985.parquet (50000 filas)\n",
      "📝 Procesando batch #987...\n",
      "✅ Batch #987 guardado en dataset_eb_tess_labeled_batch_0986.parquet (50000 filas)\n",
      "📝 Procesando batch #988...\n",
      "✅ Batch #988 guardado en dataset_eb_tess_labeled_batch_0987.parquet (50000 filas)\n",
      "📝 Procesando batch #989...\n",
      "✅ Batch #989 guardado en dataset_eb_tess_labeled_batch_0988.parquet (50000 filas)\n",
      "📝 Procesando batch #990...\n",
      "✅ Batch #990 guardado en dataset_eb_tess_labeled_batch_0989.parquet (48576 filas)\n",
      "📝 Procesando batch #991...\n",
      "✅ Batch #991 guardado en dataset_eb_tess_labeled_batch_0990.parquet (50000 filas)\n",
      "📝 Procesando batch #992...\n",
      "✅ Batch #992 guardado en dataset_eb_tess_labeled_batch_0991.parquet (50000 filas)\n",
      "📝 Procesando batch #993...\n",
      "✅ Batch #993 guardado en dataset_eb_tess_labeled_batch_0992.parquet (50000 filas)\n",
      "📝 Procesando batch #994...\n",
      "✅ Batch #994 guardado en dataset_eb_tess_labeled_batch_0993.parquet (50000 filas)\n",
      "📝 Procesando batch #995...\n",
      "✅ Batch #995 guardado en dataset_eb_tess_labeled_batch_0994.parquet (50000 filas)\n",
      "📝 Procesando batch #996...\n",
      "✅ Batch #996 guardado en dataset_eb_tess_labeled_batch_0995.parquet (50000 filas)\n",
      "📝 Procesando batch #997...\n",
      "✅ Batch #997 guardado en dataset_eb_tess_labeled_batch_0996.parquet (50000 filas)\n",
      "📝 Procesando batch #998...\n",
      "✅ Batch #998 guardado en dataset_eb_tess_labeled_batch_0997.parquet (50000 filas)\n",
      "📝 Procesando batch #999...\n",
      "✅ Batch #999 guardado en dataset_eb_tess_labeled_batch_0998.parquet (50000 filas)\n",
      "📝 Procesando batch #1000...\n",
      "✅ Batch #1000 guardado en dataset_eb_tess_labeled_batch_0999.parquet (50000 filas)\n",
      "📝 Procesando batch #1001...\n",
      "✅ Batch #1001 guardado en dataset_eb_tess_labeled_batch_1000.parquet (50000 filas)\n",
      "📝 Procesando batch #1002...\n",
      "✅ Batch #1002 guardado en dataset_eb_tess_labeled_batch_1001.parquet (50000 filas)\n",
      "📝 Procesando batch #1003...\n",
      "✅ Batch #1003 guardado en dataset_eb_tess_labeled_batch_1002.parquet (50000 filas)\n",
      "📝 Procesando batch #1004...\n",
      "✅ Batch #1004 guardado en dataset_eb_tess_labeled_batch_1003.parquet (50000 filas)\n",
      "📝 Procesando batch #1005...\n",
      "✅ Batch #1005 guardado en dataset_eb_tess_labeled_batch_1004.parquet (50000 filas)\n",
      "📝 Procesando batch #1006...\n",
      "✅ Batch #1006 guardado en dataset_eb_tess_labeled_batch_1005.parquet (50000 filas)\n",
      "📝 Procesando batch #1007...\n",
      "✅ Batch #1007 guardado en dataset_eb_tess_labeled_batch_1006.parquet (50000 filas)\n",
      "📝 Procesando batch #1008...\n",
      "✅ Batch #1008 guardado en dataset_eb_tess_labeled_batch_1007.parquet (50000 filas)\n",
      "📝 Procesando batch #1009...\n",
      "✅ Batch #1009 guardado en dataset_eb_tess_labeled_batch_1008.parquet (50000 filas)\n",
      "📝 Procesando batch #1010...\n",
      "✅ Batch #1010 guardado en dataset_eb_tess_labeled_batch_1009.parquet (50000 filas)\n",
      "📝 Procesando batch #1011...\n",
      "✅ Batch #1011 guardado en dataset_eb_tess_labeled_batch_1010.parquet (48576 filas)\n",
      "📝 Procesando batch #1012...\n",
      "✅ Batch #1012 guardado en dataset_eb_tess_labeled_batch_1011.parquet (50000 filas)\n",
      "📝 Procesando batch #1013...\n",
      "✅ Batch #1013 guardado en dataset_eb_tess_labeled_batch_1012.parquet (50000 filas)\n",
      "📝 Procesando batch #1014...\n",
      "✅ Batch #1014 guardado en dataset_eb_tess_labeled_batch_1013.parquet (50000 filas)\n",
      "📝 Procesando batch #1015...\n",
      "✅ Batch #1015 guardado en dataset_eb_tess_labeled_batch_1014.parquet (50000 filas)\n",
      "📝 Procesando batch #1016...\n",
      "✅ Batch #1016 guardado en dataset_eb_tess_labeled_batch_1015.parquet (50000 filas)\n",
      "📝 Procesando batch #1017...\n",
      "✅ Batch #1017 guardado en dataset_eb_tess_labeled_batch_1016.parquet (50000 filas)\n",
      "📝 Procesando batch #1018...\n",
      "✅ Batch #1018 guardado en dataset_eb_tess_labeled_batch_1017.parquet (50000 filas)\n",
      "📝 Procesando batch #1019...\n",
      "✅ Batch #1019 guardado en dataset_eb_tess_labeled_batch_1018.parquet (50000 filas)\n",
      "📝 Procesando batch #1020...\n",
      "✅ Batch #1020 guardado en dataset_eb_tess_labeled_batch_1019.parquet (50000 filas)\n",
      "📝 Procesando batch #1021...\n",
      "✅ Batch #1021 guardado en dataset_eb_tess_labeled_batch_1020.parquet (50000 filas)\n",
      "📝 Procesando batch #1022...\n",
      "✅ Batch #1022 guardado en dataset_eb_tess_labeled_batch_1021.parquet (50000 filas)\n",
      "📝 Procesando batch #1023...\n",
      "✅ Batch #1023 guardado en dataset_eb_tess_labeled_batch_1022.parquet (50000 filas)\n",
      "📝 Procesando batch #1024...\n",
      "✅ Batch #1024 guardado en dataset_eb_tess_labeled_batch_1023.parquet (50000 filas)\n",
      "📝 Procesando batch #1025...\n",
      "✅ Batch #1025 guardado en dataset_eb_tess_labeled_batch_1024.parquet (50000 filas)\n",
      "📝 Procesando batch #1026...\n",
      "✅ Batch #1026 guardado en dataset_eb_tess_labeled_batch_1025.parquet (50000 filas)\n",
      "📝 Procesando batch #1027...\n",
      "✅ Batch #1027 guardado en dataset_eb_tess_labeled_batch_1026.parquet (50000 filas)\n",
      "📝 Procesando batch #1028...\n",
      "✅ Batch #1028 guardado en dataset_eb_tess_labeled_batch_1027.parquet (50000 filas)\n",
      "📝 Procesando batch #1029...\n",
      "✅ Batch #1029 guardado en dataset_eb_tess_labeled_batch_1028.parquet (50000 filas)\n",
      "📝 Procesando batch #1030...\n",
      "✅ Batch #1030 guardado en dataset_eb_tess_labeled_batch_1029.parquet (50000 filas)\n",
      "📝 Procesando batch #1031...\n",
      "✅ Batch #1031 guardado en dataset_eb_tess_labeled_batch_1030.parquet (50000 filas)\n",
      "📝 Procesando batch #1032...\n",
      "✅ Batch #1032 guardado en dataset_eb_tess_labeled_batch_1031.parquet (48576 filas)\n",
      "📝 Procesando batch #1033...\n",
      "✅ Batch #1033 guardado en dataset_eb_tess_labeled_batch_1032.parquet (50000 filas)\n",
      "📝 Procesando batch #1034...\n",
      "✅ Batch #1034 guardado en dataset_eb_tess_labeled_batch_1033.parquet (50000 filas)\n",
      "📝 Procesando batch #1035...\n",
      "✅ Batch #1035 guardado en dataset_eb_tess_labeled_batch_1034.parquet (50000 filas)\n",
      "📝 Procesando batch #1036...\n",
      "✅ Batch #1036 guardado en dataset_eb_tess_labeled_batch_1035.parquet (50000 filas)\n",
      "📝 Procesando batch #1037...\n",
      "✅ Batch #1037 guardado en dataset_eb_tess_labeled_batch_1036.parquet (50000 filas)\n",
      "📝 Procesando batch #1038...\n",
      "✅ Batch #1038 guardado en dataset_eb_tess_labeled_batch_1037.parquet (50000 filas)\n",
      "📝 Procesando batch #1039...\n",
      "✅ Batch #1039 guardado en dataset_eb_tess_labeled_batch_1038.parquet (50000 filas)\n",
      "📝 Procesando batch #1040...\n",
      "✅ Batch #1040 guardado en dataset_eb_tess_labeled_batch_1039.parquet (50000 filas)\n",
      "📝 Procesando batch #1041...\n",
      "✅ Batch #1041 guardado en dataset_eb_tess_labeled_batch_1040.parquet (50000 filas)\n",
      "📝 Procesando batch #1042...\n",
      "✅ Batch #1042 guardado en dataset_eb_tess_labeled_batch_1041.parquet (50000 filas)\n",
      "📝 Procesando batch #1043...\n",
      "✅ Batch #1043 guardado en dataset_eb_tess_labeled_batch_1042.parquet (50000 filas)\n",
      "📝 Procesando batch #1044...\n",
      "✅ Batch #1044 guardado en dataset_eb_tess_labeled_batch_1043.parquet (50000 filas)\n",
      "📝 Procesando batch #1045...\n",
      "✅ Batch #1045 guardado en dataset_eb_tess_labeled_batch_1044.parquet (50000 filas)\n",
      "📝 Procesando batch #1046...\n",
      "✅ Batch #1046 guardado en dataset_eb_tess_labeled_batch_1045.parquet (50000 filas)\n",
      "📝 Procesando batch #1047...\n",
      "✅ Batch #1047 guardado en dataset_eb_tess_labeled_batch_1046.parquet (50000 filas)\n",
      "📝 Procesando batch #1048...\n",
      "✅ Batch #1048 guardado en dataset_eb_tess_labeled_batch_1047.parquet (50000 filas)\n",
      "📝 Procesando batch #1049...\n",
      "✅ Batch #1049 guardado en dataset_eb_tess_labeled_batch_1048.parquet (50000 filas)\n",
      "📝 Procesando batch #1050...\n",
      "✅ Batch #1050 guardado en dataset_eb_tess_labeled_batch_1049.parquet (50000 filas)\n",
      "📝 Procesando batch #1051...\n",
      "✅ Batch #1051 guardado en dataset_eb_tess_labeled_batch_1050.parquet (50000 filas)\n",
      "📝 Procesando batch #1052...\n",
      "✅ Batch #1052 guardado en dataset_eb_tess_labeled_batch_1051.parquet (50000 filas)\n",
      "📝 Procesando batch #1053...\n",
      "✅ Batch #1053 guardado en dataset_eb_tess_labeled_batch_1052.parquet (48576 filas)\n",
      "📝 Procesando batch #1054...\n",
      "✅ Batch #1054 guardado en dataset_eb_tess_labeled_batch_1053.parquet (50000 filas)\n",
      "📝 Procesando batch #1055...\n",
      "✅ Batch #1055 guardado en dataset_eb_tess_labeled_batch_1054.parquet (50000 filas)\n",
      "📝 Procesando batch #1056...\n",
      "✅ Batch #1056 guardado en dataset_eb_tess_labeled_batch_1055.parquet (50000 filas)\n",
      "📝 Procesando batch #1057...\n",
      "✅ Batch #1057 guardado en dataset_eb_tess_labeled_batch_1056.parquet (50000 filas)\n",
      "📝 Procesando batch #1058...\n",
      "✅ Batch #1058 guardado en dataset_eb_tess_labeled_batch_1057.parquet (50000 filas)\n",
      "📝 Procesando batch #1059...\n",
      "✅ Batch #1059 guardado en dataset_eb_tess_labeled_batch_1058.parquet (50000 filas)\n",
      "📝 Procesando batch #1060...\n",
      "✅ Batch #1060 guardado en dataset_eb_tess_labeled_batch_1059.parquet (50000 filas)\n",
      "📝 Procesando batch #1061...\n",
      "✅ Batch #1061 guardado en dataset_eb_tess_labeled_batch_1060.parquet (50000 filas)\n",
      "📝 Procesando batch #1062...\n",
      "✅ Batch #1062 guardado en dataset_eb_tess_labeled_batch_1061.parquet (50000 filas)\n",
      "📝 Procesando batch #1063...\n",
      "✅ Batch #1063 guardado en dataset_eb_tess_labeled_batch_1062.parquet (50000 filas)\n",
      "📝 Procesando batch #1064...\n",
      "✅ Batch #1064 guardado en dataset_eb_tess_labeled_batch_1063.parquet (50000 filas)\n",
      "📝 Procesando batch #1065...\n",
      "✅ Batch #1065 guardado en dataset_eb_tess_labeled_batch_1064.parquet (50000 filas)\n",
      "📝 Procesando batch #1066...\n",
      "✅ Batch #1066 guardado en dataset_eb_tess_labeled_batch_1065.parquet (50000 filas)\n",
      "📝 Procesando batch #1067...\n",
      "✅ Batch #1067 guardado en dataset_eb_tess_labeled_batch_1066.parquet (50000 filas)\n",
      "📝 Procesando batch #1068...\n",
      "✅ Batch #1068 guardado en dataset_eb_tess_labeled_batch_1067.parquet (50000 filas)\n",
      "📝 Procesando batch #1069...\n",
      "✅ Batch #1069 guardado en dataset_eb_tess_labeled_batch_1068.parquet (50000 filas)\n",
      "📝 Procesando batch #1070...\n",
      "✅ Batch #1070 guardado en dataset_eb_tess_labeled_batch_1069.parquet (50000 filas)\n",
      "📝 Procesando batch #1071...\n",
      "✅ Batch #1071 guardado en dataset_eb_tess_labeled_batch_1070.parquet (50000 filas)\n",
      "📝 Procesando batch #1072...\n",
      "✅ Batch #1072 guardado en dataset_eb_tess_labeled_batch_1071.parquet (50000 filas)\n",
      "📝 Procesando batch #1073...\n",
      "✅ Batch #1073 guardado en dataset_eb_tess_labeled_batch_1072.parquet (50000 filas)\n",
      "📝 Procesando batch #1074...\n",
      "✅ Batch #1074 guardado en dataset_eb_tess_labeled_batch_1073.parquet (48576 filas)\n",
      "📝 Procesando batch #1075...\n",
      "✅ Batch #1075 guardado en dataset_eb_tess_labeled_batch_1074.parquet (50000 filas)\n",
      "📝 Procesando batch #1076...\n",
      "✅ Batch #1076 guardado en dataset_eb_tess_labeled_batch_1075.parquet (50000 filas)\n",
      "📝 Procesando batch #1077...\n",
      "✅ Batch #1077 guardado en dataset_eb_tess_labeled_batch_1076.parquet (50000 filas)\n",
      "📝 Procesando batch #1078...\n",
      "✅ Batch #1078 guardado en dataset_eb_tess_labeled_batch_1077.parquet (50000 filas)\n",
      "📝 Procesando batch #1079...\n",
      "✅ Batch #1079 guardado en dataset_eb_tess_labeled_batch_1078.parquet (50000 filas)\n",
      "📝 Procesando batch #1080...\n",
      "✅ Batch #1080 guardado en dataset_eb_tess_labeled_batch_1079.parquet (50000 filas)\n",
      "📝 Procesando batch #1081...\n",
      "✅ Batch #1081 guardado en dataset_eb_tess_labeled_batch_1080.parquet (50000 filas)\n",
      "📝 Procesando batch #1082...\n",
      "✅ Batch #1082 guardado en dataset_eb_tess_labeled_batch_1081.parquet (50000 filas)\n",
      "📝 Procesando batch #1083...\n",
      "✅ Batch #1083 guardado en dataset_eb_tess_labeled_batch_1082.parquet (50000 filas)\n",
      "📝 Procesando batch #1084...\n",
      "✅ Batch #1084 guardado en dataset_eb_tess_labeled_batch_1083.parquet (50000 filas)\n",
      "📝 Procesando batch #1085...\n",
      "✅ Batch #1085 guardado en dataset_eb_tess_labeled_batch_1084.parquet (50000 filas)\n",
      "📝 Procesando batch #1086...\n",
      "✅ Batch #1086 guardado en dataset_eb_tess_labeled_batch_1085.parquet (50000 filas)\n",
      "📝 Procesando batch #1087...\n",
      "✅ Batch #1087 guardado en dataset_eb_tess_labeled_batch_1086.parquet (50000 filas)\n",
      "📝 Procesando batch #1088...\n",
      "✅ Batch #1088 guardado en dataset_eb_tess_labeled_batch_1087.parquet (50000 filas)\n",
      "📝 Procesando batch #1089...\n",
      "✅ Batch #1089 guardado en dataset_eb_tess_labeled_batch_1088.parquet (50000 filas)\n",
      "📝 Procesando batch #1090...\n",
      "✅ Batch #1090 guardado en dataset_eb_tess_labeled_batch_1089.parquet (50000 filas)\n",
      "📝 Procesando batch #1091...\n",
      "✅ Batch #1091 guardado en dataset_eb_tess_labeled_batch_1090.parquet (50000 filas)\n",
      "📝 Procesando batch #1092...\n",
      "✅ Batch #1092 guardado en dataset_eb_tess_labeled_batch_1091.parquet (50000 filas)\n",
      "📝 Procesando batch #1093...\n",
      "✅ Batch #1093 guardado en dataset_eb_tess_labeled_batch_1092.parquet (50000 filas)\n",
      "📝 Procesando batch #1094...\n",
      "✅ Batch #1094 guardado en dataset_eb_tess_labeled_batch_1093.parquet (50000 filas)\n",
      "📝 Procesando batch #1095...\n",
      "✅ Batch #1095 guardado en dataset_eb_tess_labeled_batch_1094.parquet (48576 filas)\n",
      "📝 Procesando batch #1096...\n",
      "✅ Batch #1096 guardado en dataset_eb_tess_labeled_batch_1095.parquet (50000 filas)\n",
      "📝 Procesando batch #1097...\n",
      "✅ Batch #1097 guardado en dataset_eb_tess_labeled_batch_1096.parquet (50000 filas)\n",
      "📝 Procesando batch #1098...\n",
      "✅ Batch #1098 guardado en dataset_eb_tess_labeled_batch_1097.parquet (50000 filas)\n",
      "📝 Procesando batch #1099...\n",
      "✅ Batch #1099 guardado en dataset_eb_tess_labeled_batch_1098.parquet (50000 filas)\n",
      "📝 Procesando batch #1100...\n",
      "✅ Batch #1100 guardado en dataset_eb_tess_labeled_batch_1099.parquet (50000 filas)\n",
      "📝 Procesando batch #1101...\n",
      "✅ Batch #1101 guardado en dataset_eb_tess_labeled_batch_1100.parquet (50000 filas)\n",
      "📝 Procesando batch #1102...\n",
      "✅ Batch #1102 guardado en dataset_eb_tess_labeled_batch_1101.parquet (50000 filas)\n",
      "📝 Procesando batch #1103...\n",
      "✅ Batch #1103 guardado en dataset_eb_tess_labeled_batch_1102.parquet (50000 filas)\n",
      "📝 Procesando batch #1104...\n",
      "✅ Batch #1104 guardado en dataset_eb_tess_labeled_batch_1103.parquet (50000 filas)\n",
      "📝 Procesando batch #1105...\n",
      "✅ Batch #1105 guardado en dataset_eb_tess_labeled_batch_1104.parquet (50000 filas)\n",
      "📝 Procesando batch #1106...\n",
      "✅ Batch #1106 guardado en dataset_eb_tess_labeled_batch_1105.parquet (50000 filas)\n",
      "📝 Procesando batch #1107...\n",
      "✅ Batch #1107 guardado en dataset_eb_tess_labeled_batch_1106.parquet (50000 filas)\n",
      "📝 Procesando batch #1108...\n",
      "✅ Batch #1108 guardado en dataset_eb_tess_labeled_batch_1107.parquet (50000 filas)\n",
      "📝 Procesando batch #1109...\n",
      "✅ Batch #1109 guardado en dataset_eb_tess_labeled_batch_1108.parquet (50000 filas)\n",
      "📝 Procesando batch #1110...\n",
      "✅ Batch #1110 guardado en dataset_eb_tess_labeled_batch_1109.parquet (23514 filas)\n",
      "📝 Procesando batch #1111...\n",
      "✅ Batch #1111 guardado en dataset_eb_tess_labeled_batch_1110.parquet (50000 filas)\n",
      "📝 Procesando batch #1112...\n",
      "✅ Batch #1112 guardado en dataset_eb_tess_labeled_batch_1111.parquet (50000 filas)\n",
      "📝 Procesando batch #1113...\n",
      "✅ Batch #1113 guardado en dataset_eb_tess_labeled_batch_1112.parquet (50000 filas)\n",
      "📝 Procesando batch #1114...\n",
      "✅ Batch #1114 guardado en dataset_eb_tess_labeled_batch_1113.parquet (50000 filas)\n",
      "📝 Procesando batch #1115...\n",
      "✅ Batch #1115 guardado en dataset_eb_tess_labeled_batch_1114.parquet (50000 filas)\n",
      "📝 Procesando batch #1116...\n",
      "✅ Batch #1116 guardado en dataset_eb_tess_labeled_batch_1115.parquet (50000 filas)\n",
      "📝 Procesando batch #1117...\n",
      "✅ Batch #1117 guardado en dataset_eb_tess_labeled_batch_1116.parquet (50000 filas)\n",
      "📝 Procesando batch #1118...\n",
      "✅ Batch #1118 guardado en dataset_eb_tess_labeled_batch_1117.parquet (50000 filas)\n",
      "📝 Procesando batch #1119...\n",
      "✅ Batch #1119 guardado en dataset_eb_tess_labeled_batch_1118.parquet (50000 filas)\n",
      "📝 Procesando batch #1120...\n",
      "✅ Batch #1120 guardado en dataset_eb_tess_labeled_batch_1119.parquet (50000 filas)\n",
      "📝 Procesando batch #1121...\n",
      "✅ Batch #1121 guardado en dataset_eb_tess_labeled_batch_1120.parquet (50000 filas)\n",
      "📝 Procesando batch #1122...\n",
      "✅ Batch #1122 guardado en dataset_eb_tess_labeled_batch_1121.parquet (50000 filas)\n",
      "📝 Procesando batch #1123...\n",
      "✅ Batch #1123 guardado en dataset_eb_tess_labeled_batch_1122.parquet (50000 filas)\n",
      "📝 Procesando batch #1124...\n",
      "✅ Batch #1124 guardado en dataset_eb_tess_labeled_batch_1123.parquet (50000 filas)\n",
      "📝 Procesando batch #1125...\n",
      "✅ Batch #1125 guardado en dataset_eb_tess_labeled_batch_1124.parquet (50000 filas)\n",
      "📝 Procesando batch #1126...\n",
      "✅ Batch #1126 guardado en dataset_eb_tess_labeled_batch_1125.parquet (50000 filas)\n",
      "📝 Procesando batch #1127...\n",
      "✅ Batch #1127 guardado en dataset_eb_tess_labeled_batch_1126.parquet (50000 filas)\n",
      "📝 Procesando batch #1128...\n",
      "✅ Batch #1128 guardado en dataset_eb_tess_labeled_batch_1127.parquet (50000 filas)\n",
      "📝 Procesando batch #1129...\n",
      "✅ Batch #1129 guardado en dataset_eb_tess_labeled_batch_1128.parquet (50000 filas)\n",
      "📝 Procesando batch #1130...\n",
      "✅ Batch #1130 guardado en dataset_eb_tess_labeled_batch_1129.parquet (50000 filas)\n",
      "📝 Procesando batch #1131...\n",
      "✅ Batch #1131 guardado en dataset_eb_tess_labeled_batch_1130.parquet (48576 filas)\n",
      "📝 Procesando batch #1132...\n",
      "✅ Batch #1132 guardado en dataset_eb_tess_labeled_batch_1131.parquet (50000 filas)\n",
      "📝 Procesando batch #1133...\n",
      "✅ Batch #1133 guardado en dataset_eb_tess_labeled_batch_1132.parquet (50000 filas)\n",
      "📝 Procesando batch #1134...\n",
      "✅ Batch #1134 guardado en dataset_eb_tess_labeled_batch_1133.parquet (50000 filas)\n",
      "📝 Procesando batch #1135...\n",
      "✅ Batch #1135 guardado en dataset_eb_tess_labeled_batch_1134.parquet (50000 filas)\n",
      "📝 Procesando batch #1136...\n",
      "✅ Batch #1136 guardado en dataset_eb_tess_labeled_batch_1135.parquet (50000 filas)\n",
      "📝 Procesando batch #1137...\n",
      "✅ Batch #1137 guardado en dataset_eb_tess_labeled_batch_1136.parquet (50000 filas)\n",
      "📝 Procesando batch #1138...\n",
      "✅ Batch #1138 guardado en dataset_eb_tess_labeled_batch_1137.parquet (50000 filas)\n",
      "📝 Procesando batch #1139...\n",
      "✅ Batch #1139 guardado en dataset_eb_tess_labeled_batch_1138.parquet (50000 filas)\n",
      "📝 Procesando batch #1140...\n",
      "✅ Batch #1140 guardado en dataset_eb_tess_labeled_batch_1139.parquet (50000 filas)\n",
      "📝 Procesando batch #1141...\n",
      "✅ Batch #1141 guardado en dataset_eb_tess_labeled_batch_1140.parquet (50000 filas)\n",
      "📝 Procesando batch #1142...\n",
      "✅ Batch #1142 guardado en dataset_eb_tess_labeled_batch_1141.parquet (50000 filas)\n",
      "📝 Procesando batch #1143...\n",
      "✅ Batch #1143 guardado en dataset_eb_tess_labeled_batch_1142.parquet (50000 filas)\n",
      "📝 Procesando batch #1144...\n",
      "✅ Batch #1144 guardado en dataset_eb_tess_labeled_batch_1143.parquet (50000 filas)\n",
      "📝 Procesando batch #1145...\n",
      "✅ Batch #1145 guardado en dataset_eb_tess_labeled_batch_1144.parquet (50000 filas)\n",
      "📝 Procesando batch #1146...\n",
      "✅ Batch #1146 guardado en dataset_eb_tess_labeled_batch_1145.parquet (50000 filas)\n",
      "📝 Procesando batch #1147...\n",
      "✅ Batch #1147 guardado en dataset_eb_tess_labeled_batch_1146.parquet (50000 filas)\n",
      "📝 Procesando batch #1148...\n",
      "✅ Batch #1148 guardado en dataset_eb_tess_labeled_batch_1147.parquet (50000 filas)\n",
      "📝 Procesando batch #1149...\n",
      "✅ Batch #1149 guardado en dataset_eb_tess_labeled_batch_1148.parquet (50000 filas)\n",
      "📝 Procesando batch #1150...\n",
      "✅ Batch #1150 guardado en dataset_eb_tess_labeled_batch_1149.parquet (50000 filas)\n",
      "📝 Procesando batch #1151...\n",
      "✅ Batch #1151 guardado en dataset_eb_tess_labeled_batch_1150.parquet (50000 filas)\n",
      "📝 Procesando batch #1152...\n",
      "✅ Batch #1152 guardado en dataset_eb_tess_labeled_batch_1151.parquet (48576 filas)\n",
      "📝 Procesando batch #1153...\n",
      "✅ Batch #1153 guardado en dataset_eb_tess_labeled_batch_1152.parquet (50000 filas)\n",
      "📝 Procesando batch #1154...\n",
      "✅ Batch #1154 guardado en dataset_eb_tess_labeled_batch_1153.parquet (50000 filas)\n",
      "📝 Procesando batch #1155...\n",
      "✅ Batch #1155 guardado en dataset_eb_tess_labeled_batch_1154.parquet (50000 filas)\n",
      "📝 Procesando batch #1156...\n",
      "✅ Batch #1156 guardado en dataset_eb_tess_labeled_batch_1155.parquet (50000 filas)\n",
      "📝 Procesando batch #1157...\n",
      "✅ Batch #1157 guardado en dataset_eb_tess_labeled_batch_1156.parquet (50000 filas)\n",
      "📝 Procesando batch #1158...\n",
      "✅ Batch #1158 guardado en dataset_eb_tess_labeled_batch_1157.parquet (50000 filas)\n",
      "📝 Procesando batch #1159...\n",
      "✅ Batch #1159 guardado en dataset_eb_tess_labeled_batch_1158.parquet (50000 filas)\n",
      "📝 Procesando batch #1160...\n",
      "✅ Batch #1160 guardado en dataset_eb_tess_labeled_batch_1159.parquet (50000 filas)\n",
      "📝 Procesando batch #1161...\n",
      "✅ Batch #1161 guardado en dataset_eb_tess_labeled_batch_1160.parquet (50000 filas)\n",
      "📝 Procesando batch #1162...\n",
      "✅ Batch #1162 guardado en dataset_eb_tess_labeled_batch_1161.parquet (50000 filas)\n",
      "📝 Procesando batch #1163...\n",
      "✅ Batch #1163 guardado en dataset_eb_tess_labeled_batch_1162.parquet (50000 filas)\n",
      "📝 Procesando batch #1164...\n",
      "✅ Batch #1164 guardado en dataset_eb_tess_labeled_batch_1163.parquet (50000 filas)\n",
      "📝 Procesando batch #1165...\n",
      "✅ Batch #1165 guardado en dataset_eb_tess_labeled_batch_1164.parquet (50000 filas)\n",
      "📝 Procesando batch #1166...\n",
      "✅ Batch #1166 guardado en dataset_eb_tess_labeled_batch_1165.parquet (50000 filas)\n",
      "📝 Procesando batch #1167...\n",
      "✅ Batch #1167 guardado en dataset_eb_tess_labeled_batch_1166.parquet (50000 filas)\n",
      "📝 Procesando batch #1168...\n",
      "✅ Batch #1168 guardado en dataset_eb_tess_labeled_batch_1167.parquet (50000 filas)\n",
      "📝 Procesando batch #1169...\n",
      "✅ Batch #1169 guardado en dataset_eb_tess_labeled_batch_1168.parquet (50000 filas)\n",
      "📝 Procesando batch #1170...\n",
      "✅ Batch #1170 guardado en dataset_eb_tess_labeled_batch_1169.parquet (50000 filas)\n",
      "📝 Procesando batch #1171...\n",
      "✅ Batch #1171 guardado en dataset_eb_tess_labeled_batch_1170.parquet (50000 filas)\n",
      "📝 Procesando batch #1172...\n",
      "✅ Batch #1172 guardado en dataset_eb_tess_labeled_batch_1171.parquet (50000 filas)\n",
      "📝 Procesando batch #1173...\n",
      "✅ Batch #1173 guardado en dataset_eb_tess_labeled_batch_1172.parquet (48576 filas)\n",
      "📝 Procesando batch #1174...\n",
      "✅ Batch #1174 guardado en dataset_eb_tess_labeled_batch_1173.parquet (50000 filas)\n",
      "📝 Procesando batch #1175...\n",
      "✅ Batch #1175 guardado en dataset_eb_tess_labeled_batch_1174.parquet (50000 filas)\n",
      "📝 Procesando batch #1176...\n",
      "✅ Batch #1176 guardado en dataset_eb_tess_labeled_batch_1175.parquet (50000 filas)\n",
      "📝 Procesando batch #1177...\n",
      "✅ Batch #1177 guardado en dataset_eb_tess_labeled_batch_1176.parquet (50000 filas)\n",
      "📝 Procesando batch #1178...\n",
      "✅ Batch #1178 guardado en dataset_eb_tess_labeled_batch_1177.parquet (50000 filas)\n",
      "📝 Procesando batch #1179...\n",
      "✅ Batch #1179 guardado en dataset_eb_tess_labeled_batch_1178.parquet (50000 filas)\n",
      "📝 Procesando batch #1180...\n",
      "✅ Batch #1180 guardado en dataset_eb_tess_labeled_batch_1179.parquet (50000 filas)\n",
      "📝 Procesando batch #1181...\n",
      "✅ Batch #1181 guardado en dataset_eb_tess_labeled_batch_1180.parquet (50000 filas)\n",
      "📝 Procesando batch #1182...\n",
      "✅ Batch #1182 guardado en dataset_eb_tess_labeled_batch_1181.parquet (50000 filas)\n",
      "📝 Procesando batch #1183...\n",
      "✅ Batch #1183 guardado en dataset_eb_tess_labeled_batch_1182.parquet (50000 filas)\n",
      "📝 Procesando batch #1184...\n",
      "✅ Batch #1184 guardado en dataset_eb_tess_labeled_batch_1183.parquet (50000 filas)\n",
      "📝 Procesando batch #1185...\n",
      "✅ Batch #1185 guardado en dataset_eb_tess_labeled_batch_1184.parquet (50000 filas)\n",
      "📝 Procesando batch #1186...\n",
      "✅ Batch #1186 guardado en dataset_eb_tess_labeled_batch_1185.parquet (50000 filas)\n",
      "📝 Procesando batch #1187...\n",
      "✅ Batch #1187 guardado en dataset_eb_tess_labeled_batch_1186.parquet (50000 filas)\n",
      "📝 Procesando batch #1188...\n",
      "✅ Batch #1188 guardado en dataset_eb_tess_labeled_batch_1187.parquet (50000 filas)\n",
      "📝 Procesando batch #1189...\n",
      "✅ Batch #1189 guardado en dataset_eb_tess_labeled_batch_1188.parquet (50000 filas)\n",
      "📝 Procesando batch #1190...\n",
      "✅ Batch #1190 guardado en dataset_eb_tess_labeled_batch_1189.parquet (50000 filas)\n",
      "📝 Procesando batch #1191...\n",
      "✅ Batch #1191 guardado en dataset_eb_tess_labeled_batch_1190.parquet (50000 filas)\n",
      "📝 Procesando batch #1192...\n",
      "✅ Batch #1192 guardado en dataset_eb_tess_labeled_batch_1191.parquet (50000 filas)\n",
      "📝 Procesando batch #1193...\n",
      "✅ Batch #1193 guardado en dataset_eb_tess_labeled_batch_1192.parquet (50000 filas)\n",
      "📝 Procesando batch #1194...\n",
      "✅ Batch #1194 guardado en dataset_eb_tess_labeled_batch_1193.parquet (48576 filas)\n",
      "📝 Procesando batch #1195...\n",
      "✅ Batch #1195 guardado en dataset_eb_tess_labeled_batch_1194.parquet (50000 filas)\n",
      "📝 Procesando batch #1196...\n",
      "✅ Batch #1196 guardado en dataset_eb_tess_labeled_batch_1195.parquet (50000 filas)\n",
      "📝 Procesando batch #1197...\n",
      "✅ Batch #1197 guardado en dataset_eb_tess_labeled_batch_1196.parquet (50000 filas)\n",
      "📝 Procesando batch #1198...\n",
      "✅ Batch #1198 guardado en dataset_eb_tess_labeled_batch_1197.parquet (50000 filas)\n",
      "📝 Procesando batch #1199...\n",
      "✅ Batch #1199 guardado en dataset_eb_tess_labeled_batch_1198.parquet (50000 filas)\n",
      "📝 Procesando batch #1200...\n",
      "✅ Batch #1200 guardado en dataset_eb_tess_labeled_batch_1199.parquet (50000 filas)\n",
      "📝 Procesando batch #1201...\n",
      "✅ Batch #1201 guardado en dataset_eb_tess_labeled_batch_1200.parquet (50000 filas)\n",
      "📝 Procesando batch #1202...\n",
      "✅ Batch #1202 guardado en dataset_eb_tess_labeled_batch_1201.parquet (50000 filas)\n",
      "📝 Procesando batch #1203...\n",
      "✅ Batch #1203 guardado en dataset_eb_tess_labeled_batch_1202.parquet (50000 filas)\n",
      "📝 Procesando batch #1204...\n",
      "✅ Batch #1204 guardado en dataset_eb_tess_labeled_batch_1203.parquet (50000 filas)\n",
      "📝 Procesando batch #1205...\n",
      "✅ Batch #1205 guardado en dataset_eb_tess_labeled_batch_1204.parquet (50000 filas)\n",
      "📝 Procesando batch #1206...\n",
      "✅ Batch #1206 guardado en dataset_eb_tess_labeled_batch_1205.parquet (50000 filas)\n",
      "📝 Procesando batch #1207...\n",
      "✅ Batch #1207 guardado en dataset_eb_tess_labeled_batch_1206.parquet (50000 filas)\n",
      "📝 Procesando batch #1208...\n",
      "✅ Batch #1208 guardado en dataset_eb_tess_labeled_batch_1207.parquet (50000 filas)\n",
      "📝 Procesando batch #1209...\n",
      "✅ Batch #1209 guardado en dataset_eb_tess_labeled_batch_1208.parquet (50000 filas)\n",
      "📝 Procesando batch #1210...\n",
      "✅ Batch #1210 guardado en dataset_eb_tess_labeled_batch_1209.parquet (50000 filas)\n",
      "📝 Procesando batch #1211...\n",
      "✅ Batch #1211 guardado en dataset_eb_tess_labeled_batch_1210.parquet (50000 filas)\n",
      "📝 Procesando batch #1212...\n",
      "✅ Batch #1212 guardado en dataset_eb_tess_labeled_batch_1211.parquet (50000 filas)\n",
      "📝 Procesando batch #1213...\n",
      "✅ Batch #1213 guardado en dataset_eb_tess_labeled_batch_1212.parquet (50000 filas)\n",
      "📝 Procesando batch #1214...\n",
      "✅ Batch #1214 guardado en dataset_eb_tess_labeled_batch_1213.parquet (50000 filas)\n",
      "📝 Procesando batch #1215...\n",
      "✅ Batch #1215 guardado en dataset_eb_tess_labeled_batch_1214.parquet (48576 filas)\n",
      "📝 Procesando batch #1216...\n",
      "✅ Batch #1216 guardado en dataset_eb_tess_labeled_batch_1215.parquet (50000 filas)\n",
      "📝 Procesando batch #1217...\n",
      "✅ Batch #1217 guardado en dataset_eb_tess_labeled_batch_1216.parquet (50000 filas)\n",
      "📝 Procesando batch #1218...\n",
      "✅ Batch #1218 guardado en dataset_eb_tess_labeled_batch_1217.parquet (50000 filas)\n",
      "📝 Procesando batch #1219...\n",
      "✅ Batch #1219 guardado en dataset_eb_tess_labeled_batch_1218.parquet (50000 filas)\n",
      "📝 Procesando batch #1220...\n",
      "✅ Batch #1220 guardado en dataset_eb_tess_labeled_batch_1219.parquet (50000 filas)\n",
      "📝 Procesando batch #1221...\n",
      "✅ Batch #1221 guardado en dataset_eb_tess_labeled_batch_1220.parquet (50000 filas)\n",
      "📝 Procesando batch #1222...\n",
      "✅ Batch #1222 guardado en dataset_eb_tess_labeled_batch_1221.parquet (50000 filas)\n",
      "📝 Procesando batch #1223...\n",
      "✅ Batch #1223 guardado en dataset_eb_tess_labeled_batch_1222.parquet (50000 filas)\n",
      "📝 Procesando batch #1224...\n",
      "✅ Batch #1224 guardado en dataset_eb_tess_labeled_batch_1223.parquet (50000 filas)\n",
      "📝 Procesando batch #1225...\n",
      "✅ Batch #1225 guardado en dataset_eb_tess_labeled_batch_1224.parquet (50000 filas)\n",
      "📝 Procesando batch #1226...\n",
      "✅ Batch #1226 guardado en dataset_eb_tess_labeled_batch_1225.parquet (50000 filas)\n",
      "📝 Procesando batch #1227...\n",
      "✅ Batch #1227 guardado en dataset_eb_tess_labeled_batch_1226.parquet (50000 filas)\n",
      "📝 Procesando batch #1228...\n",
      "✅ Batch #1228 guardado en dataset_eb_tess_labeled_batch_1227.parquet (50000 filas)\n",
      "📝 Procesando batch #1229...\n",
      "✅ Batch #1229 guardado en dataset_eb_tess_labeled_batch_1228.parquet (50000 filas)\n",
      "📝 Procesando batch #1230...\n",
      "✅ Batch #1230 guardado en dataset_eb_tess_labeled_batch_1229.parquet (50000 filas)\n",
      "📝 Procesando batch #1231...\n",
      "✅ Batch #1231 guardado en dataset_eb_tess_labeled_batch_1230.parquet (50000 filas)\n",
      "📝 Procesando batch #1232...\n",
      "✅ Batch #1232 guardado en dataset_eb_tess_labeled_batch_1231.parquet (50000 filas)\n",
      "📝 Procesando batch #1233...\n",
      "✅ Batch #1233 guardado en dataset_eb_tess_labeled_batch_1232.parquet (50000 filas)\n",
      "📝 Procesando batch #1234...\n",
      "✅ Batch #1234 guardado en dataset_eb_tess_labeled_batch_1233.parquet (50000 filas)\n",
      "📝 Procesando batch #1235...\n",
      "✅ Batch #1235 guardado en dataset_eb_tess_labeled_batch_1234.parquet (50000 filas)\n",
      "📝 Procesando batch #1236...\n",
      "✅ Batch #1236 guardado en dataset_eb_tess_labeled_batch_1235.parquet (48576 filas)\n",
      "📝 Procesando batch #1237...\n",
      "✅ Batch #1237 guardado en dataset_eb_tess_labeled_batch_1236.parquet (50000 filas)\n",
      "📝 Procesando batch #1238...\n",
      "✅ Batch #1238 guardado en dataset_eb_tess_labeled_batch_1237.parquet (50000 filas)\n",
      "📝 Procesando batch #1239...\n",
      "✅ Batch #1239 guardado en dataset_eb_tess_labeled_batch_1238.parquet (50000 filas)\n",
      "📝 Procesando batch #1240...\n",
      "✅ Batch #1240 guardado en dataset_eb_tess_labeled_batch_1239.parquet (50000 filas)\n",
      "📝 Procesando batch #1241...\n",
      "✅ Batch #1241 guardado en dataset_eb_tess_labeled_batch_1240.parquet (50000 filas)\n",
      "📝 Procesando batch #1242...\n",
      "✅ Batch #1242 guardado en dataset_eb_tess_labeled_batch_1241.parquet (50000 filas)\n",
      "📝 Procesando batch #1243...\n",
      "✅ Batch #1243 guardado en dataset_eb_tess_labeled_batch_1242.parquet (50000 filas)\n",
      "📝 Procesando batch #1244...\n",
      "✅ Batch #1244 guardado en dataset_eb_tess_labeled_batch_1243.parquet (50000 filas)\n",
      "📝 Procesando batch #1245...\n",
      "✅ Batch #1245 guardado en dataset_eb_tess_labeled_batch_1244.parquet (50000 filas)\n",
      "📝 Procesando batch #1246...\n",
      "✅ Batch #1246 guardado en dataset_eb_tess_labeled_batch_1245.parquet (50000 filas)\n",
      "📝 Procesando batch #1247...\n",
      "✅ Batch #1247 guardado en dataset_eb_tess_labeled_batch_1246.parquet (50000 filas)\n",
      "📝 Procesando batch #1248...\n",
      "✅ Batch #1248 guardado en dataset_eb_tess_labeled_batch_1247.parquet (50000 filas)\n",
      "📝 Procesando batch #1249...\n",
      "✅ Batch #1249 guardado en dataset_eb_tess_labeled_batch_1248.parquet (50000 filas)\n",
      "📝 Procesando batch #1250...\n",
      "✅ Batch #1250 guardado en dataset_eb_tess_labeled_batch_1249.parquet (50000 filas)\n",
      "📝 Procesando batch #1251...\n",
      "✅ Batch #1251 guardado en dataset_eb_tess_labeled_batch_1250.parquet (50000 filas)\n",
      "📝 Procesando batch #1252...\n",
      "✅ Batch #1252 guardado en dataset_eb_tess_labeled_batch_1251.parquet (50000 filas)\n",
      "📝 Procesando batch #1253...\n",
      "✅ Batch #1253 guardado en dataset_eb_tess_labeled_batch_1252.parquet (50000 filas)\n",
      "📝 Procesando batch #1254...\n",
      "✅ Batch #1254 guardado en dataset_eb_tess_labeled_batch_1253.parquet (50000 filas)\n",
      "📝 Procesando batch #1255...\n",
      "✅ Batch #1255 guardado en dataset_eb_tess_labeled_batch_1254.parquet (50000 filas)\n",
      "📝 Procesando batch #1256...\n",
      "✅ Batch #1256 guardado en dataset_eb_tess_labeled_batch_1255.parquet (50000 filas)\n",
      "📝 Procesando batch #1257...\n",
      "✅ Batch #1257 guardado en dataset_eb_tess_labeled_batch_1256.parquet (48576 filas)\n",
      "📝 Procesando batch #1258...\n",
      "✅ Batch #1258 guardado en dataset_eb_tess_labeled_batch_1257.parquet (50000 filas)\n",
      "📝 Procesando batch #1259...\n",
      "✅ Batch #1259 guardado en dataset_eb_tess_labeled_batch_1258.parquet (50000 filas)\n",
      "📝 Procesando batch #1260...\n",
      "✅ Batch #1260 guardado en dataset_eb_tess_labeled_batch_1259.parquet (50000 filas)\n",
      "📝 Procesando batch #1261...\n",
      "✅ Batch #1261 guardado en dataset_eb_tess_labeled_batch_1260.parquet (50000 filas)\n",
      "📝 Procesando batch #1262...\n",
      "✅ Batch #1262 guardado en dataset_eb_tess_labeled_batch_1261.parquet (50000 filas)\n",
      "📝 Procesando batch #1263...\n",
      "✅ Batch #1263 guardado en dataset_eb_tess_labeled_batch_1262.parquet (50000 filas)\n",
      "📝 Procesando batch #1264...\n",
      "✅ Batch #1264 guardado en dataset_eb_tess_labeled_batch_1263.parquet (50000 filas)\n",
      "📝 Procesando batch #1265...\n",
      "✅ Batch #1265 guardado en dataset_eb_tess_labeled_batch_1264.parquet (50000 filas)\n",
      "📝 Procesando batch #1266...\n",
      "✅ Batch #1266 guardado en dataset_eb_tess_labeled_batch_1265.parquet (50000 filas)\n",
      "📝 Procesando batch #1267...\n",
      "✅ Batch #1267 guardado en dataset_eb_tess_labeled_batch_1266.parquet (50000 filas)\n",
      "📝 Procesando batch #1268...\n",
      "✅ Batch #1268 guardado en dataset_eb_tess_labeled_batch_1267.parquet (50000 filas)\n",
      "📝 Procesando batch #1269...\n",
      "✅ Batch #1269 guardado en dataset_eb_tess_labeled_batch_1268.parquet (50000 filas)\n",
      "📝 Procesando batch #1270...\n",
      "✅ Batch #1270 guardado en dataset_eb_tess_labeled_batch_1269.parquet (50000 filas)\n",
      "📝 Procesando batch #1271...\n",
      "✅ Batch #1271 guardado en dataset_eb_tess_labeled_batch_1270.parquet (50000 filas)\n",
      "📝 Procesando batch #1272...\n",
      "✅ Batch #1272 guardado en dataset_eb_tess_labeled_batch_1271.parquet (50000 filas)\n",
      "📝 Procesando batch #1273...\n",
      "✅ Batch #1273 guardado en dataset_eb_tess_labeled_batch_1272.parquet (50000 filas)\n",
      "📝 Procesando batch #1274...\n",
      "✅ Batch #1274 guardado en dataset_eb_tess_labeled_batch_1273.parquet (21056 filas)\n",
      "📝 Procesando batch #1275...\n",
      "✅ Batch #1275 guardado en dataset_eb_tess_labeled_batch_1274.parquet (50000 filas)\n",
      "📝 Procesando batch #1276...\n",
      "✅ Batch #1276 guardado en dataset_eb_tess_labeled_batch_1275.parquet (50000 filas)\n",
      "📝 Procesando batch #1277...\n",
      "✅ Batch #1277 guardado en dataset_eb_tess_labeled_batch_1276.parquet (50000 filas)\n",
      "📝 Procesando batch #1278...\n",
      "✅ Batch #1278 guardado en dataset_eb_tess_labeled_batch_1277.parquet (50000 filas)\n",
      "📝 Procesando batch #1279...\n",
      "✅ Batch #1279 guardado en dataset_eb_tess_labeled_batch_1278.parquet (50000 filas)\n",
      "📝 Procesando batch #1280...\n",
      "✅ Batch #1280 guardado en dataset_eb_tess_labeled_batch_1279.parquet (50000 filas)\n",
      "📝 Procesando batch #1281...\n",
      "✅ Batch #1281 guardado en dataset_eb_tess_labeled_batch_1280.parquet (50000 filas)\n",
      "📝 Procesando batch #1282...\n",
      "✅ Batch #1282 guardado en dataset_eb_tess_labeled_batch_1281.parquet (50000 filas)\n",
      "📝 Procesando batch #1283...\n",
      "✅ Batch #1283 guardado en dataset_eb_tess_labeled_batch_1282.parquet (50000 filas)\n",
      "📝 Procesando batch #1284...\n",
      "✅ Batch #1284 guardado en dataset_eb_tess_labeled_batch_1283.parquet (50000 filas)\n",
      "📝 Procesando batch #1285...\n",
      "✅ Batch #1285 guardado en dataset_eb_tess_labeled_batch_1284.parquet (50000 filas)\n",
      "📝 Procesando batch #1286...\n",
      "✅ Batch #1286 guardado en dataset_eb_tess_labeled_batch_1285.parquet (50000 filas)\n",
      "📝 Procesando batch #1287...\n",
      "✅ Batch #1287 guardado en dataset_eb_tess_labeled_batch_1286.parquet (50000 filas)\n",
      "📝 Procesando batch #1288...\n",
      "✅ Batch #1288 guardado en dataset_eb_tess_labeled_batch_1287.parquet (50000 filas)\n",
      "📝 Procesando batch #1289...\n",
      "✅ Batch #1289 guardado en dataset_eb_tess_labeled_batch_1288.parquet (50000 filas)\n",
      "📝 Procesando batch #1290...\n",
      "✅ Batch #1290 guardado en dataset_eb_tess_labeled_batch_1289.parquet (50000 filas)\n",
      "📝 Procesando batch #1291...\n",
      "✅ Batch #1291 guardado en dataset_eb_tess_labeled_batch_1290.parquet (50000 filas)\n",
      "📝 Procesando batch #1292...\n",
      "✅ Batch #1292 guardado en dataset_eb_tess_labeled_batch_1291.parquet (50000 filas)\n",
      "📝 Procesando batch #1293...\n",
      "✅ Batch #1293 guardado en dataset_eb_tess_labeled_batch_1292.parquet (50000 filas)\n",
      "📝 Procesando batch #1294...\n",
      "✅ Batch #1294 guardado en dataset_eb_tess_labeled_batch_1293.parquet (50000 filas)\n",
      "📝 Procesando batch #1295...\n",
      "✅ Batch #1295 guardado en dataset_eb_tess_labeled_batch_1294.parquet (48576 filas)\n",
      "📝 Procesando batch #1296...\n",
      "✅ Batch #1296 guardado en dataset_eb_tess_labeled_batch_1295.parquet (50000 filas)\n",
      "📝 Procesando batch #1297...\n",
      "✅ Batch #1297 guardado en dataset_eb_tess_labeled_batch_1296.parquet (50000 filas)\n",
      "📝 Procesando batch #1298...\n",
      "✅ Batch #1298 guardado en dataset_eb_tess_labeled_batch_1297.parquet (50000 filas)\n",
      "📝 Procesando batch #1299...\n",
      "✅ Batch #1299 guardado en dataset_eb_tess_labeled_batch_1298.parquet (50000 filas)\n",
      "📝 Procesando batch #1300...\n",
      "✅ Batch #1300 guardado en dataset_eb_tess_labeled_batch_1299.parquet (50000 filas)\n",
      "📝 Procesando batch #1301...\n",
      "✅ Batch #1301 guardado en dataset_eb_tess_labeled_batch_1300.parquet (50000 filas)\n",
      "📝 Procesando batch #1302...\n",
      "✅ Batch #1302 guardado en dataset_eb_tess_labeled_batch_1301.parquet (50000 filas)\n",
      "📝 Procesando batch #1303...\n",
      "✅ Batch #1303 guardado en dataset_eb_tess_labeled_batch_1302.parquet (50000 filas)\n",
      "📝 Procesando batch #1304...\n",
      "✅ Batch #1304 guardado en dataset_eb_tess_labeled_batch_1303.parquet (50000 filas)\n",
      "📝 Procesando batch #1305...\n",
      "✅ Batch #1305 guardado en dataset_eb_tess_labeled_batch_1304.parquet (50000 filas)\n",
      "📝 Procesando batch #1306...\n",
      "✅ Batch #1306 guardado en dataset_eb_tess_labeled_batch_1305.parquet (50000 filas)\n",
      "📝 Procesando batch #1307...\n",
      "✅ Batch #1307 guardado en dataset_eb_tess_labeled_batch_1306.parquet (50000 filas)\n",
      "📝 Procesando batch #1308...\n",
      "✅ Batch #1308 guardado en dataset_eb_tess_labeled_batch_1307.parquet (50000 filas)\n",
      "📝 Procesando batch #1309...\n",
      "✅ Batch #1309 guardado en dataset_eb_tess_labeled_batch_1308.parquet (50000 filas)\n",
      "📝 Procesando batch #1310...\n",
      "✅ Batch #1310 guardado en dataset_eb_tess_labeled_batch_1309.parquet (50000 filas)\n",
      "📝 Procesando batch #1311...\n",
      "✅ Batch #1311 guardado en dataset_eb_tess_labeled_batch_1310.parquet (50000 filas)\n",
      "📝 Procesando batch #1312...\n",
      "✅ Batch #1312 guardado en dataset_eb_tess_labeled_batch_1311.parquet (50000 filas)\n",
      "📝 Procesando batch #1313...\n",
      "✅ Batch #1313 guardado en dataset_eb_tess_labeled_batch_1312.parquet (50000 filas)\n",
      "📝 Procesando batch #1314...\n",
      "✅ Batch #1314 guardado en dataset_eb_tess_labeled_batch_1313.parquet (50000 filas)\n",
      "📝 Procesando batch #1315...\n",
      "✅ Batch #1315 guardado en dataset_eb_tess_labeled_batch_1314.parquet (50000 filas)\n",
      "📝 Procesando batch #1316...\n",
      "✅ Batch #1316 guardado en dataset_eb_tess_labeled_batch_1315.parquet (48576 filas)\n",
      "📝 Procesando batch #1317...\n",
      "✅ Batch #1317 guardado en dataset_eb_tess_labeled_batch_1316.parquet (50000 filas)\n",
      "📝 Procesando batch #1318...\n",
      "✅ Batch #1318 guardado en dataset_eb_tess_labeled_batch_1317.parquet (50000 filas)\n",
      "📝 Procesando batch #1319...\n",
      "✅ Batch #1319 guardado en dataset_eb_tess_labeled_batch_1318.parquet (50000 filas)\n",
      "📝 Procesando batch #1320...\n",
      "✅ Batch #1320 guardado en dataset_eb_tess_labeled_batch_1319.parquet (50000 filas)\n",
      "📝 Procesando batch #1321...\n",
      "✅ Batch #1321 guardado en dataset_eb_tess_labeled_batch_1320.parquet (50000 filas)\n",
      "📝 Procesando batch #1322...\n",
      "✅ Batch #1322 guardado en dataset_eb_tess_labeled_batch_1321.parquet (50000 filas)\n",
      "📝 Procesando batch #1323...\n",
      "✅ Batch #1323 guardado en dataset_eb_tess_labeled_batch_1322.parquet (50000 filas)\n",
      "📝 Procesando batch #1324...\n",
      "✅ Batch #1324 guardado en dataset_eb_tess_labeled_batch_1323.parquet (50000 filas)\n",
      "📝 Procesando batch #1325...\n",
      "✅ Batch #1325 guardado en dataset_eb_tess_labeled_batch_1324.parquet (50000 filas)\n",
      "📝 Procesando batch #1326...\n",
      "✅ Batch #1326 guardado en dataset_eb_tess_labeled_batch_1325.parquet (50000 filas)\n",
      "📝 Procesando batch #1327...\n",
      "✅ Batch #1327 guardado en dataset_eb_tess_labeled_batch_1326.parquet (50000 filas)\n",
      "📝 Procesando batch #1328...\n",
      "✅ Batch #1328 guardado en dataset_eb_tess_labeled_batch_1327.parquet (50000 filas)\n",
      "📝 Procesando batch #1329...\n",
      "✅ Batch #1329 guardado en dataset_eb_tess_labeled_batch_1328.parquet (50000 filas)\n",
      "📝 Procesando batch #1330...\n",
      "✅ Batch #1330 guardado en dataset_eb_tess_labeled_batch_1329.parquet (50000 filas)\n",
      "📝 Procesando batch #1331...\n",
      "✅ Batch #1331 guardado en dataset_eb_tess_labeled_batch_1330.parquet (50000 filas)\n",
      "📝 Procesando batch #1332...\n",
      "✅ Batch #1332 guardado en dataset_eb_tess_labeled_batch_1331.parquet (50000 filas)\n",
      "📝 Procesando batch #1333...\n",
      "✅ Batch #1333 guardado en dataset_eb_tess_labeled_batch_1332.parquet (50000 filas)\n",
      "📝 Procesando batch #1334...\n",
      "✅ Batch #1334 guardado en dataset_eb_tess_labeled_batch_1333.parquet (50000 filas)\n",
      "📝 Procesando batch #1335...\n",
      "✅ Batch #1335 guardado en dataset_eb_tess_labeled_batch_1334.parquet (50000 filas)\n",
      "📝 Procesando batch #1336...\n",
      "✅ Batch #1336 guardado en dataset_eb_tess_labeled_batch_1335.parquet (50000 filas)\n",
      "📝 Procesando batch #1337...\n",
      "✅ Batch #1337 guardado en dataset_eb_tess_labeled_batch_1336.parquet (48576 filas)\n",
      "📝 Procesando batch #1338...\n",
      "✅ Batch #1338 guardado en dataset_eb_tess_labeled_batch_1337.parquet (50000 filas)\n",
      "📝 Procesando batch #1339...\n",
      "✅ Batch #1339 guardado en dataset_eb_tess_labeled_batch_1338.parquet (50000 filas)\n",
      "📝 Procesando batch #1340...\n",
      "✅ Batch #1340 guardado en dataset_eb_tess_labeled_batch_1339.parquet (50000 filas)\n",
      "📝 Procesando batch #1341...\n",
      "✅ Batch #1341 guardado en dataset_eb_tess_labeled_batch_1340.parquet (50000 filas)\n",
      "📝 Procesando batch #1342...\n",
      "✅ Batch #1342 guardado en dataset_eb_tess_labeled_batch_1341.parquet (50000 filas)\n",
      "📝 Procesando batch #1343...\n",
      "✅ Batch #1343 guardado en dataset_eb_tess_labeled_batch_1342.parquet (50000 filas)\n",
      "📝 Procesando batch #1344...\n",
      "✅ Batch #1344 guardado en dataset_eb_tess_labeled_batch_1343.parquet (50000 filas)\n",
      "📝 Procesando batch #1345...\n",
      "✅ Batch #1345 guardado en dataset_eb_tess_labeled_batch_1344.parquet (50000 filas)\n",
      "📝 Procesando batch #1346...\n",
      "✅ Batch #1346 guardado en dataset_eb_tess_labeled_batch_1345.parquet (50000 filas)\n",
      "📝 Procesando batch #1347...\n",
      "✅ Batch #1347 guardado en dataset_eb_tess_labeled_batch_1346.parquet (50000 filas)\n",
      "📝 Procesando batch #1348...\n",
      "✅ Batch #1348 guardado en dataset_eb_tess_labeled_batch_1347.parquet (50000 filas)\n",
      "📝 Procesando batch #1349...\n",
      "✅ Batch #1349 guardado en dataset_eb_tess_labeled_batch_1348.parquet (50000 filas)\n",
      "📝 Procesando batch #1350...\n",
      "✅ Batch #1350 guardado en dataset_eb_tess_labeled_batch_1349.parquet (50000 filas)\n",
      "📝 Procesando batch #1351...\n",
      "✅ Batch #1351 guardado en dataset_eb_tess_labeled_batch_1350.parquet (50000 filas)\n",
      "📝 Procesando batch #1352...\n",
      "✅ Batch #1352 guardado en dataset_eb_tess_labeled_batch_1351.parquet (50000 filas)\n",
      "📝 Procesando batch #1353...\n",
      "✅ Batch #1353 guardado en dataset_eb_tess_labeled_batch_1352.parquet (50000 filas)\n",
      "📝 Procesando batch #1354...\n",
      "✅ Batch #1354 guardado en dataset_eb_tess_labeled_batch_1353.parquet (50000 filas)\n",
      "📝 Procesando batch #1355...\n",
      "✅ Batch #1355 guardado en dataset_eb_tess_labeled_batch_1354.parquet (50000 filas)\n",
      "📝 Procesando batch #1356...\n",
      "✅ Batch #1356 guardado en dataset_eb_tess_labeled_batch_1355.parquet (50000 filas)\n",
      "📝 Procesando batch #1357...\n",
      "✅ Batch #1357 guardado en dataset_eb_tess_labeled_batch_1356.parquet (50000 filas)\n",
      "📝 Procesando batch #1358...\n",
      "✅ Batch #1358 guardado en dataset_eb_tess_labeled_batch_1357.parquet (48576 filas)\n",
      "📝 Procesando batch #1359...\n",
      "✅ Batch #1359 guardado en dataset_eb_tess_labeled_batch_1358.parquet (50000 filas)\n",
      "📝 Procesando batch #1360...\n",
      "✅ Batch #1360 guardado en dataset_eb_tess_labeled_batch_1359.parquet (50000 filas)\n",
      "📝 Procesando batch #1361...\n",
      "✅ Batch #1361 guardado en dataset_eb_tess_labeled_batch_1360.parquet (50000 filas)\n",
      "📝 Procesando batch #1362...\n",
      "✅ Batch #1362 guardado en dataset_eb_tess_labeled_batch_1361.parquet (50000 filas)\n",
      "📝 Procesando batch #1363...\n",
      "✅ Batch #1363 guardado en dataset_eb_tess_labeled_batch_1362.parquet (50000 filas)\n",
      "📝 Procesando batch #1364...\n",
      "✅ Batch #1364 guardado en dataset_eb_tess_labeled_batch_1363.parquet (50000 filas)\n",
      "📝 Procesando batch #1365...\n",
      "✅ Batch #1365 guardado en dataset_eb_tess_labeled_batch_1364.parquet (50000 filas)\n",
      "📝 Procesando batch #1366...\n",
      "✅ Batch #1366 guardado en dataset_eb_tess_labeled_batch_1365.parquet (50000 filas)\n",
      "📝 Procesando batch #1367...\n",
      "✅ Batch #1367 guardado en dataset_eb_tess_labeled_batch_1366.parquet (50000 filas)\n",
      "📝 Procesando batch #1368...\n",
      "✅ Batch #1368 guardado en dataset_eb_tess_labeled_batch_1367.parquet (50000 filas)\n",
      "📝 Procesando batch #1369...\n",
      "✅ Batch #1369 guardado en dataset_eb_tess_labeled_batch_1368.parquet (50000 filas)\n",
      "📝 Procesando batch #1370...\n",
      "✅ Batch #1370 guardado en dataset_eb_tess_labeled_batch_1369.parquet (50000 filas)\n",
      "📝 Procesando batch #1371...\n",
      "✅ Batch #1371 guardado en dataset_eb_tess_labeled_batch_1370.parquet (50000 filas)\n",
      "📝 Procesando batch #1372...\n",
      "✅ Batch #1372 guardado en dataset_eb_tess_labeled_batch_1371.parquet (50000 filas)\n",
      "📝 Procesando batch #1373...\n",
      "✅ Batch #1373 guardado en dataset_eb_tess_labeled_batch_1372.parquet (50000 filas)\n",
      "📝 Procesando batch #1374...\n",
      "✅ Batch #1374 guardado en dataset_eb_tess_labeled_batch_1373.parquet (50000 filas)\n",
      "📝 Procesando batch #1375...\n",
      "✅ Batch #1375 guardado en dataset_eb_tess_labeled_batch_1374.parquet (50000 filas)\n",
      "📝 Procesando batch #1376...\n",
      "✅ Batch #1376 guardado en dataset_eb_tess_labeled_batch_1375.parquet (50000 filas)\n",
      "📝 Procesando batch #1377...\n",
      "✅ Batch #1377 guardado en dataset_eb_tess_labeled_batch_1376.parquet (50000 filas)\n",
      "📝 Procesando batch #1378...\n",
      "✅ Batch #1378 guardado en dataset_eb_tess_labeled_batch_1377.parquet (50000 filas)\n",
      "📝 Procesando batch #1379...\n",
      "✅ Batch #1379 guardado en dataset_eb_tess_labeled_batch_1378.parquet (48576 filas)\n",
      "📝 Procesando batch #1380...\n",
      "✅ Batch #1380 guardado en dataset_eb_tess_labeled_batch_1379.parquet (50000 filas)\n",
      "📝 Procesando batch #1381...\n",
      "✅ Batch #1381 guardado en dataset_eb_tess_labeled_batch_1380.parquet (50000 filas)\n",
      "📝 Procesando batch #1382...\n",
      "✅ Batch #1382 guardado en dataset_eb_tess_labeled_batch_1381.parquet (50000 filas)\n",
      "📝 Procesando batch #1383...\n",
      "✅ Batch #1383 guardado en dataset_eb_tess_labeled_batch_1382.parquet (50000 filas)\n",
      "📝 Procesando batch #1384...\n",
      "✅ Batch #1384 guardado en dataset_eb_tess_labeled_batch_1383.parquet (50000 filas)\n",
      "📝 Procesando batch #1385...\n",
      "✅ Batch #1385 guardado en dataset_eb_tess_labeled_batch_1384.parquet (50000 filas)\n",
      "📝 Procesando batch #1386...\n",
      "✅ Batch #1386 guardado en dataset_eb_tess_labeled_batch_1385.parquet (50000 filas)\n",
      "📝 Procesando batch #1387...\n",
      "✅ Batch #1387 guardado en dataset_eb_tess_labeled_batch_1386.parquet (50000 filas)\n",
      "📝 Procesando batch #1388...\n",
      "✅ Batch #1388 guardado en dataset_eb_tess_labeled_batch_1387.parquet (50000 filas)\n",
      "📝 Procesando batch #1389...\n",
      "✅ Batch #1389 guardado en dataset_eb_tess_labeled_batch_1388.parquet (50000 filas)\n",
      "📝 Procesando batch #1390...\n",
      "✅ Batch #1390 guardado en dataset_eb_tess_labeled_batch_1389.parquet (50000 filas)\n",
      "📝 Procesando batch #1391...\n",
      "✅ Batch #1391 guardado en dataset_eb_tess_labeled_batch_1390.parquet (50000 filas)\n",
      "📝 Procesando batch #1392...\n",
      "✅ Batch #1392 guardado en dataset_eb_tess_labeled_batch_1391.parquet (50000 filas)\n",
      "📝 Procesando batch #1393...\n",
      "✅ Batch #1393 guardado en dataset_eb_tess_labeled_batch_1392.parquet (50000 filas)\n",
      "📝 Procesando batch #1394...\n",
      "✅ Batch #1394 guardado en dataset_eb_tess_labeled_batch_1393.parquet (50000 filas)\n",
      "📝 Procesando batch #1395...\n",
      "✅ Batch #1395 guardado en dataset_eb_tess_labeled_batch_1394.parquet (50000 filas)\n",
      "📝 Procesando batch #1396...\n",
      "✅ Batch #1396 guardado en dataset_eb_tess_labeled_batch_1395.parquet (50000 filas)\n",
      "📝 Procesando batch #1397...\n",
      "✅ Batch #1397 guardado en dataset_eb_tess_labeled_batch_1396.parquet (50000 filas)\n",
      "📝 Procesando batch #1398...\n",
      "✅ Batch #1398 guardado en dataset_eb_tess_labeled_batch_1397.parquet (50000 filas)\n",
      "📝 Procesando batch #1399...\n",
      "✅ Batch #1399 guardado en dataset_eb_tess_labeled_batch_1398.parquet (50000 filas)\n",
      "📝 Procesando batch #1400...\n",
      "✅ Batch #1400 guardado en dataset_eb_tess_labeled_batch_1399.parquet (48576 filas)\n",
      "📝 Procesando batch #1401...\n",
      "✅ Batch #1401 guardado en dataset_eb_tess_labeled_batch_1400.parquet (50000 filas)\n",
      "📝 Procesando batch #1402...\n",
      "✅ Batch #1402 guardado en dataset_eb_tess_labeled_batch_1401.parquet (50000 filas)\n",
      "📝 Procesando batch #1403...\n",
      "✅ Batch #1403 guardado en dataset_eb_tess_labeled_batch_1402.parquet (50000 filas)\n",
      "📝 Procesando batch #1404...\n",
      "✅ Batch #1404 guardado en dataset_eb_tess_labeled_batch_1403.parquet (50000 filas)\n",
      "📝 Procesando batch #1405...\n",
      "✅ Batch #1405 guardado en dataset_eb_tess_labeled_batch_1404.parquet (50000 filas)\n",
      "📝 Procesando batch #1406...\n",
      "✅ Batch #1406 guardado en dataset_eb_tess_labeled_batch_1405.parquet (50000 filas)\n",
      "📝 Procesando batch #1407...\n",
      "✅ Batch #1407 guardado en dataset_eb_tess_labeled_batch_1406.parquet (50000 filas)\n",
      "📝 Procesando batch #1408...\n",
      "✅ Batch #1408 guardado en dataset_eb_tess_labeled_batch_1407.parquet (50000 filas)\n",
      "📝 Procesando batch #1409...\n",
      "✅ Batch #1409 guardado en dataset_eb_tess_labeled_batch_1408.parquet (50000 filas)\n",
      "📝 Procesando batch #1410...\n",
      "✅ Batch #1410 guardado en dataset_eb_tess_labeled_batch_1409.parquet (50000 filas)\n",
      "📝 Procesando batch #1411...\n",
      "✅ Batch #1411 guardado en dataset_eb_tess_labeled_batch_1410.parquet (50000 filas)\n",
      "📝 Procesando batch #1412...\n",
      "✅ Batch #1412 guardado en dataset_eb_tess_labeled_batch_1411.parquet (50000 filas)\n",
      "📝 Procesando batch #1413...\n",
      "✅ Batch #1413 guardado en dataset_eb_tess_labeled_batch_1412.parquet (50000 filas)\n",
      "📝 Procesando batch #1414...\n",
      "✅ Batch #1414 guardado en dataset_eb_tess_labeled_batch_1413.parquet (50000 filas)\n",
      "📝 Procesando batch #1415...\n",
      "✅ Batch #1415 guardado en dataset_eb_tess_labeled_batch_1414.parquet (50000 filas)\n",
      "📝 Procesando batch #1416...\n",
      "✅ Batch #1416 guardado en dataset_eb_tess_labeled_batch_1415.parquet (50000 filas)\n",
      "📝 Procesando batch #1417...\n",
      "✅ Batch #1417 guardado en dataset_eb_tess_labeled_batch_1416.parquet (50000 filas)\n",
      "📝 Procesando batch #1418...\n",
      "✅ Batch #1418 guardado en dataset_eb_tess_labeled_batch_1417.parquet (11041 filas)\n",
      "📝 Procesando batch #1419...\n",
      "✅ Batch #1419 guardado en dataset_eb_tess_labeled_batch_1418.parquet (50000 filas)\n",
      "📝 Procesando batch #1420...\n",
      "✅ Batch #1420 guardado en dataset_eb_tess_labeled_batch_1419.parquet (50000 filas)\n",
      "📝 Procesando batch #1421...\n",
      "✅ Batch #1421 guardado en dataset_eb_tess_labeled_batch_1420.parquet (50000 filas)\n",
      "📝 Procesando batch #1422...\n",
      "✅ Batch #1422 guardado en dataset_eb_tess_labeled_batch_1421.parquet (50000 filas)\n",
      "📝 Procesando batch #1423...\n",
      "✅ Batch #1423 guardado en dataset_eb_tess_labeled_batch_1422.parquet (50000 filas)\n",
      "📝 Procesando batch #1424...\n",
      "✅ Batch #1424 guardado en dataset_eb_tess_labeled_batch_1423.parquet (50000 filas)\n",
      "📝 Procesando batch #1425...\n",
      "✅ Batch #1425 guardado en dataset_eb_tess_labeled_batch_1424.parquet (50000 filas)\n",
      "📝 Procesando batch #1426...\n",
      "✅ Batch #1426 guardado en dataset_eb_tess_labeled_batch_1425.parquet (50000 filas)\n",
      "📝 Procesando batch #1427...\n",
      "✅ Batch #1427 guardado en dataset_eb_tess_labeled_batch_1426.parquet (50000 filas)\n",
      "📝 Procesando batch #1428...\n",
      "✅ Batch #1428 guardado en dataset_eb_tess_labeled_batch_1427.parquet (50000 filas)\n",
      "📝 Procesando batch #1429...\n",
      "✅ Batch #1429 guardado en dataset_eb_tess_labeled_batch_1428.parquet (50000 filas)\n",
      "📝 Procesando batch #1430...\n",
      "✅ Batch #1430 guardado en dataset_eb_tess_labeled_batch_1429.parquet (50000 filas)\n",
      "📝 Procesando batch #1431...\n",
      "✅ Batch #1431 guardado en dataset_eb_tess_labeled_batch_1430.parquet (50000 filas)\n",
      "📝 Procesando batch #1432...\n",
      "✅ Batch #1432 guardado en dataset_eb_tess_labeled_batch_1431.parquet (50000 filas)\n",
      "📝 Procesando batch #1433...\n",
      "✅ Batch #1433 guardado en dataset_eb_tess_labeled_batch_1432.parquet (50000 filas)\n",
      "📝 Procesando batch #1434...\n",
      "✅ Batch #1434 guardado en dataset_eb_tess_labeled_batch_1433.parquet (50000 filas)\n",
      "📝 Procesando batch #1435...\n",
      "✅ Batch #1435 guardado en dataset_eb_tess_labeled_batch_1434.parquet (50000 filas)\n",
      "📝 Procesando batch #1436...\n",
      "✅ Batch #1436 guardado en dataset_eb_tess_labeled_batch_1435.parquet (50000 filas)\n",
      "📝 Procesando batch #1437...\n",
      "✅ Batch #1437 guardado en dataset_eb_tess_labeled_batch_1436.parquet (50000 filas)\n",
      "📝 Procesando batch #1438...\n",
      "✅ Batch #1438 guardado en dataset_eb_tess_labeled_batch_1437.parquet (50000 filas)\n",
      "📝 Procesando batch #1439...\n",
      "✅ Batch #1439 guardado en dataset_eb_tess_labeled_batch_1438.parquet (48576 filas)\n",
      "📝 Procesando batch #1440...\n",
      "✅ Batch #1440 guardado en dataset_eb_tess_labeled_batch_1439.parquet (50000 filas)\n",
      "📝 Procesando batch #1441...\n",
      "✅ Batch #1441 guardado en dataset_eb_tess_labeled_batch_1440.parquet (50000 filas)\n",
      "📝 Procesando batch #1442...\n",
      "✅ Batch #1442 guardado en dataset_eb_tess_labeled_batch_1441.parquet (50000 filas)\n",
      "📝 Procesando batch #1443...\n",
      "✅ Batch #1443 guardado en dataset_eb_tess_labeled_batch_1442.parquet (50000 filas)\n",
      "📝 Procesando batch #1444...\n",
      "✅ Batch #1444 guardado en dataset_eb_tess_labeled_batch_1443.parquet (50000 filas)\n",
      "📝 Procesando batch #1445...\n",
      "✅ Batch #1445 guardado en dataset_eb_tess_labeled_batch_1444.parquet (50000 filas)\n",
      "📝 Procesando batch #1446...\n",
      "✅ Batch #1446 guardado en dataset_eb_tess_labeled_batch_1445.parquet (50000 filas)\n",
      "📝 Procesando batch #1447...\n",
      "✅ Batch #1447 guardado en dataset_eb_tess_labeled_batch_1446.parquet (50000 filas)\n",
      "📝 Procesando batch #1448...\n",
      "✅ Batch #1448 guardado en dataset_eb_tess_labeled_batch_1447.parquet (50000 filas)\n",
      "📝 Procesando batch #1449...\n",
      "✅ Batch #1449 guardado en dataset_eb_tess_labeled_batch_1448.parquet (50000 filas)\n",
      "📝 Procesando batch #1450...\n",
      "✅ Batch #1450 guardado en dataset_eb_tess_labeled_batch_1449.parquet (50000 filas)\n",
      "📝 Procesando batch #1451...\n",
      "✅ Batch #1451 guardado en dataset_eb_tess_labeled_batch_1450.parquet (50000 filas)\n",
      "📝 Procesando batch #1452...\n",
      "✅ Batch #1452 guardado en dataset_eb_tess_labeled_batch_1451.parquet (50000 filas)\n",
      "📝 Procesando batch #1453...\n",
      "✅ Batch #1453 guardado en dataset_eb_tess_labeled_batch_1452.parquet (50000 filas)\n",
      "📝 Procesando batch #1454...\n",
      "✅ Batch #1454 guardado en dataset_eb_tess_labeled_batch_1453.parquet (50000 filas)\n",
      "📝 Procesando batch #1455...\n",
      "✅ Batch #1455 guardado en dataset_eb_tess_labeled_batch_1454.parquet (50000 filas)\n",
      "📝 Procesando batch #1456...\n",
      "✅ Batch #1456 guardado en dataset_eb_tess_labeled_batch_1455.parquet (50000 filas)\n",
      "📝 Procesando batch #1457...\n",
      "✅ Batch #1457 guardado en dataset_eb_tess_labeled_batch_1456.parquet (50000 filas)\n",
      "📝 Procesando batch #1458...\n",
      "✅ Batch #1458 guardado en dataset_eb_tess_labeled_batch_1457.parquet (50000 filas)\n",
      "📝 Procesando batch #1459...\n",
      "✅ Batch #1459 guardado en dataset_eb_tess_labeled_batch_1458.parquet (50000 filas)\n",
      "📝 Procesando batch #1460...\n",
      "✅ Batch #1460 guardado en dataset_eb_tess_labeled_batch_1459.parquet (48576 filas)\n",
      "📝 Procesando batch #1461...\n",
      "✅ Batch #1461 guardado en dataset_eb_tess_labeled_batch_1460.parquet (50000 filas)\n",
      "📝 Procesando batch #1462...\n",
      "✅ Batch #1462 guardado en dataset_eb_tess_labeled_batch_1461.parquet (50000 filas)\n",
      "📝 Procesando batch #1463...\n",
      "✅ Batch #1463 guardado en dataset_eb_tess_labeled_batch_1462.parquet (50000 filas)\n",
      "📝 Procesando batch #1464...\n",
      "✅ Batch #1464 guardado en dataset_eb_tess_labeled_batch_1463.parquet (50000 filas)\n",
      "📝 Procesando batch #1465...\n",
      "✅ Batch #1465 guardado en dataset_eb_tess_labeled_batch_1464.parquet (50000 filas)\n",
      "📝 Procesando batch #1466...\n",
      "✅ Batch #1466 guardado en dataset_eb_tess_labeled_batch_1465.parquet (50000 filas)\n",
      "📝 Procesando batch #1467...\n",
      "✅ Batch #1467 guardado en dataset_eb_tess_labeled_batch_1466.parquet (50000 filas)\n",
      "📝 Procesando batch #1468...\n",
      "✅ Batch #1468 guardado en dataset_eb_tess_labeled_batch_1467.parquet (50000 filas)\n",
      "📝 Procesando batch #1469...\n",
      "✅ Batch #1469 guardado en dataset_eb_tess_labeled_batch_1468.parquet (50000 filas)\n",
      "📝 Procesando batch #1470...\n",
      "✅ Batch #1470 guardado en dataset_eb_tess_labeled_batch_1469.parquet (50000 filas)\n",
      "📝 Procesando batch #1471...\n",
      "✅ Batch #1471 guardado en dataset_eb_tess_labeled_batch_1470.parquet (50000 filas)\n",
      "📝 Procesando batch #1472...\n",
      "✅ Batch #1472 guardado en dataset_eb_tess_labeled_batch_1471.parquet (50000 filas)\n",
      "📝 Procesando batch #1473...\n",
      "✅ Batch #1473 guardado en dataset_eb_tess_labeled_batch_1472.parquet (50000 filas)\n",
      "📝 Procesando batch #1474...\n",
      "✅ Batch #1474 guardado en dataset_eb_tess_labeled_batch_1473.parquet (50000 filas)\n",
      "📝 Procesando batch #1475...\n",
      "✅ Batch #1475 guardado en dataset_eb_tess_labeled_batch_1474.parquet (50000 filas)\n",
      "📝 Procesando batch #1476...\n",
      "✅ Batch #1476 guardado en dataset_eb_tess_labeled_batch_1475.parquet (50000 filas)\n",
      "📝 Procesando batch #1477...\n",
      "✅ Batch #1477 guardado en dataset_eb_tess_labeled_batch_1476.parquet (50000 filas)\n",
      "📝 Procesando batch #1478...\n",
      "✅ Batch #1478 guardado en dataset_eb_tess_labeled_batch_1477.parquet (50000 filas)\n",
      "📝 Procesando batch #1479...\n",
      "✅ Batch #1479 guardado en dataset_eb_tess_labeled_batch_1478.parquet (50000 filas)\n",
      "📝 Procesando batch #1480...\n",
      "✅ Batch #1480 guardado en dataset_eb_tess_labeled_batch_1479.parquet (50000 filas)\n",
      "📝 Procesando batch #1481...\n",
      "✅ Batch #1481 guardado en dataset_eb_tess_labeled_batch_1480.parquet (48576 filas)\n",
      "📝 Procesando batch #1482...\n",
      "✅ Batch #1482 guardado en dataset_eb_tess_labeled_batch_1481.parquet (50000 filas)\n",
      "📝 Procesando batch #1483...\n",
      "✅ Batch #1483 guardado en dataset_eb_tess_labeled_batch_1482.parquet (50000 filas)\n",
      "📝 Procesando batch #1484...\n",
      "✅ Batch #1484 guardado en dataset_eb_tess_labeled_batch_1483.parquet (50000 filas)\n",
      "📝 Procesando batch #1485...\n",
      "✅ Batch #1485 guardado en dataset_eb_tess_labeled_batch_1484.parquet (50000 filas)\n",
      "📝 Procesando batch #1486...\n",
      "✅ Batch #1486 guardado en dataset_eb_tess_labeled_batch_1485.parquet (50000 filas)\n",
      "📝 Procesando batch #1487...\n",
      "✅ Batch #1487 guardado en dataset_eb_tess_labeled_batch_1486.parquet (50000 filas)\n",
      "📝 Procesando batch #1488...\n",
      "✅ Batch #1488 guardado en dataset_eb_tess_labeled_batch_1487.parquet (50000 filas)\n",
      "📝 Procesando batch #1489...\n",
      "✅ Batch #1489 guardado en dataset_eb_tess_labeled_batch_1488.parquet (50000 filas)\n",
      "📝 Procesando batch #1490...\n",
      "✅ Batch #1490 guardado en dataset_eb_tess_labeled_batch_1489.parquet (50000 filas)\n",
      "📝 Procesando batch #1491...\n",
      "✅ Batch #1491 guardado en dataset_eb_tess_labeled_batch_1490.parquet (50000 filas)\n",
      "📝 Procesando batch #1492...\n",
      "✅ Batch #1492 guardado en dataset_eb_tess_labeled_batch_1491.parquet (50000 filas)\n",
      "📝 Procesando batch #1493...\n",
      "✅ Batch #1493 guardado en dataset_eb_tess_labeled_batch_1492.parquet (50000 filas)\n",
      "📝 Procesando batch #1494...\n",
      "✅ Batch #1494 guardado en dataset_eb_tess_labeled_batch_1493.parquet (50000 filas)\n",
      "📝 Procesando batch #1495...\n",
      "✅ Batch #1495 guardado en dataset_eb_tess_labeled_batch_1494.parquet (50000 filas)\n",
      "📝 Procesando batch #1496...\n",
      "✅ Batch #1496 guardado en dataset_eb_tess_labeled_batch_1495.parquet (50000 filas)\n",
      "📝 Procesando batch #1497...\n",
      "✅ Batch #1497 guardado en dataset_eb_tess_labeled_batch_1496.parquet (50000 filas)\n",
      "📝 Procesando batch #1498...\n",
      "✅ Batch #1498 guardado en dataset_eb_tess_labeled_batch_1497.parquet (50000 filas)\n",
      "📝 Procesando batch #1499...\n",
      "✅ Batch #1499 guardado en dataset_eb_tess_labeled_batch_1498.parquet (50000 filas)\n",
      "📝 Procesando batch #1500...\n",
      "✅ Batch #1500 guardado en dataset_eb_tess_labeled_batch_1499.parquet (50000 filas)\n",
      "📝 Procesando batch #1501...\n",
      "✅ Batch #1501 guardado en dataset_eb_tess_labeled_batch_1500.parquet (50000 filas)\n",
      "📝 Procesando batch #1502...\n",
      "✅ Batch #1502 guardado en dataset_eb_tess_labeled_batch_1501.parquet (48576 filas)\n",
      "📝 Procesando batch #1503...\n",
      "✅ Batch #1503 guardado en dataset_eb_tess_labeled_batch_1502.parquet (50000 filas)\n",
      "📝 Procesando batch #1504...\n",
      "✅ Batch #1504 guardado en dataset_eb_tess_labeled_batch_1503.parquet (50000 filas)\n",
      "📝 Procesando batch #1505...\n",
      "✅ Batch #1505 guardado en dataset_eb_tess_labeled_batch_1504.parquet (50000 filas)\n",
      "📝 Procesando batch #1506...\n",
      "✅ Batch #1506 guardado en dataset_eb_tess_labeled_batch_1505.parquet (50000 filas)\n",
      "📝 Procesando batch #1507...\n",
      "✅ Batch #1507 guardado en dataset_eb_tess_labeled_batch_1506.parquet (50000 filas)\n",
      "📝 Procesando batch #1508...\n",
      "✅ Batch #1508 guardado en dataset_eb_tess_labeled_batch_1507.parquet (50000 filas)\n",
      "📝 Procesando batch #1509...\n",
      "✅ Batch #1509 guardado en dataset_eb_tess_labeled_batch_1508.parquet (50000 filas)\n",
      "📝 Procesando batch #1510...\n",
      "✅ Batch #1510 guardado en dataset_eb_tess_labeled_batch_1509.parquet (50000 filas)\n",
      "📝 Procesando batch #1511...\n",
      "✅ Batch #1511 guardado en dataset_eb_tess_labeled_batch_1510.parquet (50000 filas)\n",
      "📝 Procesando batch #1512...\n",
      "✅ Batch #1512 guardado en dataset_eb_tess_labeled_batch_1511.parquet (50000 filas)\n",
      "📝 Procesando batch #1513...\n",
      "✅ Batch #1513 guardado en dataset_eb_tess_labeled_batch_1512.parquet (50000 filas)\n",
      "📝 Procesando batch #1514...\n",
      "✅ Batch #1514 guardado en dataset_eb_tess_labeled_batch_1513.parquet (50000 filas)\n",
      "📝 Procesando batch #1515...\n",
      "✅ Batch #1515 guardado en dataset_eb_tess_labeled_batch_1514.parquet (50000 filas)\n",
      "📝 Procesando batch #1516...\n",
      "✅ Batch #1516 guardado en dataset_eb_tess_labeled_batch_1515.parquet (50000 filas)\n",
      "📝 Procesando batch #1517...\n",
      "✅ Batch #1517 guardado en dataset_eb_tess_labeled_batch_1516.parquet (50000 filas)\n",
      "📝 Procesando batch #1518...\n",
      "✅ Batch #1518 guardado en dataset_eb_tess_labeled_batch_1517.parquet (50000 filas)\n",
      "📝 Procesando batch #1519...\n",
      "✅ Batch #1519 guardado en dataset_eb_tess_labeled_batch_1518.parquet (50000 filas)\n",
      "📝 Procesando batch #1520...\n",
      "✅ Batch #1520 guardado en dataset_eb_tess_labeled_batch_1519.parquet (50000 filas)\n",
      "📝 Procesando batch #1521...\n",
      "✅ Batch #1521 guardado en dataset_eb_tess_labeled_batch_1520.parquet (50000 filas)\n",
      "📝 Procesando batch #1522...\n",
      "✅ Batch #1522 guardado en dataset_eb_tess_labeled_batch_1521.parquet (50000 filas)\n",
      "📝 Procesando batch #1523...\n",
      "✅ Batch #1523 guardado en dataset_eb_tess_labeled_batch_1522.parquet (48576 filas)\n",
      "📝 Procesando batch #1524...\n",
      "✅ Batch #1524 guardado en dataset_eb_tess_labeled_batch_1523.parquet (50000 filas)\n",
      "📝 Procesando batch #1525...\n",
      "✅ Batch #1525 guardado en dataset_eb_tess_labeled_batch_1524.parquet (50000 filas)\n",
      "📝 Procesando batch #1526...\n",
      "✅ Batch #1526 guardado en dataset_eb_tess_labeled_batch_1525.parquet (50000 filas)\n",
      "📝 Procesando batch #1527...\n",
      "✅ Batch #1527 guardado en dataset_eb_tess_labeled_batch_1526.parquet (50000 filas)\n",
      "📝 Procesando batch #1528...\n",
      "✅ Batch #1528 guardado en dataset_eb_tess_labeled_batch_1527.parquet (50000 filas)\n",
      "📝 Procesando batch #1529...\n",
      "✅ Batch #1529 guardado en dataset_eb_tess_labeled_batch_1528.parquet (50000 filas)\n",
      "📝 Procesando batch #1530...\n",
      "✅ Batch #1530 guardado en dataset_eb_tess_labeled_batch_1529.parquet (50000 filas)\n",
      "📝 Procesando batch #1531...\n",
      "✅ Batch #1531 guardado en dataset_eb_tess_labeled_batch_1530.parquet (50000 filas)\n",
      "📝 Procesando batch #1532...\n",
      "✅ Batch #1532 guardado en dataset_eb_tess_labeled_batch_1531.parquet (50000 filas)\n",
      "📝 Procesando batch #1533...\n",
      "✅ Batch #1533 guardado en dataset_eb_tess_labeled_batch_1532.parquet (50000 filas)\n",
      "📝 Procesando batch #1534...\n",
      "✅ Batch #1534 guardado en dataset_eb_tess_labeled_batch_1533.parquet (50000 filas)\n",
      "📝 Procesando batch #1535...\n",
      "✅ Batch #1535 guardado en dataset_eb_tess_labeled_batch_1534.parquet (50000 filas)\n",
      "📝 Procesando batch #1536...\n",
      "✅ Batch #1536 guardado en dataset_eb_tess_labeled_batch_1535.parquet (50000 filas)\n",
      "📝 Procesando batch #1537...\n",
      "✅ Batch #1537 guardado en dataset_eb_tess_labeled_batch_1536.parquet (50000 filas)\n",
      "📝 Procesando batch #1538...\n",
      "✅ Batch #1538 guardado en dataset_eb_tess_labeled_batch_1537.parquet (50000 filas)\n",
      "📝 Procesando batch #1539...\n",
      "✅ Batch #1539 guardado en dataset_eb_tess_labeled_batch_1538.parquet (50000 filas)\n",
      "📝 Procesando batch #1540...\n",
      "✅ Batch #1540 guardado en dataset_eb_tess_labeled_batch_1539.parquet (50000 filas)\n",
      "📝 Procesando batch #1541...\n",
      "✅ Batch #1541 guardado en dataset_eb_tess_labeled_batch_1540.parquet (50000 filas)\n",
      "📝 Procesando batch #1542...\n",
      "✅ Batch #1542 guardado en dataset_eb_tess_labeled_batch_1541.parquet (50000 filas)\n",
      "📝 Procesando batch #1543...\n",
      "✅ Batch #1543 guardado en dataset_eb_tess_labeled_batch_1542.parquet (50000 filas)\n",
      "📝 Procesando batch #1544...\n",
      "✅ Batch #1544 guardado en dataset_eb_tess_labeled_batch_1543.parquet (48576 filas)\n",
      "📝 Procesando batch #1545...\n",
      "✅ Batch #1545 guardado en dataset_eb_tess_labeled_batch_1544.parquet (50000 filas)\n",
      "📝 Procesando batch #1546...\n",
      "✅ Batch #1546 guardado en dataset_eb_tess_labeled_batch_1545.parquet (50000 filas)\n",
      "📝 Procesando batch #1547...\n",
      "✅ Batch #1547 guardado en dataset_eb_tess_labeled_batch_1546.parquet (50000 filas)\n",
      "📝 Procesando batch #1548...\n",
      "✅ Batch #1548 guardado en dataset_eb_tess_labeled_batch_1547.parquet (50000 filas)\n",
      "📝 Procesando batch #1549...\n",
      "✅ Batch #1549 guardado en dataset_eb_tess_labeled_batch_1548.parquet (50000 filas)\n",
      "📝 Procesando batch #1550...\n",
      "✅ Batch #1550 guardado en dataset_eb_tess_labeled_batch_1549.parquet (50000 filas)\n",
      "📝 Procesando batch #1551...\n",
      "✅ Batch #1551 guardado en dataset_eb_tess_labeled_batch_1550.parquet (50000 filas)\n",
      "📝 Procesando batch #1552...\n",
      "✅ Batch #1552 guardado en dataset_eb_tess_labeled_batch_1551.parquet (50000 filas)\n",
      "📝 Procesando batch #1553...\n",
      "✅ Batch #1553 guardado en dataset_eb_tess_labeled_batch_1552.parquet (50000 filas)\n",
      "📝 Procesando batch #1554...\n",
      "✅ Batch #1554 guardado en dataset_eb_tess_labeled_batch_1553.parquet (50000 filas)\n",
      "📝 Procesando batch #1555...\n",
      "✅ Batch #1555 guardado en dataset_eb_tess_labeled_batch_1554.parquet (50000 filas)\n",
      "📝 Procesando batch #1556...\n",
      "✅ Batch #1556 guardado en dataset_eb_tess_labeled_batch_1555.parquet (50000 filas)\n",
      "📝 Procesando batch #1557...\n",
      "✅ Batch #1557 guardado en dataset_eb_tess_labeled_batch_1556.parquet (50000 filas)\n",
      "📝 Procesando batch #1558...\n",
      "✅ Batch #1558 guardado en dataset_eb_tess_labeled_batch_1557.parquet (50000 filas)\n",
      "📝 Procesando batch #1559...\n",
      "✅ Batch #1559 guardado en dataset_eb_tess_labeled_batch_1558.parquet (50000 filas)\n",
      "📝 Procesando batch #1560...\n",
      "✅ Batch #1560 guardado en dataset_eb_tess_labeled_batch_1559.parquet (50000 filas)\n",
      "📝 Procesando batch #1561...\n",
      "✅ Batch #1561 guardado en dataset_eb_tess_labeled_batch_1560.parquet (50000 filas)\n",
      "📝 Procesando batch #1562...\n",
      "✅ Batch #1562 guardado en dataset_eb_tess_labeled_batch_1561.parquet (50000 filas)\n",
      "📝 Procesando batch #1563...\n",
      "✅ Batch #1563 guardado en dataset_eb_tess_labeled_batch_1562.parquet (50000 filas)\n",
      "📝 Procesando batch #1564...\n",
      "✅ Batch #1564 guardado en dataset_eb_tess_labeled_batch_1563.parquet (50000 filas)\n",
      "📝 Procesando batch #1565...\n",
      "✅ Batch #1565 guardado en dataset_eb_tess_labeled_batch_1564.parquet (48576 filas)\n",
      "📝 Procesando batch #1566...\n",
      "✅ Batch #1566 guardado en dataset_eb_tess_labeled_batch_1565.parquet (50000 filas)\n",
      "📝 Procesando batch #1567...\n",
      "✅ Batch #1567 guardado en dataset_eb_tess_labeled_batch_1566.parquet (50000 filas)\n",
      "📝 Procesando batch #1568...\n",
      "✅ Batch #1568 guardado en dataset_eb_tess_labeled_batch_1567.parquet (50000 filas)\n",
      "📝 Procesando batch #1569...\n",
      "✅ Batch #1569 guardado en dataset_eb_tess_labeled_batch_1568.parquet (50000 filas)\n",
      "📝 Procesando batch #1570...\n",
      "✅ Batch #1570 guardado en dataset_eb_tess_labeled_batch_1569.parquet (50000 filas)\n",
      "📝 Procesando batch #1571...\n",
      "✅ Batch #1571 guardado en dataset_eb_tess_labeled_batch_1570.parquet (9904 filas)\n",
      "📝 Procesando batch #1572...\n",
      "✅ Batch #1572 guardado en dataset_eb_tess_labeled_batch_1571.parquet (50000 filas)\n",
      "📝 Procesando batch #1573...\n",
      "✅ Batch #1573 guardado en dataset_eb_tess_labeled_batch_1572.parquet (50000 filas)\n",
      "📝 Procesando batch #1574...\n",
      "✅ Batch #1574 guardado en dataset_eb_tess_labeled_batch_1573.parquet (50000 filas)\n",
      "📝 Procesando batch #1575...\n",
      "✅ Batch #1575 guardado en dataset_eb_tess_labeled_batch_1574.parquet (50000 filas)\n",
      "📝 Procesando batch #1576...\n",
      "✅ Batch #1576 guardado en dataset_eb_tess_labeled_batch_1575.parquet (50000 filas)\n",
      "📝 Procesando batch #1577...\n",
      "✅ Batch #1577 guardado en dataset_eb_tess_labeled_batch_1576.parquet (50000 filas)\n",
      "📝 Procesando batch #1578...\n",
      "✅ Batch #1578 guardado en dataset_eb_tess_labeled_batch_1577.parquet (50000 filas)\n",
      "📝 Procesando batch #1579...\n",
      "✅ Batch #1579 guardado en dataset_eb_tess_labeled_batch_1578.parquet (50000 filas)\n",
      "📝 Procesando batch #1580...\n",
      "✅ Batch #1580 guardado en dataset_eb_tess_labeled_batch_1579.parquet (50000 filas)\n",
      "📝 Procesando batch #1581...\n",
      "✅ Batch #1581 guardado en dataset_eb_tess_labeled_batch_1580.parquet (50000 filas)\n",
      "📝 Procesando batch #1582...\n",
      "✅ Batch #1582 guardado en dataset_eb_tess_labeled_batch_1581.parquet (50000 filas)\n",
      "📝 Procesando batch #1583...\n",
      "✅ Batch #1583 guardado en dataset_eb_tess_labeled_batch_1582.parquet (50000 filas)\n",
      "📝 Procesando batch #1584...\n",
      "✅ Batch #1584 guardado en dataset_eb_tess_labeled_batch_1583.parquet (50000 filas)\n",
      "📝 Procesando batch #1585...\n",
      "✅ Batch #1585 guardado en dataset_eb_tess_labeled_batch_1584.parquet (50000 filas)\n",
      "📝 Procesando batch #1586...\n",
      "✅ Batch #1586 guardado en dataset_eb_tess_labeled_batch_1585.parquet (50000 filas)\n",
      "📝 Procesando batch #1587...\n",
      "✅ Batch #1587 guardado en dataset_eb_tess_labeled_batch_1586.parquet (50000 filas)\n",
      "📝 Procesando batch #1588...\n",
      "✅ Batch #1588 guardado en dataset_eb_tess_labeled_batch_1587.parquet (50000 filas)\n",
      "📝 Procesando batch #1589...\n",
      "✅ Batch #1589 guardado en dataset_eb_tess_labeled_batch_1588.parquet (50000 filas)\n",
      "📝 Procesando batch #1590...\n",
      "✅ Batch #1590 guardado en dataset_eb_tess_labeled_batch_1589.parquet (50000 filas)\n",
      "📝 Procesando batch #1591...\n",
      "✅ Batch #1591 guardado en dataset_eb_tess_labeled_batch_1590.parquet (50000 filas)\n",
      "📝 Procesando batch #1592...\n",
      "✅ Batch #1592 guardado en dataset_eb_tess_labeled_batch_1591.parquet (48576 filas)\n",
      "📝 Procesando batch #1593...\n",
      "✅ Batch #1593 guardado en dataset_eb_tess_labeled_batch_1592.parquet (50000 filas)\n",
      "📝 Procesando batch #1594...\n",
      "✅ Batch #1594 guardado en dataset_eb_tess_labeled_batch_1593.parquet (50000 filas)\n",
      "📝 Procesando batch #1595...\n",
      "✅ Batch #1595 guardado en dataset_eb_tess_labeled_batch_1594.parquet (50000 filas)\n",
      "📝 Procesando batch #1596...\n",
      "✅ Batch #1596 guardado en dataset_eb_tess_labeled_batch_1595.parquet (50000 filas)\n",
      "📝 Procesando batch #1597...\n",
      "✅ Batch #1597 guardado en dataset_eb_tess_labeled_batch_1596.parquet (50000 filas)\n",
      "📝 Procesando batch #1598...\n",
      "✅ Batch #1598 guardado en dataset_eb_tess_labeled_batch_1597.parquet (50000 filas)\n",
      "📝 Procesando batch #1599...\n",
      "✅ Batch #1599 guardado en dataset_eb_tess_labeled_batch_1598.parquet (50000 filas)\n",
      "📝 Procesando batch #1600...\n",
      "✅ Batch #1600 guardado en dataset_eb_tess_labeled_batch_1599.parquet (50000 filas)\n",
      "📝 Procesando batch #1601...\n",
      "✅ Batch #1601 guardado en dataset_eb_tess_labeled_batch_1600.parquet (50000 filas)\n",
      "📝 Procesando batch #1602...\n",
      "✅ Batch #1602 guardado en dataset_eb_tess_labeled_batch_1601.parquet (50000 filas)\n",
      "📝 Procesando batch #1603...\n",
      "✅ Batch #1603 guardado en dataset_eb_tess_labeled_batch_1602.parquet (50000 filas)\n",
      "📝 Procesando batch #1604...\n",
      "✅ Batch #1604 guardado en dataset_eb_tess_labeled_batch_1603.parquet (50000 filas)\n",
      "📝 Procesando batch #1605...\n",
      "✅ Batch #1605 guardado en dataset_eb_tess_labeled_batch_1604.parquet (50000 filas)\n",
      "📝 Procesando batch #1606...\n",
      "✅ Batch #1606 guardado en dataset_eb_tess_labeled_batch_1605.parquet (50000 filas)\n",
      "📝 Procesando batch #1607...\n",
      "✅ Batch #1607 guardado en dataset_eb_tess_labeled_batch_1606.parquet (50000 filas)\n",
      "📝 Procesando batch #1608...\n",
      "✅ Batch #1608 guardado en dataset_eb_tess_labeled_batch_1607.parquet (50000 filas)\n",
      "📝 Procesando batch #1609...\n",
      "✅ Batch #1609 guardado en dataset_eb_tess_labeled_batch_1608.parquet (50000 filas)\n",
      "📝 Procesando batch #1610...\n",
      "✅ Batch #1610 guardado en dataset_eb_tess_labeled_batch_1609.parquet (50000 filas)\n",
      "📝 Procesando batch #1611...\n",
      "✅ Batch #1611 guardado en dataset_eb_tess_labeled_batch_1610.parquet (50000 filas)\n",
      "📝 Procesando batch #1612...\n",
      "✅ Batch #1612 guardado en dataset_eb_tess_labeled_batch_1611.parquet (50000 filas)\n",
      "📝 Procesando batch #1613...\n",
      "✅ Batch #1613 guardado en dataset_eb_tess_labeled_batch_1612.parquet (48576 filas)\n",
      "📝 Procesando batch #1614...\n",
      "✅ Batch #1614 guardado en dataset_eb_tess_labeled_batch_1613.parquet (50000 filas)\n",
      "📝 Procesando batch #1615...\n",
      "✅ Batch #1615 guardado en dataset_eb_tess_labeled_batch_1614.parquet (50000 filas)\n",
      "📝 Procesando batch #1616...\n",
      "✅ Batch #1616 guardado en dataset_eb_tess_labeled_batch_1615.parquet (50000 filas)\n",
      "📝 Procesando batch #1617...\n",
      "✅ Batch #1617 guardado en dataset_eb_tess_labeled_batch_1616.parquet (50000 filas)\n",
      "📝 Procesando batch #1618...\n",
      "✅ Batch #1618 guardado en dataset_eb_tess_labeled_batch_1617.parquet (50000 filas)\n",
      "📝 Procesando batch #1619...\n",
      "✅ Batch #1619 guardado en dataset_eb_tess_labeled_batch_1618.parquet (50000 filas)\n",
      "📝 Procesando batch #1620...\n",
      "✅ Batch #1620 guardado en dataset_eb_tess_labeled_batch_1619.parquet (50000 filas)\n",
      "📝 Procesando batch #1621...\n",
      "✅ Batch #1621 guardado en dataset_eb_tess_labeled_batch_1620.parquet (50000 filas)\n",
      "📝 Procesando batch #1622...\n",
      "✅ Batch #1622 guardado en dataset_eb_tess_labeled_batch_1621.parquet (50000 filas)\n",
      "📝 Procesando batch #1623...\n",
      "✅ Batch #1623 guardado en dataset_eb_tess_labeled_batch_1622.parquet (50000 filas)\n",
      "📝 Procesando batch #1624...\n",
      "✅ Batch #1624 guardado en dataset_eb_tess_labeled_batch_1623.parquet (50000 filas)\n",
      "📝 Procesando batch #1625...\n",
      "✅ Batch #1625 guardado en dataset_eb_tess_labeled_batch_1624.parquet (50000 filas)\n",
      "📝 Procesando batch #1626...\n",
      "✅ Batch #1626 guardado en dataset_eb_tess_labeled_batch_1625.parquet (50000 filas)\n",
      "📝 Procesando batch #1627...\n",
      "✅ Batch #1627 guardado en dataset_eb_tess_labeled_batch_1626.parquet (50000 filas)\n",
      "📝 Procesando batch #1628...\n",
      "✅ Batch #1628 guardado en dataset_eb_tess_labeled_batch_1627.parquet (50000 filas)\n",
      "📝 Procesando batch #1629...\n",
      "✅ Batch #1629 guardado en dataset_eb_tess_labeled_batch_1628.parquet (50000 filas)\n",
      "📝 Procesando batch #1630...\n",
      "✅ Batch #1630 guardado en dataset_eb_tess_labeled_batch_1629.parquet (50000 filas)\n",
      "📝 Procesando batch #1631...\n",
      "✅ Batch #1631 guardado en dataset_eb_tess_labeled_batch_1630.parquet (50000 filas)\n",
      "📝 Procesando batch #1632...\n",
      "✅ Batch #1632 guardado en dataset_eb_tess_labeled_batch_1631.parquet (50000 filas)\n",
      "📝 Procesando batch #1633...\n",
      "✅ Batch #1633 guardado en dataset_eb_tess_labeled_batch_1632.parquet (50000 filas)\n",
      "📝 Procesando batch #1634...\n",
      "✅ Batch #1634 guardado en dataset_eb_tess_labeled_batch_1633.parquet (48576 filas)\n",
      "📝 Procesando batch #1635...\n",
      "✅ Batch #1635 guardado en dataset_eb_tess_labeled_batch_1634.parquet (50000 filas)\n",
      "📝 Procesando batch #1636...\n",
      "✅ Batch #1636 guardado en dataset_eb_tess_labeled_batch_1635.parquet (50000 filas)\n",
      "📝 Procesando batch #1637...\n",
      "✅ Batch #1637 guardado en dataset_eb_tess_labeled_batch_1636.parquet (50000 filas)\n",
      "📝 Procesando batch #1638...\n",
      "✅ Batch #1638 guardado en dataset_eb_tess_labeled_batch_1637.parquet (50000 filas)\n",
      "📝 Procesando batch #1639...\n",
      "✅ Batch #1639 guardado en dataset_eb_tess_labeled_batch_1638.parquet (50000 filas)\n",
      "📝 Procesando batch #1640...\n",
      "✅ Batch #1640 guardado en dataset_eb_tess_labeled_batch_1639.parquet (50000 filas)\n",
      "📝 Procesando batch #1641...\n",
      "✅ Batch #1641 guardado en dataset_eb_tess_labeled_batch_1640.parquet (50000 filas)\n",
      "📝 Procesando batch #1642...\n",
      "✅ Batch #1642 guardado en dataset_eb_tess_labeled_batch_1641.parquet (50000 filas)\n",
      "📝 Procesando batch #1643...\n",
      "✅ Batch #1643 guardado en dataset_eb_tess_labeled_batch_1642.parquet (50000 filas)\n",
      "📝 Procesando batch #1644...\n",
      "✅ Batch #1644 guardado en dataset_eb_tess_labeled_batch_1643.parquet (50000 filas)\n",
      "📝 Procesando batch #1645...\n",
      "✅ Batch #1645 guardado en dataset_eb_tess_labeled_batch_1644.parquet (50000 filas)\n",
      "📝 Procesando batch #1646...\n",
      "✅ Batch #1646 guardado en dataset_eb_tess_labeled_batch_1645.parquet (50000 filas)\n",
      "📝 Procesando batch #1647...\n",
      "✅ Batch #1647 guardado en dataset_eb_tess_labeled_batch_1646.parquet (50000 filas)\n",
      "📝 Procesando batch #1648...\n",
      "✅ Batch #1648 guardado en dataset_eb_tess_labeled_batch_1647.parquet (50000 filas)\n",
      "📝 Procesando batch #1649...\n",
      "✅ Batch #1649 guardado en dataset_eb_tess_labeled_batch_1648.parquet (50000 filas)\n",
      "📝 Procesando batch #1650...\n",
      "✅ Batch #1650 guardado en dataset_eb_tess_labeled_batch_1649.parquet (50000 filas)\n",
      "📝 Procesando batch #1651...\n",
      "✅ Batch #1651 guardado en dataset_eb_tess_labeled_batch_1650.parquet (50000 filas)\n",
      "📝 Procesando batch #1652...\n",
      "✅ Batch #1652 guardado en dataset_eb_tess_labeled_batch_1651.parquet (50000 filas)\n",
      "📝 Procesando batch #1653...\n",
      "✅ Batch #1653 guardado en dataset_eb_tess_labeled_batch_1652.parquet (50000 filas)\n",
      "📝 Procesando batch #1654...\n",
      "✅ Batch #1654 guardado en dataset_eb_tess_labeled_batch_1653.parquet (50000 filas)\n",
      "📝 Procesando batch #1655...\n",
      "✅ Batch #1655 guardado en dataset_eb_tess_labeled_batch_1654.parquet (48576 filas)\n",
      "📝 Procesando batch #1656...\n",
      "✅ Batch #1656 guardado en dataset_eb_tess_labeled_batch_1655.parquet (50000 filas)\n",
      "📝 Procesando batch #1657...\n",
      "✅ Batch #1657 guardado en dataset_eb_tess_labeled_batch_1656.parquet (50000 filas)\n",
      "📝 Procesando batch #1658...\n",
      "✅ Batch #1658 guardado en dataset_eb_tess_labeled_batch_1657.parquet (50000 filas)\n",
      "📝 Procesando batch #1659...\n",
      "✅ Batch #1659 guardado en dataset_eb_tess_labeled_batch_1658.parquet (50000 filas)\n",
      "📝 Procesando batch #1660...\n",
      "✅ Batch #1660 guardado en dataset_eb_tess_labeled_batch_1659.parquet (50000 filas)\n",
      "📝 Procesando batch #1661...\n",
      "✅ Batch #1661 guardado en dataset_eb_tess_labeled_batch_1660.parquet (50000 filas)\n",
      "📝 Procesando batch #1662...\n",
      "✅ Batch #1662 guardado en dataset_eb_tess_labeled_batch_1661.parquet (50000 filas)\n",
      "📝 Procesando batch #1663...\n",
      "✅ Batch #1663 guardado en dataset_eb_tess_labeled_batch_1662.parquet (50000 filas)\n",
      "📝 Procesando batch #1664...\n",
      "✅ Batch #1664 guardado en dataset_eb_tess_labeled_batch_1663.parquet (50000 filas)\n",
      "📝 Procesando batch #1665...\n",
      "✅ Batch #1665 guardado en dataset_eb_tess_labeled_batch_1664.parquet (50000 filas)\n",
      "📝 Procesando batch #1666...\n",
      "✅ Batch #1666 guardado en dataset_eb_tess_labeled_batch_1665.parquet (50000 filas)\n",
      "📝 Procesando batch #1667...\n",
      "✅ Batch #1667 guardado en dataset_eb_tess_labeled_batch_1666.parquet (50000 filas)\n",
      "📝 Procesando batch #1668...\n",
      "✅ Batch #1668 guardado en dataset_eb_tess_labeled_batch_1667.parquet (50000 filas)\n",
      "📝 Procesando batch #1669...\n",
      "✅ Batch #1669 guardado en dataset_eb_tess_labeled_batch_1668.parquet (50000 filas)\n",
      "📝 Procesando batch #1670...\n",
      "✅ Batch #1670 guardado en dataset_eb_tess_labeled_batch_1669.parquet (50000 filas)\n",
      "📝 Procesando batch #1671...\n",
      "✅ Batch #1671 guardado en dataset_eb_tess_labeled_batch_1670.parquet (50000 filas)\n",
      "📝 Procesando batch #1672...\n",
      "✅ Batch #1672 guardado en dataset_eb_tess_labeled_batch_1671.parquet (50000 filas)\n",
      "📝 Procesando batch #1673...\n",
      "✅ Batch #1673 guardado en dataset_eb_tess_labeled_batch_1672.parquet (50000 filas)\n",
      "📝 Procesando batch #1674...\n",
      "✅ Batch #1674 guardado en dataset_eb_tess_labeled_batch_1673.parquet (50000 filas)\n",
      "📝 Procesando batch #1675...\n",
      "✅ Batch #1675 guardado en dataset_eb_tess_labeled_batch_1674.parquet (50000 filas)\n",
      "📝 Procesando batch #1676...\n",
      "✅ Batch #1676 guardado en dataset_eb_tess_labeled_batch_1675.parquet (48576 filas)\n",
      "📝 Procesando batch #1677...\n",
      "✅ Batch #1677 guardado en dataset_eb_tess_labeled_batch_1676.parquet (50000 filas)\n",
      "📝 Procesando batch #1678...\n",
      "✅ Batch #1678 guardado en dataset_eb_tess_labeled_batch_1677.parquet (50000 filas)\n",
      "📝 Procesando batch #1679...\n",
      "✅ Batch #1679 guardado en dataset_eb_tess_labeled_batch_1678.parquet (50000 filas)\n",
      "📝 Procesando batch #1680...\n",
      "✅ Batch #1680 guardado en dataset_eb_tess_labeled_batch_1679.parquet (50000 filas)\n",
      "📝 Procesando batch #1681...\n",
      "✅ Batch #1681 guardado en dataset_eb_tess_labeled_batch_1680.parquet (50000 filas)\n",
      "📝 Procesando batch #1682...\n",
      "✅ Batch #1682 guardado en dataset_eb_tess_labeled_batch_1681.parquet (50000 filas)\n",
      "📝 Procesando batch #1683...\n",
      "✅ Batch #1683 guardado en dataset_eb_tess_labeled_batch_1682.parquet (50000 filas)\n",
      "📝 Procesando batch #1684...\n",
      "✅ Batch #1684 guardado en dataset_eb_tess_labeled_batch_1683.parquet (50000 filas)\n",
      "📝 Procesando batch #1685...\n",
      "✅ Batch #1685 guardado en dataset_eb_tess_labeled_batch_1684.parquet (50000 filas)\n",
      "📝 Procesando batch #1686...\n",
      "✅ Batch #1686 guardado en dataset_eb_tess_labeled_batch_1685.parquet (50000 filas)\n",
      "📝 Procesando batch #1687...\n",
      "✅ Batch #1687 guardado en dataset_eb_tess_labeled_batch_1686.parquet (50000 filas)\n",
      "📝 Procesando batch #1688...\n",
      "✅ Batch #1688 guardado en dataset_eb_tess_labeled_batch_1687.parquet (50000 filas)\n",
      "📝 Procesando batch #1689...\n",
      "✅ Batch #1689 guardado en dataset_eb_tess_labeled_batch_1688.parquet (50000 filas)\n",
      "📝 Procesando batch #1690...\n",
      "✅ Batch #1690 guardado en dataset_eb_tess_labeled_batch_1689.parquet (50000 filas)\n",
      "📝 Procesando batch #1691...\n",
      "✅ Batch #1691 guardado en dataset_eb_tess_labeled_batch_1690.parquet (50000 filas)\n",
      "📝 Procesando batch #1692...\n",
      "✅ Batch #1692 guardado en dataset_eb_tess_labeled_batch_1691.parquet (50000 filas)\n",
      "📝 Procesando batch #1693...\n",
      "✅ Batch #1693 guardado en dataset_eb_tess_labeled_batch_1692.parquet (50000 filas)\n",
      "📝 Procesando batch #1694...\n",
      "✅ Batch #1694 guardado en dataset_eb_tess_labeled_batch_1693.parquet (50000 filas)\n",
      "📝 Procesando batch #1695...\n",
      "✅ Batch #1695 guardado en dataset_eb_tess_labeled_batch_1694.parquet (50000 filas)\n",
      "📝 Procesando batch #1696...\n",
      "✅ Batch #1696 guardado en dataset_eb_tess_labeled_batch_1695.parquet (50000 filas)\n",
      "📝 Procesando batch #1697...\n",
      "✅ Batch #1697 guardado en dataset_eb_tess_labeled_batch_1696.parquet (48576 filas)\n",
      "📝 Procesando batch #1698...\n",
      "✅ Batch #1698 guardado en dataset_eb_tess_labeled_batch_1697.parquet (50000 filas)\n",
      "📝 Procesando batch #1699...\n",
      "✅ Batch #1699 guardado en dataset_eb_tess_labeled_batch_1698.parquet (50000 filas)\n",
      "📝 Procesando batch #1700...\n",
      "✅ Batch #1700 guardado en dataset_eb_tess_labeled_batch_1699.parquet (50000 filas)\n",
      "📝 Procesando batch #1701...\n",
      "✅ Batch #1701 guardado en dataset_eb_tess_labeled_batch_1700.parquet (50000 filas)\n",
      "📝 Procesando batch #1702...\n",
      "✅ Batch #1702 guardado en dataset_eb_tess_labeled_batch_1701.parquet (50000 filas)\n",
      "📝 Procesando batch #1703...\n",
      "✅ Batch #1703 guardado en dataset_eb_tess_labeled_batch_1702.parquet (50000 filas)\n",
      "📝 Procesando batch #1704...\n",
      "✅ Batch #1704 guardado en dataset_eb_tess_labeled_batch_1703.parquet (50000 filas)\n",
      "📝 Procesando batch #1705...\n",
      "✅ Batch #1705 guardado en dataset_eb_tess_labeled_batch_1704.parquet (50000 filas)\n",
      "📝 Procesando batch #1706...\n",
      "✅ Batch #1706 guardado en dataset_eb_tess_labeled_batch_1705.parquet (50000 filas)\n",
      "📝 Procesando batch #1707...\n",
      "✅ Batch #1707 guardado en dataset_eb_tess_labeled_batch_1706.parquet (50000 filas)\n",
      "📝 Procesando batch #1708...\n",
      "✅ Batch #1708 guardado en dataset_eb_tess_labeled_batch_1707.parquet (50000 filas)\n",
      "📝 Procesando batch #1709...\n",
      "✅ Batch #1709 guardado en dataset_eb_tess_labeled_batch_1708.parquet (50000 filas)\n",
      "📝 Procesando batch #1710...\n",
      "✅ Batch #1710 guardado en dataset_eb_tess_labeled_batch_1709.parquet (50000 filas)\n",
      "📝 Procesando batch #1711...\n",
      "✅ Batch #1711 guardado en dataset_eb_tess_labeled_batch_1710.parquet (50000 filas)\n",
      "📝 Procesando batch #1712...\n",
      "✅ Batch #1712 guardado en dataset_eb_tess_labeled_batch_1711.parquet (50000 filas)\n",
      "📝 Procesando batch #1713...\n",
      "✅ Batch #1713 guardado en dataset_eb_tess_labeled_batch_1712.parquet (50000 filas)\n",
      "📝 Procesando batch #1714...\n",
      "✅ Batch #1714 guardado en dataset_eb_tess_labeled_batch_1713.parquet (50000 filas)\n",
      "📝 Procesando batch #1715...\n",
      "✅ Batch #1715 guardado en dataset_eb_tess_labeled_batch_1714.parquet (50000 filas)\n",
      "📝 Procesando batch #1716...\n",
      "✅ Batch #1716 guardado en dataset_eb_tess_labeled_batch_1715.parquet (50000 filas)\n",
      "📝 Procesando batch #1717...\n",
      "✅ Batch #1717 guardado en dataset_eb_tess_labeled_batch_1716.parquet (50000 filas)\n",
      "📝 Procesando batch #1718...\n",
      "✅ Batch #1718 guardado en dataset_eb_tess_labeled_batch_1717.parquet (48576 filas)\n",
      "📝 Procesando batch #1719...\n",
      "✅ Batch #1719 guardado en dataset_eb_tess_labeled_batch_1718.parquet (50000 filas)\n",
      "📝 Procesando batch #1720...\n",
      "✅ Batch #1720 guardado en dataset_eb_tess_labeled_batch_1719.parquet (50000 filas)\n",
      "📝 Procesando batch #1721...\n",
      "✅ Batch #1721 guardado en dataset_eb_tess_labeled_batch_1720.parquet (50000 filas)\n",
      "📝 Procesando batch #1722...\n",
      "✅ Batch #1722 guardado en dataset_eb_tess_labeled_batch_1721.parquet (50000 filas)\n",
      "📝 Procesando batch #1723...\n",
      "✅ Batch #1723 guardado en dataset_eb_tess_labeled_batch_1722.parquet (50000 filas)\n",
      "📝 Procesando batch #1724...\n",
      "✅ Batch #1724 guardado en dataset_eb_tess_labeled_batch_1723.parquet (50000 filas)\n",
      "📝 Procesando batch #1725...\n",
      "✅ Batch #1725 guardado en dataset_eb_tess_labeled_batch_1724.parquet (50000 filas)\n",
      "📝 Procesando batch #1726...\n",
      "✅ Batch #1726 guardado en dataset_eb_tess_labeled_batch_1725.parquet (50000 filas)\n",
      "📝 Procesando batch #1727...\n",
      "✅ Batch #1727 guardado en dataset_eb_tess_labeled_batch_1726.parquet (50000 filas)\n",
      "📝 Procesando batch #1728...\n",
      "✅ Batch #1728 guardado en dataset_eb_tess_labeled_batch_1727.parquet (34653 filas)\n",
      "📝 Procesando batch #1729...\n",
      "✅ Batch #1729 guardado en dataset_eb_tess_labeled_batch_1728.parquet (50000 filas)\n",
      "📝 Procesando batch #1730...\n",
      "✅ Batch #1730 guardado en dataset_eb_tess_labeled_batch_1729.parquet (50000 filas)\n",
      "📝 Procesando batch #1731...\n",
      "✅ Batch #1731 guardado en dataset_eb_tess_labeled_batch_1730.parquet (50000 filas)\n",
      "📝 Procesando batch #1732...\n",
      "✅ Batch #1732 guardado en dataset_eb_tess_labeled_batch_1731.parquet (50000 filas)\n",
      "📝 Procesando batch #1733...\n",
      "✅ Batch #1733 guardado en dataset_eb_tess_labeled_batch_1732.parquet (50000 filas)\n",
      "📝 Procesando batch #1734...\n",
      "✅ Batch #1734 guardado en dataset_eb_tess_labeled_batch_1733.parquet (50000 filas)\n",
      "📝 Procesando batch #1735...\n",
      "✅ Batch #1735 guardado en dataset_eb_tess_labeled_batch_1734.parquet (50000 filas)\n",
      "📝 Procesando batch #1736...\n",
      "✅ Batch #1736 guardado en dataset_eb_tess_labeled_batch_1735.parquet (50000 filas)\n",
      "📝 Procesando batch #1737...\n",
      "✅ Batch #1737 guardado en dataset_eb_tess_labeled_batch_1736.parquet (50000 filas)\n",
      "📝 Procesando batch #1738...\n",
      "✅ Batch #1738 guardado en dataset_eb_tess_labeled_batch_1737.parquet (50000 filas)\n",
      "📝 Procesando batch #1739...\n",
      "✅ Batch #1739 guardado en dataset_eb_tess_labeled_batch_1738.parquet (50000 filas)\n",
      "📝 Procesando batch #1740...\n",
      "✅ Batch #1740 guardado en dataset_eb_tess_labeled_batch_1739.parquet (50000 filas)\n",
      "📝 Procesando batch #1741...\n",
      "✅ Batch #1741 guardado en dataset_eb_tess_labeled_batch_1740.parquet (50000 filas)\n",
      "📝 Procesando batch #1742...\n",
      "✅ Batch #1742 guardado en dataset_eb_tess_labeled_batch_1741.parquet (50000 filas)\n",
      "📝 Procesando batch #1743...\n",
      "✅ Batch #1743 guardado en dataset_eb_tess_labeled_batch_1742.parquet (50000 filas)\n",
      "📝 Procesando batch #1744...\n",
      "✅ Batch #1744 guardado en dataset_eb_tess_labeled_batch_1743.parquet (50000 filas)\n",
      "📝 Procesando batch #1745...\n",
      "✅ Batch #1745 guardado en dataset_eb_tess_labeled_batch_1744.parquet (50000 filas)\n",
      "📝 Procesando batch #1746...\n",
      "✅ Batch #1746 guardado en dataset_eb_tess_labeled_batch_1745.parquet (50000 filas)\n",
      "📝 Procesando batch #1747...\n",
      "✅ Batch #1747 guardado en dataset_eb_tess_labeled_batch_1746.parquet (50000 filas)\n",
      "📝 Procesando batch #1748...\n",
      "✅ Batch #1748 guardado en dataset_eb_tess_labeled_batch_1747.parquet (50000 filas)\n",
      "📝 Procesando batch #1749...\n",
      "✅ Batch #1749 guardado en dataset_eb_tess_labeled_batch_1748.parquet (48576 filas)\n",
      "📝 Procesando batch #1750...\n",
      "✅ Batch #1750 guardado en dataset_eb_tess_labeled_batch_1749.parquet (50000 filas)\n",
      "📝 Procesando batch #1751...\n",
      "✅ Batch #1751 guardado en dataset_eb_tess_labeled_batch_1750.parquet (50000 filas)\n",
      "📝 Procesando batch #1752...\n",
      "✅ Batch #1752 guardado en dataset_eb_tess_labeled_batch_1751.parquet (50000 filas)\n",
      "📝 Procesando batch #1753...\n",
      "✅ Batch #1753 guardado en dataset_eb_tess_labeled_batch_1752.parquet (50000 filas)\n",
      "📝 Procesando batch #1754...\n",
      "✅ Batch #1754 guardado en dataset_eb_tess_labeled_batch_1753.parquet (50000 filas)\n",
      "📝 Procesando batch #1755...\n",
      "✅ Batch #1755 guardado en dataset_eb_tess_labeled_batch_1754.parquet (50000 filas)\n",
      "📝 Procesando batch #1756...\n",
      "✅ Batch #1756 guardado en dataset_eb_tess_labeled_batch_1755.parquet (50000 filas)\n",
      "📝 Procesando batch #1757...\n",
      "✅ Batch #1757 guardado en dataset_eb_tess_labeled_batch_1756.parquet (50000 filas)\n",
      "📝 Procesando batch #1758...\n",
      "✅ Batch #1758 guardado en dataset_eb_tess_labeled_batch_1757.parquet (50000 filas)\n",
      "📝 Procesando batch #1759...\n",
      "✅ Batch #1759 guardado en dataset_eb_tess_labeled_batch_1758.parquet (50000 filas)\n",
      "📝 Procesando batch #1760...\n",
      "✅ Batch #1760 guardado en dataset_eb_tess_labeled_batch_1759.parquet (50000 filas)\n",
      "📝 Procesando batch #1761...\n",
      "✅ Batch #1761 guardado en dataset_eb_tess_labeled_batch_1760.parquet (50000 filas)\n",
      "📝 Procesando batch #1762...\n",
      "✅ Batch #1762 guardado en dataset_eb_tess_labeled_batch_1761.parquet (50000 filas)\n",
      "📝 Procesando batch #1763...\n",
      "✅ Batch #1763 guardado en dataset_eb_tess_labeled_batch_1762.parquet (50000 filas)\n",
      "📝 Procesando batch #1764...\n",
      "✅ Batch #1764 guardado en dataset_eb_tess_labeled_batch_1763.parquet (50000 filas)\n",
      "📝 Procesando batch #1765...\n",
      "✅ Batch #1765 guardado en dataset_eb_tess_labeled_batch_1764.parquet (50000 filas)\n",
      "📝 Procesando batch #1766...\n",
      "✅ Batch #1766 guardado en dataset_eb_tess_labeled_batch_1765.parquet (50000 filas)\n",
      "📝 Procesando batch #1767...\n",
      "✅ Batch #1767 guardado en dataset_eb_tess_labeled_batch_1766.parquet (50000 filas)\n",
      "📝 Procesando batch #1768...\n",
      "✅ Batch #1768 guardado en dataset_eb_tess_labeled_batch_1767.parquet (50000 filas)\n",
      "📝 Procesando batch #1769...\n",
      "✅ Batch #1769 guardado en dataset_eb_tess_labeled_batch_1768.parquet (50000 filas)\n",
      "📝 Procesando batch #1770...\n",
      "✅ Batch #1770 guardado en dataset_eb_tess_labeled_batch_1769.parquet (48576 filas)\n",
      "📝 Procesando batch #1771...\n",
      "✅ Batch #1771 guardado en dataset_eb_tess_labeled_batch_1770.parquet (50000 filas)\n",
      "📝 Procesando batch #1772...\n",
      "✅ Batch #1772 guardado en dataset_eb_tess_labeled_batch_1771.parquet (50000 filas)\n",
      "📝 Procesando batch #1773...\n",
      "✅ Batch #1773 guardado en dataset_eb_tess_labeled_batch_1772.parquet (50000 filas)\n",
      "📝 Procesando batch #1774...\n",
      "✅ Batch #1774 guardado en dataset_eb_tess_labeled_batch_1773.parquet (50000 filas)\n",
      "📝 Procesando batch #1775...\n",
      "✅ Batch #1775 guardado en dataset_eb_tess_labeled_batch_1774.parquet (50000 filas)\n",
      "📝 Procesando batch #1776...\n",
      "✅ Batch #1776 guardado en dataset_eb_tess_labeled_batch_1775.parquet (50000 filas)\n",
      "📝 Procesando batch #1777...\n",
      "✅ Batch #1777 guardado en dataset_eb_tess_labeled_batch_1776.parquet (50000 filas)\n",
      "📝 Procesando batch #1778...\n",
      "✅ Batch #1778 guardado en dataset_eb_tess_labeled_batch_1777.parquet (50000 filas)\n",
      "📝 Procesando batch #1779...\n",
      "✅ Batch #1779 guardado en dataset_eb_tess_labeled_batch_1778.parquet (50000 filas)\n",
      "📝 Procesando batch #1780...\n",
      "✅ Batch #1780 guardado en dataset_eb_tess_labeled_batch_1779.parquet (50000 filas)\n",
      "📝 Procesando batch #1781...\n",
      "✅ Batch #1781 guardado en dataset_eb_tess_labeled_batch_1780.parquet (50000 filas)\n",
      "📝 Procesando batch #1782...\n",
      "✅ Batch #1782 guardado en dataset_eb_tess_labeled_batch_1781.parquet (50000 filas)\n",
      "📝 Procesando batch #1783...\n",
      "✅ Batch #1783 guardado en dataset_eb_tess_labeled_batch_1782.parquet (50000 filas)\n",
      "📝 Procesando batch #1784...\n",
      "✅ Batch #1784 guardado en dataset_eb_tess_labeled_batch_1783.parquet (50000 filas)\n",
      "📝 Procesando batch #1785...\n",
      "✅ Batch #1785 guardado en dataset_eb_tess_labeled_batch_1784.parquet (50000 filas)\n",
      "📝 Procesando batch #1786...\n",
      "✅ Batch #1786 guardado en dataset_eb_tess_labeled_batch_1785.parquet (50000 filas)\n",
      "📝 Procesando batch #1787...\n",
      "✅ Batch #1787 guardado en dataset_eb_tess_labeled_batch_1786.parquet (50000 filas)\n",
      "📝 Procesando batch #1788...\n",
      "✅ Batch #1788 guardado en dataset_eb_tess_labeled_batch_1787.parquet (50000 filas)\n",
      "📝 Procesando batch #1789...\n",
      "✅ Batch #1789 guardado en dataset_eb_tess_labeled_batch_1788.parquet (50000 filas)\n",
      "📝 Procesando batch #1790...\n",
      "✅ Batch #1790 guardado en dataset_eb_tess_labeled_batch_1789.parquet (50000 filas)\n",
      "📝 Procesando batch #1791...\n",
      "✅ Batch #1791 guardado en dataset_eb_tess_labeled_batch_1790.parquet (48576 filas)\n",
      "📝 Procesando batch #1792...\n",
      "✅ Batch #1792 guardado en dataset_eb_tess_labeled_batch_1791.parquet (50000 filas)\n",
      "📝 Procesando batch #1793...\n",
      "✅ Batch #1793 guardado en dataset_eb_tess_labeled_batch_1792.parquet (50000 filas)\n",
      "📝 Procesando batch #1794...\n",
      "✅ Batch #1794 guardado en dataset_eb_tess_labeled_batch_1793.parquet (50000 filas)\n",
      "📝 Procesando batch #1795...\n",
      "✅ Batch #1795 guardado en dataset_eb_tess_labeled_batch_1794.parquet (50000 filas)\n",
      "📝 Procesando batch #1796...\n",
      "✅ Batch #1796 guardado en dataset_eb_tess_labeled_batch_1795.parquet (50000 filas)\n",
      "📝 Procesando batch #1797...\n",
      "✅ Batch #1797 guardado en dataset_eb_tess_labeled_batch_1796.parquet (50000 filas)\n",
      "📝 Procesando batch #1798...\n",
      "✅ Batch #1798 guardado en dataset_eb_tess_labeled_batch_1797.parquet (50000 filas)\n",
      "📝 Procesando batch #1799...\n",
      "✅ Batch #1799 guardado en dataset_eb_tess_labeled_batch_1798.parquet (50000 filas)\n",
      "📝 Procesando batch #1800...\n",
      "✅ Batch #1800 guardado en dataset_eb_tess_labeled_batch_1799.parquet (50000 filas)\n",
      "📝 Procesando batch #1801...\n",
      "✅ Batch #1801 guardado en dataset_eb_tess_labeled_batch_1800.parquet (50000 filas)\n",
      "📝 Procesando batch #1802...\n",
      "✅ Batch #1802 guardado en dataset_eb_tess_labeled_batch_1801.parquet (50000 filas)\n",
      "📝 Procesando batch #1803...\n",
      "✅ Batch #1803 guardado en dataset_eb_tess_labeled_batch_1802.parquet (50000 filas)\n",
      "📝 Procesando batch #1804...\n",
      "✅ Batch #1804 guardado en dataset_eb_tess_labeled_batch_1803.parquet (50000 filas)\n",
      "📝 Procesando batch #1805...\n",
      "✅ Batch #1805 guardado en dataset_eb_tess_labeled_batch_1804.parquet (50000 filas)\n",
      "📝 Procesando batch #1806...\n",
      "✅ Batch #1806 guardado en dataset_eb_tess_labeled_batch_1805.parquet (50000 filas)\n",
      "📝 Procesando batch #1807...\n",
      "✅ Batch #1807 guardado en dataset_eb_tess_labeled_batch_1806.parquet (50000 filas)\n",
      "📝 Procesando batch #1808...\n",
      "✅ Batch #1808 guardado en dataset_eb_tess_labeled_batch_1807.parquet (50000 filas)\n",
      "📝 Procesando batch #1809...\n",
      "✅ Batch #1809 guardado en dataset_eb_tess_labeled_batch_1808.parquet (50000 filas)\n",
      "📝 Procesando batch #1810...\n",
      "✅ Batch #1810 guardado en dataset_eb_tess_labeled_batch_1809.parquet (50000 filas)\n",
      "📝 Procesando batch #1811...\n",
      "✅ Batch #1811 guardado en dataset_eb_tess_labeled_batch_1810.parquet (50000 filas)\n",
      "📝 Procesando batch #1812...\n",
      "✅ Batch #1812 guardado en dataset_eb_tess_labeled_batch_1811.parquet (48576 filas)\n",
      "📝 Procesando batch #1813...\n",
      "✅ Batch #1813 guardado en dataset_eb_tess_labeled_batch_1812.parquet (50000 filas)\n",
      "📝 Procesando batch #1814...\n",
      "✅ Batch #1814 guardado en dataset_eb_tess_labeled_batch_1813.parquet (50000 filas)\n",
      "📝 Procesando batch #1815...\n",
      "✅ Batch #1815 guardado en dataset_eb_tess_labeled_batch_1814.parquet (50000 filas)\n",
      "📝 Procesando batch #1816...\n",
      "✅ Batch #1816 guardado en dataset_eb_tess_labeled_batch_1815.parquet (50000 filas)\n",
      "📝 Procesando batch #1817...\n",
      "✅ Batch #1817 guardado en dataset_eb_tess_labeled_batch_1816.parquet (50000 filas)\n",
      "📝 Procesando batch #1818...\n",
      "✅ Batch #1818 guardado en dataset_eb_tess_labeled_batch_1817.parquet (50000 filas)\n",
      "📝 Procesando batch #1819...\n",
      "✅ Batch #1819 guardado en dataset_eb_tess_labeled_batch_1818.parquet (50000 filas)\n",
      "📝 Procesando batch #1820...\n",
      "✅ Batch #1820 guardado en dataset_eb_tess_labeled_batch_1819.parquet (50000 filas)\n",
      "📝 Procesando batch #1821...\n",
      "✅ Batch #1821 guardado en dataset_eb_tess_labeled_batch_1820.parquet (50000 filas)\n",
      "📝 Procesando batch #1822...\n",
      "✅ Batch #1822 guardado en dataset_eb_tess_labeled_batch_1821.parquet (50000 filas)\n",
      "📝 Procesando batch #1823...\n",
      "✅ Batch #1823 guardado en dataset_eb_tess_labeled_batch_1822.parquet (50000 filas)\n",
      "📝 Procesando batch #1824...\n",
      "✅ Batch #1824 guardado en dataset_eb_tess_labeled_batch_1823.parquet (50000 filas)\n",
      "📝 Procesando batch #1825...\n",
      "✅ Batch #1825 guardado en dataset_eb_tess_labeled_batch_1824.parquet (50000 filas)\n",
      "📝 Procesando batch #1826...\n",
      "✅ Batch #1826 guardado en dataset_eb_tess_labeled_batch_1825.parquet (50000 filas)\n",
      "📝 Procesando batch #1827...\n",
      "✅ Batch #1827 guardado en dataset_eb_tess_labeled_batch_1826.parquet (50000 filas)\n",
      "📝 Procesando batch #1828...\n",
      "✅ Batch #1828 guardado en dataset_eb_tess_labeled_batch_1827.parquet (50000 filas)\n",
      "📝 Procesando batch #1829...\n",
      "✅ Batch #1829 guardado en dataset_eb_tess_labeled_batch_1828.parquet (50000 filas)\n",
      "📝 Procesando batch #1830...\n",
      "✅ Batch #1830 guardado en dataset_eb_tess_labeled_batch_1829.parquet (50000 filas)\n",
      "📝 Procesando batch #1831...\n",
      "✅ Batch #1831 guardado en dataset_eb_tess_labeled_batch_1830.parquet (50000 filas)\n",
      "📝 Procesando batch #1832...\n",
      "✅ Batch #1832 guardado en dataset_eb_tess_labeled_batch_1831.parquet (50000 filas)\n",
      "📝 Procesando batch #1833...\n",
      "✅ Batch #1833 guardado en dataset_eb_tess_labeled_batch_1832.parquet (48576 filas)\n",
      "📝 Procesando batch #1834...\n",
      "✅ Batch #1834 guardado en dataset_eb_tess_labeled_batch_1833.parquet (50000 filas)\n",
      "📝 Procesando batch #1835...\n",
      "✅ Batch #1835 guardado en dataset_eb_tess_labeled_batch_1834.parquet (50000 filas)\n",
      "📝 Procesando batch #1836...\n",
      "✅ Batch #1836 guardado en dataset_eb_tess_labeled_batch_1835.parquet (50000 filas)\n",
      "📝 Procesando batch #1837...\n",
      "✅ Batch #1837 guardado en dataset_eb_tess_labeled_batch_1836.parquet (50000 filas)\n",
      "📝 Procesando batch #1838...\n",
      "✅ Batch #1838 guardado en dataset_eb_tess_labeled_batch_1837.parquet (50000 filas)\n",
      "📝 Procesando batch #1839...\n",
      "✅ Batch #1839 guardado en dataset_eb_tess_labeled_batch_1838.parquet (50000 filas)\n",
      "📝 Procesando batch #1840...\n",
      "✅ Batch #1840 guardado en dataset_eb_tess_labeled_batch_1839.parquet (50000 filas)\n",
      "📝 Procesando batch #1841...\n",
      "✅ Batch #1841 guardado en dataset_eb_tess_labeled_batch_1840.parquet (50000 filas)\n",
      "📝 Procesando batch #1842...\n",
      "✅ Batch #1842 guardado en dataset_eb_tess_labeled_batch_1841.parquet (50000 filas)\n",
      "📝 Procesando batch #1843...\n",
      "✅ Batch #1843 guardado en dataset_eb_tess_labeled_batch_1842.parquet (50000 filas)\n",
      "📝 Procesando batch #1844...\n",
      "✅ Batch #1844 guardado en dataset_eb_tess_labeled_batch_1843.parquet (50000 filas)\n",
      "📝 Procesando batch #1845...\n",
      "✅ Batch #1845 guardado en dataset_eb_tess_labeled_batch_1844.parquet (50000 filas)\n",
      "📝 Procesando batch #1846...\n",
      "✅ Batch #1846 guardado en dataset_eb_tess_labeled_batch_1845.parquet (50000 filas)\n",
      "📝 Procesando batch #1847...\n",
      "✅ Batch #1847 guardado en dataset_eb_tess_labeled_batch_1846.parquet (50000 filas)\n",
      "📝 Procesando batch #1848...\n",
      "✅ Batch #1848 guardado en dataset_eb_tess_labeled_batch_1847.parquet (50000 filas)\n",
      "📝 Procesando batch #1849...\n",
      "✅ Batch #1849 guardado en dataset_eb_tess_labeled_batch_1848.parquet (50000 filas)\n",
      "📝 Procesando batch #1850...\n",
      "✅ Batch #1850 guardado en dataset_eb_tess_labeled_batch_1849.parquet (50000 filas)\n",
      "📝 Procesando batch #1851...\n",
      "✅ Batch #1851 guardado en dataset_eb_tess_labeled_batch_1850.parquet (50000 filas)\n",
      "📝 Procesando batch #1852...\n",
      "✅ Batch #1852 guardado en dataset_eb_tess_labeled_batch_1851.parquet (50000 filas)\n",
      "📝 Procesando batch #1853...\n",
      "✅ Batch #1853 guardado en dataset_eb_tess_labeled_batch_1852.parquet (50000 filas)\n",
      "📝 Procesando batch #1854...\n",
      "✅ Batch #1854 guardado en dataset_eb_tess_labeled_batch_1853.parquet (48576 filas)\n",
      "📝 Procesando batch #1855...\n",
      "✅ Batch #1855 guardado en dataset_eb_tess_labeled_batch_1854.parquet (50000 filas)\n",
      "📝 Procesando batch #1856...\n",
      "✅ Batch #1856 guardado en dataset_eb_tess_labeled_batch_1855.parquet (50000 filas)\n",
      "📝 Procesando batch #1857...\n",
      "✅ Batch #1857 guardado en dataset_eb_tess_labeled_batch_1856.parquet (50000 filas)\n",
      "📝 Procesando batch #1858...\n",
      "✅ Batch #1858 guardado en dataset_eb_tess_labeled_batch_1857.parquet (50000 filas)\n",
      "📝 Procesando batch #1859...\n",
      "✅ Batch #1859 guardado en dataset_eb_tess_labeled_batch_1858.parquet (50000 filas)\n",
      "📝 Procesando batch #1860...\n",
      "✅ Batch #1860 guardado en dataset_eb_tess_labeled_batch_1859.parquet (50000 filas)\n",
      "📝 Procesando batch #1861...\n",
      "✅ Batch #1861 guardado en dataset_eb_tess_labeled_batch_1860.parquet (50000 filas)\n",
      "📝 Procesando batch #1862...\n",
      "✅ Batch #1862 guardado en dataset_eb_tess_labeled_batch_1861.parquet (50000 filas)\n",
      "📝 Procesando batch #1863...\n",
      "✅ Batch #1863 guardado en dataset_eb_tess_labeled_batch_1862.parquet (50000 filas)\n",
      "📝 Procesando batch #1864...\n",
      "✅ Batch #1864 guardado en dataset_eb_tess_labeled_batch_1863.parquet (50000 filas)\n",
      "📝 Procesando batch #1865...\n",
      "✅ Batch #1865 guardado en dataset_eb_tess_labeled_batch_1864.parquet (50000 filas)\n",
      "📝 Procesando batch #1866...\n",
      "✅ Batch #1866 guardado en dataset_eb_tess_labeled_batch_1865.parquet (50000 filas)\n",
      "📝 Procesando batch #1867...\n",
      "✅ Batch #1867 guardado en dataset_eb_tess_labeled_batch_1866.parquet (50000 filas)\n",
      "📝 Procesando batch #1868...\n",
      "✅ Batch #1868 guardado en dataset_eb_tess_labeled_batch_1867.parquet (50000 filas)\n",
      "📝 Procesando batch #1869...\n",
      "✅ Batch #1869 guardado en dataset_eb_tess_labeled_batch_1868.parquet (50000 filas)\n",
      "📝 Procesando batch #1870...\n",
      "✅ Batch #1870 guardado en dataset_eb_tess_labeled_batch_1869.parquet (50000 filas)\n",
      "📝 Procesando batch #1871...\n",
      "✅ Batch #1871 guardado en dataset_eb_tess_labeled_batch_1870.parquet (50000 filas)\n",
      "📝 Procesando batch #1872...\n",
      "✅ Batch #1872 guardado en dataset_eb_tess_labeled_batch_1871.parquet (50000 filas)\n",
      "📝 Procesando batch #1873...\n",
      "✅ Batch #1873 guardado en dataset_eb_tess_labeled_batch_1872.parquet (50000 filas)\n",
      "📝 Procesando batch #1874...\n",
      "✅ Batch #1874 guardado en dataset_eb_tess_labeled_batch_1873.parquet (50000 filas)\n",
      "📝 Procesando batch #1875...\n",
      "✅ Batch #1875 guardado en dataset_eb_tess_labeled_batch_1874.parquet (48576 filas)\n",
      "📝 Procesando batch #1876...\n",
      "✅ Batch #1876 guardado en dataset_eb_tess_labeled_batch_1875.parquet (50000 filas)\n",
      "📝 Procesando batch #1877...\n",
      "✅ Batch #1877 guardado en dataset_eb_tess_labeled_batch_1876.parquet (50000 filas)\n",
      "📝 Procesando batch #1878...\n",
      "✅ Batch #1878 guardado en dataset_eb_tess_labeled_batch_1877.parquet (50000 filas)\n",
      "📝 Procesando batch #1879...\n",
      "✅ Batch #1879 guardado en dataset_eb_tess_labeled_batch_1878.parquet (50000 filas)\n",
      "📝 Procesando batch #1880...\n",
      "✅ Batch #1880 guardado en dataset_eb_tess_labeled_batch_1879.parquet (50000 filas)\n",
      "📝 Procesando batch #1881...\n",
      "✅ Batch #1881 guardado en dataset_eb_tess_labeled_batch_1880.parquet (50000 filas)\n",
      "📝 Procesando batch #1882...\n",
      "✅ Batch #1882 guardado en dataset_eb_tess_labeled_batch_1881.parquet (50000 filas)\n",
      "📝 Procesando batch #1883...\n",
      "✅ Batch #1883 guardado en dataset_eb_tess_labeled_batch_1882.parquet (50000 filas)\n",
      "📝 Procesando batch #1884...\n",
      "✅ Batch #1884 guardado en dataset_eb_tess_labeled_batch_1883.parquet (50000 filas)\n",
      "📝 Procesando batch #1885...\n",
      "✅ Batch #1885 guardado en dataset_eb_tess_labeled_batch_1884.parquet (31344 filas)\n",
      "📝 Procesando batch #1886...\n",
      "✅ Batch #1886 guardado en dataset_eb_tess_labeled_batch_1885.parquet (50000 filas)\n",
      "📝 Procesando batch #1887...\n",
      "✅ Batch #1887 guardado en dataset_eb_tess_labeled_batch_1886.parquet (50000 filas)\n",
      "📝 Procesando batch #1888...\n",
      "✅ Batch #1888 guardado en dataset_eb_tess_labeled_batch_1887.parquet (50000 filas)\n",
      "📝 Procesando batch #1889...\n",
      "✅ Batch #1889 guardado en dataset_eb_tess_labeled_batch_1888.parquet (50000 filas)\n",
      "📝 Procesando batch #1890...\n",
      "✅ Batch #1890 guardado en dataset_eb_tess_labeled_batch_1889.parquet (50000 filas)\n",
      "📝 Procesando batch #1891...\n",
      "✅ Batch #1891 guardado en dataset_eb_tess_labeled_batch_1890.parquet (50000 filas)\n",
      "📝 Procesando batch #1892...\n",
      "✅ Batch #1892 guardado en dataset_eb_tess_labeled_batch_1891.parquet (50000 filas)\n",
      "📝 Procesando batch #1893...\n",
      "✅ Batch #1893 guardado en dataset_eb_tess_labeled_batch_1892.parquet (50000 filas)\n",
      "📝 Procesando batch #1894...\n",
      "✅ Batch #1894 guardado en dataset_eb_tess_labeled_batch_1893.parquet (50000 filas)\n",
      "📝 Procesando batch #1895...\n",
      "✅ Batch #1895 guardado en dataset_eb_tess_labeled_batch_1894.parquet (50000 filas)\n",
      "📝 Procesando batch #1896...\n",
      "✅ Batch #1896 guardado en dataset_eb_tess_labeled_batch_1895.parquet (50000 filas)\n",
      "📝 Procesando batch #1897...\n",
      "✅ Batch #1897 guardado en dataset_eb_tess_labeled_batch_1896.parquet (50000 filas)\n",
      "📝 Procesando batch #1898...\n",
      "✅ Batch #1898 guardado en dataset_eb_tess_labeled_batch_1897.parquet (50000 filas)\n",
      "📝 Procesando batch #1899...\n",
      "✅ Batch #1899 guardado en dataset_eb_tess_labeled_batch_1898.parquet (50000 filas)\n",
      "📝 Procesando batch #1900...\n",
      "✅ Batch #1900 guardado en dataset_eb_tess_labeled_batch_1899.parquet (50000 filas)\n",
      "📝 Procesando batch #1901...\n",
      "✅ Batch #1901 guardado en dataset_eb_tess_labeled_batch_1900.parquet (50000 filas)\n",
      "📝 Procesando batch #1902...\n",
      "✅ Batch #1902 guardado en dataset_eb_tess_labeled_batch_1901.parquet (50000 filas)\n",
      "📝 Procesando batch #1903...\n",
      "✅ Batch #1903 guardado en dataset_eb_tess_labeled_batch_1902.parquet (50000 filas)\n",
      "📝 Procesando batch #1904...\n",
      "✅ Batch #1904 guardado en dataset_eb_tess_labeled_batch_1903.parquet (50000 filas)\n",
      "📝 Procesando batch #1905...\n",
      "✅ Batch #1905 guardado en dataset_eb_tess_labeled_batch_1904.parquet (50000 filas)\n",
      "📝 Procesando batch #1906...\n",
      "✅ Batch #1906 guardado en dataset_eb_tess_labeled_batch_1905.parquet (48576 filas)\n",
      "📝 Procesando batch #1907...\n",
      "✅ Batch #1907 guardado en dataset_eb_tess_labeled_batch_1906.parquet (50000 filas)\n",
      "📝 Procesando batch #1908...\n",
      "✅ Batch #1908 guardado en dataset_eb_tess_labeled_batch_1907.parquet (50000 filas)\n",
      "📝 Procesando batch #1909...\n",
      "✅ Batch #1909 guardado en dataset_eb_tess_labeled_batch_1908.parquet (50000 filas)\n",
      "📝 Procesando batch #1910...\n",
      "✅ Batch #1910 guardado en dataset_eb_tess_labeled_batch_1909.parquet (50000 filas)\n",
      "📝 Procesando batch #1911...\n",
      "✅ Batch #1911 guardado en dataset_eb_tess_labeled_batch_1910.parquet (50000 filas)\n",
      "📝 Procesando batch #1912...\n",
      "✅ Batch #1912 guardado en dataset_eb_tess_labeled_batch_1911.parquet (50000 filas)\n",
      "📝 Procesando batch #1913...\n",
      "✅ Batch #1913 guardado en dataset_eb_tess_labeled_batch_1912.parquet (50000 filas)\n",
      "📝 Procesando batch #1914...\n",
      "✅ Batch #1914 guardado en dataset_eb_tess_labeled_batch_1913.parquet (50000 filas)\n",
      "📝 Procesando batch #1915...\n",
      "✅ Batch #1915 guardado en dataset_eb_tess_labeled_batch_1914.parquet (50000 filas)\n",
      "📝 Procesando batch #1916...\n",
      "✅ Batch #1916 guardado en dataset_eb_tess_labeled_batch_1915.parquet (50000 filas)\n",
      "📝 Procesando batch #1917...\n",
      "✅ Batch #1917 guardado en dataset_eb_tess_labeled_batch_1916.parquet (50000 filas)\n",
      "📝 Procesando batch #1918...\n",
      "✅ Batch #1918 guardado en dataset_eb_tess_labeled_batch_1917.parquet (50000 filas)\n",
      "📝 Procesando batch #1919...\n",
      "✅ Batch #1919 guardado en dataset_eb_tess_labeled_batch_1918.parquet (50000 filas)\n",
      "📝 Procesando batch #1920...\n",
      "✅ Batch #1920 guardado en dataset_eb_tess_labeled_batch_1919.parquet (50000 filas)\n",
      "📝 Procesando batch #1921...\n",
      "✅ Batch #1921 guardado en dataset_eb_tess_labeled_batch_1920.parquet (50000 filas)\n",
      "📝 Procesando batch #1922...\n",
      "✅ Batch #1922 guardado en dataset_eb_tess_labeled_batch_1921.parquet (50000 filas)\n",
      "📝 Procesando batch #1923...\n",
      "✅ Batch #1923 guardado en dataset_eb_tess_labeled_batch_1922.parquet (50000 filas)\n",
      "📝 Procesando batch #1924...\n",
      "✅ Batch #1924 guardado en dataset_eb_tess_labeled_batch_1923.parquet (50000 filas)\n",
      "📝 Procesando batch #1925...\n",
      "✅ Batch #1925 guardado en dataset_eb_tess_labeled_batch_1924.parquet (50000 filas)\n",
      "📝 Procesando batch #1926...\n",
      "✅ Batch #1926 guardado en dataset_eb_tess_labeled_batch_1925.parquet (50000 filas)\n",
      "📝 Procesando batch #1927...\n",
      "✅ Batch #1927 guardado en dataset_eb_tess_labeled_batch_1926.parquet (48576 filas)\n",
      "📝 Procesando batch #1928...\n",
      "✅ Batch #1928 guardado en dataset_eb_tess_labeled_batch_1927.parquet (50000 filas)\n",
      "📝 Procesando batch #1929...\n",
      "✅ Batch #1929 guardado en dataset_eb_tess_labeled_batch_1928.parquet (50000 filas)\n",
      "📝 Procesando batch #1930...\n",
      "✅ Batch #1930 guardado en dataset_eb_tess_labeled_batch_1929.parquet (50000 filas)\n",
      "📝 Procesando batch #1931...\n",
      "✅ Batch #1931 guardado en dataset_eb_tess_labeled_batch_1930.parquet (50000 filas)\n",
      "📝 Procesando batch #1932...\n",
      "✅ Batch #1932 guardado en dataset_eb_tess_labeled_batch_1931.parquet (50000 filas)\n",
      "📝 Procesando batch #1933...\n",
      "✅ Batch #1933 guardado en dataset_eb_tess_labeled_batch_1932.parquet (50000 filas)\n",
      "📝 Procesando batch #1934...\n",
      "✅ Batch #1934 guardado en dataset_eb_tess_labeled_batch_1933.parquet (50000 filas)\n",
      "📝 Procesando batch #1935...\n",
      "✅ Batch #1935 guardado en dataset_eb_tess_labeled_batch_1934.parquet (50000 filas)\n",
      "📝 Procesando batch #1936...\n",
      "✅ Batch #1936 guardado en dataset_eb_tess_labeled_batch_1935.parquet (50000 filas)\n",
      "📝 Procesando batch #1937...\n",
      "✅ Batch #1937 guardado en dataset_eb_tess_labeled_batch_1936.parquet (50000 filas)\n",
      "📝 Procesando batch #1938...\n",
      "✅ Batch #1938 guardado en dataset_eb_tess_labeled_batch_1937.parquet (50000 filas)\n",
      "📝 Procesando batch #1939...\n",
      "✅ Batch #1939 guardado en dataset_eb_tess_labeled_batch_1938.parquet (50000 filas)\n",
      "📝 Procesando batch #1940...\n",
      "✅ Batch #1940 guardado en dataset_eb_tess_labeled_batch_1939.parquet (50000 filas)\n",
      "📝 Procesando batch #1941...\n",
      "✅ Batch #1941 guardado en dataset_eb_tess_labeled_batch_1940.parquet (50000 filas)\n",
      "📝 Procesando batch #1942...\n",
      "✅ Batch #1942 guardado en dataset_eb_tess_labeled_batch_1941.parquet (50000 filas)\n",
      "📝 Procesando batch #1943...\n",
      "✅ Batch #1943 guardado en dataset_eb_tess_labeled_batch_1942.parquet (50000 filas)\n",
      "📝 Procesando batch #1944...\n",
      "✅ Batch #1944 guardado en dataset_eb_tess_labeled_batch_1943.parquet (50000 filas)\n",
      "📝 Procesando batch #1945...\n",
      "✅ Batch #1945 guardado en dataset_eb_tess_labeled_batch_1944.parquet (50000 filas)\n",
      "📝 Procesando batch #1946...\n",
      "✅ Batch #1946 guardado en dataset_eb_tess_labeled_batch_1945.parquet (50000 filas)\n",
      "📝 Procesando batch #1947...\n",
      "✅ Batch #1947 guardado en dataset_eb_tess_labeled_batch_1946.parquet (50000 filas)\n",
      "📝 Procesando batch #1948...\n",
      "✅ Batch #1948 guardado en dataset_eb_tess_labeled_batch_1947.parquet (48576 filas)\n",
      "📝 Procesando batch #1949...\n",
      "✅ Batch #1949 guardado en dataset_eb_tess_labeled_batch_1948.parquet (50000 filas)\n",
      "📝 Procesando batch #1950...\n",
      "✅ Batch #1950 guardado en dataset_eb_tess_labeled_batch_1949.parquet (50000 filas)\n",
      "📝 Procesando batch #1951...\n",
      "✅ Batch #1951 guardado en dataset_eb_tess_labeled_batch_1950.parquet (50000 filas)\n",
      "📝 Procesando batch #1952...\n",
      "✅ Batch #1952 guardado en dataset_eb_tess_labeled_batch_1951.parquet (50000 filas)\n",
      "📝 Procesando batch #1953...\n",
      "✅ Batch #1953 guardado en dataset_eb_tess_labeled_batch_1952.parquet (50000 filas)\n",
      "📝 Procesando batch #1954...\n",
      "✅ Batch #1954 guardado en dataset_eb_tess_labeled_batch_1953.parquet (50000 filas)\n",
      "📝 Procesando batch #1955...\n",
      "✅ Batch #1955 guardado en dataset_eb_tess_labeled_batch_1954.parquet (50000 filas)\n",
      "📝 Procesando batch #1956...\n",
      "✅ Batch #1956 guardado en dataset_eb_tess_labeled_batch_1955.parquet (50000 filas)\n",
      "📝 Procesando batch #1957...\n",
      "✅ Batch #1957 guardado en dataset_eb_tess_labeled_batch_1956.parquet (50000 filas)\n",
      "📝 Procesando batch #1958...\n",
      "✅ Batch #1958 guardado en dataset_eb_tess_labeled_batch_1957.parquet (50000 filas)\n",
      "📝 Procesando batch #1959...\n",
      "✅ Batch #1959 guardado en dataset_eb_tess_labeled_batch_1958.parquet (50000 filas)\n",
      "📝 Procesando batch #1960...\n",
      "✅ Batch #1960 guardado en dataset_eb_tess_labeled_batch_1959.parquet (50000 filas)\n",
      "📝 Procesando batch #1961...\n",
      "✅ Batch #1961 guardado en dataset_eb_tess_labeled_batch_1960.parquet (50000 filas)\n",
      "📝 Procesando batch #1962...\n",
      "✅ Batch #1962 guardado en dataset_eb_tess_labeled_batch_1961.parquet (50000 filas)\n",
      "📝 Procesando batch #1963...\n",
      "✅ Batch #1963 guardado en dataset_eb_tess_labeled_batch_1962.parquet (50000 filas)\n",
      "📝 Procesando batch #1964...\n",
      "✅ Batch #1964 guardado en dataset_eb_tess_labeled_batch_1963.parquet (50000 filas)\n",
      "📝 Procesando batch #1965...\n",
      "✅ Batch #1965 guardado en dataset_eb_tess_labeled_batch_1964.parquet (50000 filas)\n",
      "📝 Procesando batch #1966...\n",
      "✅ Batch #1966 guardado en dataset_eb_tess_labeled_batch_1965.parquet (50000 filas)\n",
      "📝 Procesando batch #1967...\n",
      "✅ Batch #1967 guardado en dataset_eb_tess_labeled_batch_1966.parquet (50000 filas)\n",
      "📝 Procesando batch #1968...\n",
      "✅ Batch #1968 guardado en dataset_eb_tess_labeled_batch_1967.parquet (50000 filas)\n",
      "📝 Procesando batch #1969...\n",
      "✅ Batch #1969 guardado en dataset_eb_tess_labeled_batch_1968.parquet (48576 filas)\n",
      "📝 Procesando batch #1970...\n",
      "✅ Batch #1970 guardado en dataset_eb_tess_labeled_batch_1969.parquet (50000 filas)\n",
      "📝 Procesando batch #1971...\n",
      "✅ Batch #1971 guardado en dataset_eb_tess_labeled_batch_1970.parquet (50000 filas)\n",
      "📝 Procesando batch #1972...\n",
      "✅ Batch #1972 guardado en dataset_eb_tess_labeled_batch_1971.parquet (50000 filas)\n",
      "📝 Procesando batch #1973...\n",
      "✅ Batch #1973 guardado en dataset_eb_tess_labeled_batch_1972.parquet (50000 filas)\n",
      "📝 Procesando batch #1974...\n",
      "✅ Batch #1974 guardado en dataset_eb_tess_labeled_batch_1973.parquet (50000 filas)\n",
      "📝 Procesando batch #1975...\n",
      "✅ Batch #1975 guardado en dataset_eb_tess_labeled_batch_1974.parquet (50000 filas)\n",
      "📝 Procesando batch #1976...\n",
      "✅ Batch #1976 guardado en dataset_eb_tess_labeled_batch_1975.parquet (50000 filas)\n",
      "📝 Procesando batch #1977...\n",
      "✅ Batch #1977 guardado en dataset_eb_tess_labeled_batch_1976.parquet (50000 filas)\n",
      "📝 Procesando batch #1978...\n",
      "✅ Batch #1978 guardado en dataset_eb_tess_labeled_batch_1977.parquet (50000 filas)\n",
      "📝 Procesando batch #1979...\n",
      "✅ Batch #1979 guardado en dataset_eb_tess_labeled_batch_1978.parquet (50000 filas)\n",
      "📝 Procesando batch #1980...\n",
      "✅ Batch #1980 guardado en dataset_eb_tess_labeled_batch_1979.parquet (50000 filas)\n",
      "📝 Procesando batch #1981...\n",
      "✅ Batch #1981 guardado en dataset_eb_tess_labeled_batch_1980.parquet (50000 filas)\n",
      "📝 Procesando batch #1982...\n",
      "✅ Batch #1982 guardado en dataset_eb_tess_labeled_batch_1981.parquet (50000 filas)\n",
      "📝 Procesando batch #1983...\n",
      "✅ Batch #1983 guardado en dataset_eb_tess_labeled_batch_1982.parquet (50000 filas)\n",
      "📝 Procesando batch #1984...\n",
      "✅ Batch #1984 guardado en dataset_eb_tess_labeled_batch_1983.parquet (50000 filas)\n",
      "📝 Procesando batch #1985...\n",
      "✅ Batch #1985 guardado en dataset_eb_tess_labeled_batch_1984.parquet (50000 filas)\n",
      "📝 Procesando batch #1986...\n",
      "✅ Batch #1986 guardado en dataset_eb_tess_labeled_batch_1985.parquet (50000 filas)\n",
      "📝 Procesando batch #1987...\n",
      "✅ Batch #1987 guardado en dataset_eb_tess_labeled_batch_1986.parquet (50000 filas)\n",
      "📝 Procesando batch #1988...\n",
      "✅ Batch #1988 guardado en dataset_eb_tess_labeled_batch_1987.parquet (50000 filas)\n",
      "📝 Procesando batch #1989...\n",
      "✅ Batch #1989 guardado en dataset_eb_tess_labeled_batch_1988.parquet (50000 filas)\n",
      "📝 Procesando batch #1990...\n",
      "✅ Batch #1990 guardado en dataset_eb_tess_labeled_batch_1989.parquet (48576 filas)\n",
      "📝 Procesando batch #1991...\n",
      "✅ Batch #1991 guardado en dataset_eb_tess_labeled_batch_1990.parquet (50000 filas)\n",
      "📝 Procesando batch #1992...\n",
      "✅ Batch #1992 guardado en dataset_eb_tess_labeled_batch_1991.parquet (50000 filas)\n",
      "📝 Procesando batch #1993...\n",
      "✅ Batch #1993 guardado en dataset_eb_tess_labeled_batch_1992.parquet (50000 filas)\n",
      "📝 Procesando batch #1994...\n",
      "✅ Batch #1994 guardado en dataset_eb_tess_labeled_batch_1993.parquet (50000 filas)\n",
      "📝 Procesando batch #1995...\n",
      "✅ Batch #1995 guardado en dataset_eb_tess_labeled_batch_1994.parquet (50000 filas)\n",
      "📝 Procesando batch #1996...\n",
      "✅ Batch #1996 guardado en dataset_eb_tess_labeled_batch_1995.parquet (50000 filas)\n",
      "📝 Procesando batch #1997...\n",
      "✅ Batch #1997 guardado en dataset_eb_tess_labeled_batch_1996.parquet (50000 filas)\n",
      "📝 Procesando batch #1998...\n",
      "✅ Batch #1998 guardado en dataset_eb_tess_labeled_batch_1997.parquet (50000 filas)\n",
      "📝 Procesando batch #1999...\n",
      "✅ Batch #1999 guardado en dataset_eb_tess_labeled_batch_1998.parquet (50000 filas)\n",
      "📝 Procesando batch #2000...\n",
      "✅ Batch #2000 guardado en dataset_eb_tess_labeled_batch_1999.parquet (50000 filas)\n",
      "📝 Procesando batch #2001...\n",
      "✅ Batch #2001 guardado en dataset_eb_tess_labeled_batch_2000.parquet (50000 filas)\n",
      "📝 Procesando batch #2002...\n",
      "✅ Batch #2002 guardado en dataset_eb_tess_labeled_batch_2001.parquet (50000 filas)\n",
      "📝 Procesando batch #2003...\n",
      "✅ Batch #2003 guardado en dataset_eb_tess_labeled_batch_2002.parquet (50000 filas)\n",
      "📝 Procesando batch #2004...\n",
      "✅ Batch #2004 guardado en dataset_eb_tess_labeled_batch_2003.parquet (50000 filas)\n",
      "📝 Procesando batch #2005...\n",
      "✅ Batch #2005 guardado en dataset_eb_tess_labeled_batch_2004.parquet (50000 filas)\n",
      "📝 Procesando batch #2006...\n",
      "✅ Batch #2006 guardado en dataset_eb_tess_labeled_batch_2005.parquet (50000 filas)\n",
      "📝 Procesando batch #2007...\n",
      "✅ Batch #2007 guardado en dataset_eb_tess_labeled_batch_2006.parquet (50000 filas)\n",
      "📝 Procesando batch #2008...\n",
      "✅ Batch #2008 guardado en dataset_eb_tess_labeled_batch_2007.parquet (50000 filas)\n",
      "📝 Procesando batch #2009...\n",
      "✅ Batch #2009 guardado en dataset_eb_tess_labeled_batch_2008.parquet (50000 filas)\n",
      "📝 Procesando batch #2010...\n",
      "✅ Batch #2010 guardado en dataset_eb_tess_labeled_batch_2009.parquet (50000 filas)\n",
      "📝 Procesando batch #2011...\n",
      "✅ Batch #2011 guardado en dataset_eb_tess_labeled_batch_2010.parquet (48576 filas)\n",
      "📝 Procesando batch #2012...\n",
      "✅ Batch #2012 guardado en dataset_eb_tess_labeled_batch_2011.parquet (50000 filas)\n",
      "📝 Procesando batch #2013...\n",
      "✅ Batch #2013 guardado en dataset_eb_tess_labeled_batch_2012.parquet (50000 filas)\n",
      "📝 Procesando batch #2014...\n",
      "✅ Batch #2014 guardado en dataset_eb_tess_labeled_batch_2013.parquet (50000 filas)\n",
      "📝 Procesando batch #2015...\n",
      "✅ Batch #2015 guardado en dataset_eb_tess_labeled_batch_2014.parquet (50000 filas)\n",
      "📝 Procesando batch #2016...\n",
      "✅ Batch #2016 guardado en dataset_eb_tess_labeled_batch_2015.parquet (50000 filas)\n",
      "📝 Procesando batch #2017...\n",
      "✅ Batch #2017 guardado en dataset_eb_tess_labeled_batch_2016.parquet (50000 filas)\n",
      "📝 Procesando batch #2018...\n",
      "✅ Batch #2018 guardado en dataset_eb_tess_labeled_batch_2017.parquet (50000 filas)\n",
      "📝 Procesando batch #2019...\n",
      "✅ Batch #2019 guardado en dataset_eb_tess_labeled_batch_2018.parquet (50000 filas)\n",
      "📝 Procesando batch #2020...\n",
      "✅ Batch #2020 guardado en dataset_eb_tess_labeled_batch_2019.parquet (50000 filas)\n",
      "📝 Procesando batch #2021...\n",
      "✅ Batch #2021 guardado en dataset_eb_tess_labeled_batch_2020.parquet (50000 filas)\n",
      "📝 Procesando batch #2022...\n",
      "✅ Batch #2022 guardado en dataset_eb_tess_labeled_batch_2021.parquet (50000 filas)\n",
      "📝 Procesando batch #2023...\n",
      "✅ Batch #2023 guardado en dataset_eb_tess_labeled_batch_2022.parquet (50000 filas)\n",
      "📝 Procesando batch #2024...\n",
      "✅ Batch #2024 guardado en dataset_eb_tess_labeled_batch_2023.parquet (50000 filas)\n",
      "📝 Procesando batch #2025...\n",
      "✅ Batch #2025 guardado en dataset_eb_tess_labeled_batch_2024.parquet (50000 filas)\n",
      "📝 Procesando batch #2026...\n",
      "✅ Batch #2026 guardado en dataset_eb_tess_labeled_batch_2025.parquet (50000 filas)\n",
      "📝 Procesando batch #2027...\n",
      "✅ Batch #2027 guardado en dataset_eb_tess_labeled_batch_2026.parquet (50000 filas)\n",
      "📝 Procesando batch #2028...\n",
      "✅ Batch #2028 guardado en dataset_eb_tess_labeled_batch_2027.parquet (50000 filas)\n",
      "📝 Procesando batch #2029...\n",
      "✅ Batch #2029 guardado en dataset_eb_tess_labeled_batch_2028.parquet (50000 filas)\n",
      "📝 Procesando batch #2030...\n",
      "✅ Batch #2030 guardado en dataset_eb_tess_labeled_batch_2029.parquet (50000 filas)\n",
      "📝 Procesando batch #2031...\n",
      "✅ Batch #2031 guardado en dataset_eb_tess_labeled_batch_2030.parquet (50000 filas)\n",
      "📝 Procesando batch #2032...\n",
      "✅ Batch #2032 guardado en dataset_eb_tess_labeled_batch_2031.parquet (48576 filas)\n",
      "📝 Procesando batch #2033...\n",
      "✅ Batch #2033 guardado en dataset_eb_tess_labeled_batch_2032.parquet (50000 filas)\n",
      "📝 Procesando batch #2034...\n",
      "✅ Batch #2034 guardado en dataset_eb_tess_labeled_batch_2033.parquet (50000 filas)\n",
      "📝 Procesando batch #2035...\n",
      "✅ Batch #2035 guardado en dataset_eb_tess_labeled_batch_2034.parquet (50000 filas)\n",
      "📝 Procesando batch #2036...\n",
      "✅ Batch #2036 guardado en dataset_eb_tess_labeled_batch_2035.parquet (50000 filas)\n",
      "📝 Procesando batch #2037...\n",
      "✅ Batch #2037 guardado en dataset_eb_tess_labeled_batch_2036.parquet (50000 filas)\n",
      "📝 Procesando batch #2038...\n",
      "✅ Batch #2038 guardado en dataset_eb_tess_labeled_batch_2037.parquet (50000 filas)\n",
      "📝 Procesando batch #2039...\n",
      "✅ Batch #2039 guardado en dataset_eb_tess_labeled_batch_2038.parquet (50000 filas)\n",
      "📝 Procesando batch #2040...\n",
      "✅ Batch #2040 guardado en dataset_eb_tess_labeled_batch_2039.parquet (50000 filas)\n",
      "📝 Procesando batch #2041...\n",
      "✅ Batch #2041 guardado en dataset_eb_tess_labeled_batch_2040.parquet (50000 filas)\n",
      "📝 Procesando batch #2042...\n",
      "✅ Batch #2042 guardado en dataset_eb_tess_labeled_batch_2041.parquet (50000 filas)\n",
      "📝 Procesando batch #2043...\n",
      "✅ Batch #2043 guardado en dataset_eb_tess_labeled_batch_2042.parquet (50000 filas)\n",
      "📝 Procesando batch #2044...\n",
      "✅ Batch #2044 guardado en dataset_eb_tess_labeled_batch_2043.parquet (50000 filas)\n",
      "📝 Procesando batch #2045...\n",
      "✅ Batch #2045 guardado en dataset_eb_tess_labeled_batch_2044.parquet (50000 filas)\n",
      "📝 Procesando batch #2046...\n",
      "✅ Batch #2046 guardado en dataset_eb_tess_labeled_batch_2045.parquet (50000 filas)\n",
      "📝 Procesando batch #2047...\n",
      "✅ Batch #2047 guardado en dataset_eb_tess_labeled_batch_2046.parquet (22946 filas)\n",
      "📝 Procesando batch #2048...\n",
      "✅ Batch #2048 guardado en dataset_eb_tess_labeled_batch_2047.parquet (50000 filas)\n",
      "📝 Procesando batch #2049...\n",
      "✅ Batch #2049 guardado en dataset_eb_tess_labeled_batch_2048.parquet (50000 filas)\n",
      "📝 Procesando batch #2050...\n",
      "✅ Batch #2050 guardado en dataset_eb_tess_labeled_batch_2049.parquet (50000 filas)\n",
      "📝 Procesando batch #2051...\n",
      "✅ Batch #2051 guardado en dataset_eb_tess_labeled_batch_2050.parquet (50000 filas)\n",
      "📝 Procesando batch #2052...\n",
      "✅ Batch #2052 guardado en dataset_eb_tess_labeled_batch_2051.parquet (50000 filas)\n",
      "📝 Procesando batch #2053...\n",
      "✅ Batch #2053 guardado en dataset_eb_tess_labeled_batch_2052.parquet (50000 filas)\n",
      "📝 Procesando batch #2054...\n",
      "✅ Batch #2054 guardado en dataset_eb_tess_labeled_batch_2053.parquet (50000 filas)\n",
      "📝 Procesando batch #2055...\n",
      "✅ Batch #2055 guardado en dataset_eb_tess_labeled_batch_2054.parquet (50000 filas)\n",
      "📝 Procesando batch #2056...\n",
      "✅ Batch #2056 guardado en dataset_eb_tess_labeled_batch_2055.parquet (50000 filas)\n",
      "📝 Procesando batch #2057...\n",
      "✅ Batch #2057 guardado en dataset_eb_tess_labeled_batch_2056.parquet (50000 filas)\n",
      "📝 Procesando batch #2058...\n",
      "✅ Batch #2058 guardado en dataset_eb_tess_labeled_batch_2057.parquet (50000 filas)\n",
      "📝 Procesando batch #2059...\n",
      "✅ Batch #2059 guardado en dataset_eb_tess_labeled_batch_2058.parquet (50000 filas)\n",
      "📝 Procesando batch #2060...\n",
      "✅ Batch #2060 guardado en dataset_eb_tess_labeled_batch_2059.parquet (50000 filas)\n",
      "📝 Procesando batch #2061...\n",
      "✅ Batch #2061 guardado en dataset_eb_tess_labeled_batch_2060.parquet (50000 filas)\n",
      "📝 Procesando batch #2062...\n",
      "✅ Batch #2062 guardado en dataset_eb_tess_labeled_batch_2061.parquet (50000 filas)\n",
      "📝 Procesando batch #2063...\n",
      "✅ Batch #2063 guardado en dataset_eb_tess_labeled_batch_2062.parquet (50000 filas)\n",
      "📝 Procesando batch #2064...\n",
      "✅ Batch #2064 guardado en dataset_eb_tess_labeled_batch_2063.parquet (50000 filas)\n",
      "📝 Procesando batch #2065...\n",
      "✅ Batch #2065 guardado en dataset_eb_tess_labeled_batch_2064.parquet (50000 filas)\n",
      "📝 Procesando batch #2066...\n",
      "✅ Batch #2066 guardado en dataset_eb_tess_labeled_batch_2065.parquet (50000 filas)\n",
      "📝 Procesando batch #2067...\n",
      "✅ Batch #2067 guardado en dataset_eb_tess_labeled_batch_2066.parquet (50000 filas)\n",
      "📝 Procesando batch #2068...\n",
      "✅ Batch #2068 guardado en dataset_eb_tess_labeled_batch_2067.parquet (48576 filas)\n",
      "📝 Procesando batch #2069...\n",
      "✅ Batch #2069 guardado en dataset_eb_tess_labeled_batch_2068.parquet (50000 filas)\n",
      "📝 Procesando batch #2070...\n",
      "✅ Batch #2070 guardado en dataset_eb_tess_labeled_batch_2069.parquet (50000 filas)\n",
      "📝 Procesando batch #2071...\n",
      "✅ Batch #2071 guardado en dataset_eb_tess_labeled_batch_2070.parquet (50000 filas)\n",
      "📝 Procesando batch #2072...\n",
      "✅ Batch #2072 guardado en dataset_eb_tess_labeled_batch_2071.parquet (50000 filas)\n",
      "📝 Procesando batch #2073...\n",
      "✅ Batch #2073 guardado en dataset_eb_tess_labeled_batch_2072.parquet (50000 filas)\n",
      "📝 Procesando batch #2074...\n",
      "✅ Batch #2074 guardado en dataset_eb_tess_labeled_batch_2073.parquet (50000 filas)\n",
      "📝 Procesando batch #2075...\n",
      "✅ Batch #2075 guardado en dataset_eb_tess_labeled_batch_2074.parquet (50000 filas)\n",
      "📝 Procesando batch #2076...\n",
      "✅ Batch #2076 guardado en dataset_eb_tess_labeled_batch_2075.parquet (50000 filas)\n",
      "📝 Procesando batch #2077...\n",
      "✅ Batch #2077 guardado en dataset_eb_tess_labeled_batch_2076.parquet (50000 filas)\n",
      "📝 Procesando batch #2078...\n",
      "✅ Batch #2078 guardado en dataset_eb_tess_labeled_batch_2077.parquet (50000 filas)\n",
      "📝 Procesando batch #2079...\n",
      "✅ Batch #2079 guardado en dataset_eb_tess_labeled_batch_2078.parquet (50000 filas)\n",
      "📝 Procesando batch #2080...\n",
      "✅ Batch #2080 guardado en dataset_eb_tess_labeled_batch_2079.parquet (50000 filas)\n",
      "📝 Procesando batch #2081...\n",
      "✅ Batch #2081 guardado en dataset_eb_tess_labeled_batch_2080.parquet (50000 filas)\n",
      "📝 Procesando batch #2082...\n",
      "✅ Batch #2082 guardado en dataset_eb_tess_labeled_batch_2081.parquet (50000 filas)\n",
      "📝 Procesando batch #2083...\n",
      "✅ Batch #2083 guardado en dataset_eb_tess_labeled_batch_2082.parquet (50000 filas)\n",
      "📝 Procesando batch #2084...\n",
      "✅ Batch #2084 guardado en dataset_eb_tess_labeled_batch_2083.parquet (50000 filas)\n",
      "📝 Procesando batch #2085...\n",
      "✅ Batch #2085 guardado en dataset_eb_tess_labeled_batch_2084.parquet (50000 filas)\n",
      "📝 Procesando batch #2086...\n",
      "✅ Batch #2086 guardado en dataset_eb_tess_labeled_batch_2085.parquet (50000 filas)\n",
      "📝 Procesando batch #2087...\n",
      "✅ Batch #2087 guardado en dataset_eb_tess_labeled_batch_2086.parquet (50000 filas)\n",
      "📝 Procesando batch #2088...\n",
      "✅ Batch #2088 guardado en dataset_eb_tess_labeled_batch_2087.parquet (50000 filas)\n",
      "📝 Procesando batch #2089...\n",
      "✅ Batch #2089 guardado en dataset_eb_tess_labeled_batch_2088.parquet (48576 filas)\n",
      "📝 Procesando batch #2090...\n",
      "✅ Batch #2090 guardado en dataset_eb_tess_labeled_batch_2089.parquet (50000 filas)\n",
      "📝 Procesando batch #2091...\n",
      "✅ Batch #2091 guardado en dataset_eb_tess_labeled_batch_2090.parquet (50000 filas)\n",
      "📝 Procesando batch #2092...\n",
      "✅ Batch #2092 guardado en dataset_eb_tess_labeled_batch_2091.parquet (50000 filas)\n",
      "📝 Procesando batch #2093...\n",
      "✅ Batch #2093 guardado en dataset_eb_tess_labeled_batch_2092.parquet (50000 filas)\n",
      "📝 Procesando batch #2094...\n",
      "✅ Batch #2094 guardado en dataset_eb_tess_labeled_batch_2093.parquet (50000 filas)\n",
      "📝 Procesando batch #2095...\n",
      "✅ Batch #2095 guardado en dataset_eb_tess_labeled_batch_2094.parquet (50000 filas)\n",
      "📝 Procesando batch #2096...\n",
      "✅ Batch #2096 guardado en dataset_eb_tess_labeled_batch_2095.parquet (50000 filas)\n",
      "📝 Procesando batch #2097...\n",
      "✅ Batch #2097 guardado en dataset_eb_tess_labeled_batch_2096.parquet (50000 filas)\n",
      "📝 Procesando batch #2098...\n",
      "✅ Batch #2098 guardado en dataset_eb_tess_labeled_batch_2097.parquet (50000 filas)\n",
      "📝 Procesando batch #2099...\n",
      "✅ Batch #2099 guardado en dataset_eb_tess_labeled_batch_2098.parquet (50000 filas)\n",
      "📝 Procesando batch #2100...\n",
      "✅ Batch #2100 guardado en dataset_eb_tess_labeled_batch_2099.parquet (50000 filas)\n",
      "📝 Procesando batch #2101...\n",
      "✅ Batch #2101 guardado en dataset_eb_tess_labeled_batch_2100.parquet (50000 filas)\n",
      "📝 Procesando batch #2102...\n",
      "✅ Batch #2102 guardado en dataset_eb_tess_labeled_batch_2101.parquet (50000 filas)\n",
      "📝 Procesando batch #2103...\n",
      "✅ Batch #2103 guardado en dataset_eb_tess_labeled_batch_2102.parquet (50000 filas)\n",
      "📝 Procesando batch #2104...\n",
      "✅ Batch #2104 guardado en dataset_eb_tess_labeled_batch_2103.parquet (50000 filas)\n",
      "📝 Procesando batch #2105...\n",
      "✅ Batch #2105 guardado en dataset_eb_tess_labeled_batch_2104.parquet (50000 filas)\n",
      "📝 Procesando batch #2106...\n",
      "✅ Batch #2106 guardado en dataset_eb_tess_labeled_batch_2105.parquet (50000 filas)\n",
      "📝 Procesando batch #2107...\n",
      "✅ Batch #2107 guardado en dataset_eb_tess_labeled_batch_2106.parquet (50000 filas)\n",
      "📝 Procesando batch #2108...\n",
      "✅ Batch #2108 guardado en dataset_eb_tess_labeled_batch_2107.parquet (50000 filas)\n",
      "📝 Procesando batch #2109...\n",
      "✅ Batch #2109 guardado en dataset_eb_tess_labeled_batch_2108.parquet (50000 filas)\n",
      "📝 Procesando batch #2110...\n",
      "✅ Batch #2110 guardado en dataset_eb_tess_labeled_batch_2109.parquet (48576 filas)\n",
      "📝 Procesando batch #2111...\n",
      "✅ Batch #2111 guardado en dataset_eb_tess_labeled_batch_2110.parquet (50000 filas)\n",
      "📝 Procesando batch #2112...\n",
      "✅ Batch #2112 guardado en dataset_eb_tess_labeled_batch_2111.parquet (50000 filas)\n",
      "📝 Procesando batch #2113...\n",
      "✅ Batch #2113 guardado en dataset_eb_tess_labeled_batch_2112.parquet (50000 filas)\n",
      "📝 Procesando batch #2114...\n",
      "✅ Batch #2114 guardado en dataset_eb_tess_labeled_batch_2113.parquet (50000 filas)\n",
      "📝 Procesando batch #2115...\n",
      "✅ Batch #2115 guardado en dataset_eb_tess_labeled_batch_2114.parquet (50000 filas)\n",
      "📝 Procesando batch #2116...\n",
      "✅ Batch #2116 guardado en dataset_eb_tess_labeled_batch_2115.parquet (50000 filas)\n",
      "📝 Procesando batch #2117...\n",
      "✅ Batch #2117 guardado en dataset_eb_tess_labeled_batch_2116.parquet (50000 filas)\n",
      "📝 Procesando batch #2118...\n",
      "✅ Batch #2118 guardado en dataset_eb_tess_labeled_batch_2117.parquet (50000 filas)\n",
      "📝 Procesando batch #2119...\n",
      "✅ Batch #2119 guardado en dataset_eb_tess_labeled_batch_2118.parquet (50000 filas)\n",
      "📝 Procesando batch #2120...\n",
      "✅ Batch #2120 guardado en dataset_eb_tess_labeled_batch_2119.parquet (50000 filas)\n",
      "📝 Procesando batch #2121...\n",
      "✅ Batch #2121 guardado en dataset_eb_tess_labeled_batch_2120.parquet (50000 filas)\n",
      "📝 Procesando batch #2122...\n",
      "✅ Batch #2122 guardado en dataset_eb_tess_labeled_batch_2121.parquet (50000 filas)\n",
      "📝 Procesando batch #2123...\n",
      "✅ Batch #2123 guardado en dataset_eb_tess_labeled_batch_2122.parquet (50000 filas)\n",
      "📝 Procesando batch #2124...\n",
      "✅ Batch #2124 guardado en dataset_eb_tess_labeled_batch_2123.parquet (50000 filas)\n",
      "📝 Procesando batch #2125...\n",
      "✅ Batch #2125 guardado en dataset_eb_tess_labeled_batch_2124.parquet (50000 filas)\n",
      "📝 Procesando batch #2126...\n",
      "✅ Batch #2126 guardado en dataset_eb_tess_labeled_batch_2125.parquet (50000 filas)\n",
      "📝 Procesando batch #2127...\n",
      "✅ Batch #2127 guardado en dataset_eb_tess_labeled_batch_2126.parquet (50000 filas)\n",
      "📝 Procesando batch #2128...\n",
      "✅ Batch #2128 guardado en dataset_eb_tess_labeled_batch_2127.parquet (50000 filas)\n",
      "📝 Procesando batch #2129...\n",
      "✅ Batch #2129 guardado en dataset_eb_tess_labeled_batch_2128.parquet (50000 filas)\n",
      "📝 Procesando batch #2130...\n",
      "✅ Batch #2130 guardado en dataset_eb_tess_labeled_batch_2129.parquet (50000 filas)\n",
      "📝 Procesando batch #2131...\n",
      "✅ Batch #2131 guardado en dataset_eb_tess_labeled_batch_2130.parquet (48576 filas)\n",
      "📝 Procesando batch #2132...\n",
      "✅ Batch #2132 guardado en dataset_eb_tess_labeled_batch_2131.parquet (50000 filas)\n",
      "📝 Procesando batch #2133...\n",
      "✅ Batch #2133 guardado en dataset_eb_tess_labeled_batch_2132.parquet (50000 filas)\n",
      "📝 Procesando batch #2134...\n",
      "✅ Batch #2134 guardado en dataset_eb_tess_labeled_batch_2133.parquet (50000 filas)\n",
      "📝 Procesando batch #2135...\n",
      "✅ Batch #2135 guardado en dataset_eb_tess_labeled_batch_2134.parquet (50000 filas)\n",
      "📝 Procesando batch #2136...\n",
      "✅ Batch #2136 guardado en dataset_eb_tess_labeled_batch_2135.parquet (50000 filas)\n",
      "📝 Procesando batch #2137...\n",
      "✅ Batch #2137 guardado en dataset_eb_tess_labeled_batch_2136.parquet (50000 filas)\n",
      "📝 Procesando batch #2138...\n",
      "✅ Batch #2138 guardado en dataset_eb_tess_labeled_batch_2137.parquet (50000 filas)\n",
      "📝 Procesando batch #2139...\n",
      "✅ Batch #2139 guardado en dataset_eb_tess_labeled_batch_2138.parquet (50000 filas)\n",
      "📝 Procesando batch #2140...\n",
      "✅ Batch #2140 guardado en dataset_eb_tess_labeled_batch_2139.parquet (50000 filas)\n",
      "📝 Procesando batch #2141...\n",
      "✅ Batch #2141 guardado en dataset_eb_tess_labeled_batch_2140.parquet (50000 filas)\n",
      "📝 Procesando batch #2142...\n",
      "✅ Batch #2142 guardado en dataset_eb_tess_labeled_batch_2141.parquet (50000 filas)\n",
      "📝 Procesando batch #2143...\n",
      "✅ Batch #2143 guardado en dataset_eb_tess_labeled_batch_2142.parquet (50000 filas)\n",
      "📝 Procesando batch #2144...\n",
      "✅ Batch #2144 guardado en dataset_eb_tess_labeled_batch_2143.parquet (50000 filas)\n",
      "📝 Procesando batch #2145...\n",
      "✅ Batch #2145 guardado en dataset_eb_tess_labeled_batch_2144.parquet (50000 filas)\n",
      "📝 Procesando batch #2146...\n",
      "✅ Batch #2146 guardado en dataset_eb_tess_labeled_batch_2145.parquet (50000 filas)\n",
      "📝 Procesando batch #2147...\n",
      "✅ Batch #2147 guardado en dataset_eb_tess_labeled_batch_2146.parquet (50000 filas)\n",
      "📝 Procesando batch #2148...\n",
      "✅ Batch #2148 guardado en dataset_eb_tess_labeled_batch_2147.parquet (50000 filas)\n",
      "📝 Procesando batch #2149...\n",
      "✅ Batch #2149 guardado en dataset_eb_tess_labeled_batch_2148.parquet (50000 filas)\n",
      "📝 Procesando batch #2150...\n",
      "✅ Batch #2150 guardado en dataset_eb_tess_labeled_batch_2149.parquet (50000 filas)\n",
      "📝 Procesando batch #2151...\n",
      "✅ Batch #2151 guardado en dataset_eb_tess_labeled_batch_2150.parquet (50000 filas)\n",
      "📝 Procesando batch #2152...\n",
      "✅ Batch #2152 guardado en dataset_eb_tess_labeled_batch_2151.parquet (48576 filas)\n",
      "📝 Procesando batch #2153...\n",
      "✅ Batch #2153 guardado en dataset_eb_tess_labeled_batch_2152.parquet (50000 filas)\n",
      "📝 Procesando batch #2154...\n",
      "✅ Batch #2154 guardado en dataset_eb_tess_labeled_batch_2153.parquet (50000 filas)\n",
      "📝 Procesando batch #2155...\n",
      "✅ Batch #2155 guardado en dataset_eb_tess_labeled_batch_2154.parquet (50000 filas)\n",
      "📝 Procesando batch #2156...\n",
      "✅ Batch #2156 guardado en dataset_eb_tess_labeled_batch_2155.parquet (50000 filas)\n",
      "📝 Procesando batch #2157...\n",
      "✅ Batch #2157 guardado en dataset_eb_tess_labeled_batch_2156.parquet (50000 filas)\n",
      "📝 Procesando batch #2158...\n",
      "✅ Batch #2158 guardado en dataset_eb_tess_labeled_batch_2157.parquet (50000 filas)\n",
      "📝 Procesando batch #2159...\n",
      "✅ Batch #2159 guardado en dataset_eb_tess_labeled_batch_2158.parquet (50000 filas)\n",
      "📝 Procesando batch #2160...\n",
      "✅ Batch #2160 guardado en dataset_eb_tess_labeled_batch_2159.parquet (50000 filas)\n",
      "📝 Procesando batch #2161...\n",
      "✅ Batch #2161 guardado en dataset_eb_tess_labeled_batch_2160.parquet (50000 filas)\n",
      "📝 Procesando batch #2162...\n",
      "✅ Batch #2162 guardado en dataset_eb_tess_labeled_batch_2161.parquet (50000 filas)\n",
      "📝 Procesando batch #2163...\n",
      "✅ Batch #2163 guardado en dataset_eb_tess_labeled_batch_2162.parquet (50000 filas)\n",
      "📝 Procesando batch #2164...\n",
      "✅ Batch #2164 guardado en dataset_eb_tess_labeled_batch_2163.parquet (50000 filas)\n",
      "📝 Procesando batch #2165...\n",
      "✅ Batch #2165 guardado en dataset_eb_tess_labeled_batch_2164.parquet (50000 filas)\n",
      "📝 Procesando batch #2166...\n",
      "✅ Batch #2166 guardado en dataset_eb_tess_labeled_batch_2165.parquet (50000 filas)\n",
      "📝 Procesando batch #2167...\n",
      "✅ Batch #2167 guardado en dataset_eb_tess_labeled_batch_2166.parquet (50000 filas)\n",
      "📝 Procesando batch #2168...\n",
      "✅ Batch #2168 guardado en dataset_eb_tess_labeled_batch_2167.parquet (50000 filas)\n",
      "📝 Procesando batch #2169...\n",
      "✅ Batch #2169 guardado en dataset_eb_tess_labeled_batch_2168.parquet (50000 filas)\n",
      "📝 Procesando batch #2170...\n",
      "✅ Batch #2170 guardado en dataset_eb_tess_labeled_batch_2169.parquet (50000 filas)\n",
      "📝 Procesando batch #2171...\n",
      "✅ Batch #2171 guardado en dataset_eb_tess_labeled_batch_2170.parquet (50000 filas)\n",
      "📝 Procesando batch #2172...\n",
      "✅ Batch #2172 guardado en dataset_eb_tess_labeled_batch_2171.parquet (50000 filas)\n",
      "📝 Procesando batch #2173...\n",
      "✅ Batch #2173 guardado en dataset_eb_tess_labeled_batch_2172.parquet (48576 filas)\n",
      "📝 Procesando batch #2174...\n",
      "✅ Batch #2174 guardado en dataset_eb_tess_labeled_batch_2173.parquet (50000 filas)\n",
      "📝 Procesando batch #2175...\n",
      "✅ Batch #2175 guardado en dataset_eb_tess_labeled_batch_2174.parquet (50000 filas)\n",
      "📝 Procesando batch #2176...\n",
      "✅ Batch #2176 guardado en dataset_eb_tess_labeled_batch_2175.parquet (50000 filas)\n",
      "📝 Procesando batch #2177...\n",
      "✅ Batch #2177 guardado en dataset_eb_tess_labeled_batch_2176.parquet (50000 filas)\n",
      "📝 Procesando batch #2178...\n",
      "✅ Batch #2178 guardado en dataset_eb_tess_labeled_batch_2177.parquet (50000 filas)\n",
      "📝 Procesando batch #2179...\n",
      "✅ Batch #2179 guardado en dataset_eb_tess_labeled_batch_2178.parquet (50000 filas)\n",
      "📝 Procesando batch #2180...\n",
      "✅ Batch #2180 guardado en dataset_eb_tess_labeled_batch_2179.parquet (50000 filas)\n",
      "📝 Procesando batch #2181...\n",
      "✅ Batch #2181 guardado en dataset_eb_tess_labeled_batch_2180.parquet (50000 filas)\n",
      "📝 Procesando batch #2182...\n",
      "✅ Batch #2182 guardado en dataset_eb_tess_labeled_batch_2181.parquet (50000 filas)\n",
      "📝 Procesando batch #2183...\n",
      "✅ Batch #2183 guardado en dataset_eb_tess_labeled_batch_2182.parquet (50000 filas)\n",
      "📝 Procesando batch #2184...\n",
      "✅ Batch #2184 guardado en dataset_eb_tess_labeled_batch_2183.parquet (50000 filas)\n",
      "📝 Procesando batch #2185...\n",
      "✅ Batch #2185 guardado en dataset_eb_tess_labeled_batch_2184.parquet (50000 filas)\n",
      "📝 Procesando batch #2186...\n",
      "✅ Batch #2186 guardado en dataset_eb_tess_labeled_batch_2185.parquet (50000 filas)\n",
      "📝 Procesando batch #2187...\n",
      "✅ Batch #2187 guardado en dataset_eb_tess_labeled_batch_2186.parquet (50000 filas)\n",
      "📝 Procesando batch #2188...\n",
      "✅ Batch #2188 guardado en dataset_eb_tess_labeled_batch_2187.parquet (50000 filas)\n",
      "📝 Procesando batch #2189...\n",
      "✅ Batch #2189 guardado en dataset_eb_tess_labeled_batch_2188.parquet (50000 filas)\n",
      "📝 Procesando batch #2190...\n",
      "✅ Batch #2190 guardado en dataset_eb_tess_labeled_batch_2189.parquet (50000 filas)\n",
      "📝 Procesando batch #2191...\n",
      "✅ Batch #2191 guardado en dataset_eb_tess_labeled_batch_2190.parquet (50000 filas)\n",
      "📝 Procesando batch #2192...\n",
      "✅ Batch #2192 guardado en dataset_eb_tess_labeled_batch_2191.parquet (50000 filas)\n",
      "📝 Procesando batch #2193...\n",
      "✅ Batch #2193 guardado en dataset_eb_tess_labeled_batch_2192.parquet (50000 filas)\n",
      "📝 Procesando batch #2194...\n",
      "✅ Batch #2194 guardado en dataset_eb_tess_labeled_batch_2193.parquet (48576 filas)\n",
      "📝 Procesando batch #2195...\n",
      "✅ Batch #2195 guardado en dataset_eb_tess_labeled_batch_2194.parquet (50000 filas)\n",
      "📝 Procesando batch #2196...\n",
      "✅ Batch #2196 guardado en dataset_eb_tess_labeled_batch_2195.parquet (50000 filas)\n",
      "📝 Procesando batch #2197...\n",
      "✅ Batch #2197 guardado en dataset_eb_tess_labeled_batch_2196.parquet (50000 filas)\n",
      "📝 Procesando batch #2198...\n",
      "✅ Batch #2198 guardado en dataset_eb_tess_labeled_batch_2197.parquet (50000 filas)\n",
      "📝 Procesando batch #2199...\n",
      "✅ Batch #2199 guardado en dataset_eb_tess_labeled_batch_2198.parquet (50000 filas)\n",
      "📝 Procesando batch #2200...\n",
      "✅ Batch #2200 guardado en dataset_eb_tess_labeled_batch_2199.parquet (50000 filas)\n",
      "📝 Procesando batch #2201...\n",
      "✅ Batch #2201 guardado en dataset_eb_tess_labeled_batch_2200.parquet (50000 filas)\n",
      "📝 Procesando batch #2202...\n",
      "✅ Batch #2202 guardado en dataset_eb_tess_labeled_batch_2201.parquet (50000 filas)\n",
      "📝 Procesando batch #2203...\n",
      "✅ Batch #2203 guardado en dataset_eb_tess_labeled_batch_2202.parquet (29836 filas)\n",
      "📝 Procesando batch #2204...\n",
      "✅ Batch #2204 guardado en dataset_eb_tess_labeled_batch_2203.parquet (50000 filas)\n",
      "📝 Procesando batch #2205...\n",
      "✅ Batch #2205 guardado en dataset_eb_tess_labeled_batch_2204.parquet (50000 filas)\n",
      "📝 Procesando batch #2206...\n",
      "✅ Batch #2206 guardado en dataset_eb_tess_labeled_batch_2205.parquet (50000 filas)\n",
      "📝 Procesando batch #2207...\n",
      "✅ Batch #2207 guardado en dataset_eb_tess_labeled_batch_2206.parquet (50000 filas)\n",
      "📝 Procesando batch #2208...\n",
      "✅ Batch #2208 guardado en dataset_eb_tess_labeled_batch_2207.parquet (50000 filas)\n",
      "📝 Procesando batch #2209...\n",
      "✅ Batch #2209 guardado en dataset_eb_tess_labeled_batch_2208.parquet (50000 filas)\n",
      "📝 Procesando batch #2210...\n",
      "✅ Batch #2210 guardado en dataset_eb_tess_labeled_batch_2209.parquet (50000 filas)\n",
      "📝 Procesando batch #2211...\n",
      "✅ Batch #2211 guardado en dataset_eb_tess_labeled_batch_2210.parquet (50000 filas)\n",
      "📝 Procesando batch #2212...\n",
      "✅ Batch #2212 guardado en dataset_eb_tess_labeled_batch_2211.parquet (50000 filas)\n",
      "📝 Procesando batch #2213...\n",
      "✅ Batch #2213 guardado en dataset_eb_tess_labeled_batch_2212.parquet (50000 filas)\n",
      "📝 Procesando batch #2214...\n",
      "✅ Batch #2214 guardado en dataset_eb_tess_labeled_batch_2213.parquet (50000 filas)\n",
      "📝 Procesando batch #2215...\n",
      "✅ Batch #2215 guardado en dataset_eb_tess_labeled_batch_2214.parquet (50000 filas)\n",
      "📝 Procesando batch #2216...\n",
      "✅ Batch #2216 guardado en dataset_eb_tess_labeled_batch_2215.parquet (50000 filas)\n",
      "📝 Procesando batch #2217...\n",
      "✅ Batch #2217 guardado en dataset_eb_tess_labeled_batch_2216.parquet (50000 filas)\n",
      "📝 Procesando batch #2218...\n",
      "✅ Batch #2218 guardado en dataset_eb_tess_labeled_batch_2217.parquet (50000 filas)\n",
      "📝 Procesando batch #2219...\n",
      "✅ Batch #2219 guardado en dataset_eb_tess_labeled_batch_2218.parquet (50000 filas)\n",
      "📝 Procesando batch #2220...\n",
      "✅ Batch #2220 guardado en dataset_eb_tess_labeled_batch_2219.parquet (50000 filas)\n",
      "📝 Procesando batch #2221...\n",
      "✅ Batch #2221 guardado en dataset_eb_tess_labeled_batch_2220.parquet (50000 filas)\n",
      "📝 Procesando batch #2222...\n",
      "✅ Batch #2222 guardado en dataset_eb_tess_labeled_batch_2221.parquet (50000 filas)\n",
      "📝 Procesando batch #2223...\n",
      "✅ Batch #2223 guardado en dataset_eb_tess_labeled_batch_2222.parquet (50000 filas)\n",
      "📝 Procesando batch #2224...\n",
      "✅ Batch #2224 guardado en dataset_eb_tess_labeled_batch_2223.parquet (48576 filas)\n",
      "📝 Procesando batch #2225...\n",
      "✅ Batch #2225 guardado en dataset_eb_tess_labeled_batch_2224.parquet (50000 filas)\n",
      "📝 Procesando batch #2226...\n",
      "✅ Batch #2226 guardado en dataset_eb_tess_labeled_batch_2225.parquet (50000 filas)\n",
      "📝 Procesando batch #2227...\n",
      "✅ Batch #2227 guardado en dataset_eb_tess_labeled_batch_2226.parquet (50000 filas)\n",
      "📝 Procesando batch #2228...\n",
      "✅ Batch #2228 guardado en dataset_eb_tess_labeled_batch_2227.parquet (50000 filas)\n",
      "📝 Procesando batch #2229...\n",
      "✅ Batch #2229 guardado en dataset_eb_tess_labeled_batch_2228.parquet (50000 filas)\n",
      "📝 Procesando batch #2230...\n",
      "✅ Batch #2230 guardado en dataset_eb_tess_labeled_batch_2229.parquet (50000 filas)\n",
      "📝 Procesando batch #2231...\n",
      "✅ Batch #2231 guardado en dataset_eb_tess_labeled_batch_2230.parquet (50000 filas)\n",
      "📝 Procesando batch #2232...\n",
      "✅ Batch #2232 guardado en dataset_eb_tess_labeled_batch_2231.parquet (50000 filas)\n",
      "📝 Procesando batch #2233...\n",
      "✅ Batch #2233 guardado en dataset_eb_tess_labeled_batch_2232.parquet (50000 filas)\n",
      "📝 Procesando batch #2234...\n",
      "✅ Batch #2234 guardado en dataset_eb_tess_labeled_batch_2233.parquet (50000 filas)\n",
      "📝 Procesando batch #2235...\n",
      "✅ Batch #2235 guardado en dataset_eb_tess_labeled_batch_2234.parquet (50000 filas)\n",
      "📝 Procesando batch #2236...\n",
      "✅ Batch #2236 guardado en dataset_eb_tess_labeled_batch_2235.parquet (50000 filas)\n",
      "📝 Procesando batch #2237...\n",
      "✅ Batch #2237 guardado en dataset_eb_tess_labeled_batch_2236.parquet (50000 filas)\n",
      "📝 Procesando batch #2238...\n",
      "✅ Batch #2238 guardado en dataset_eb_tess_labeled_batch_2237.parquet (50000 filas)\n",
      "📝 Procesando batch #2239...\n",
      "✅ Batch #2239 guardado en dataset_eb_tess_labeled_batch_2238.parquet (50000 filas)\n",
      "📝 Procesando batch #2240...\n",
      "✅ Batch #2240 guardado en dataset_eb_tess_labeled_batch_2239.parquet (50000 filas)\n",
      "📝 Procesando batch #2241...\n",
      "✅ Batch #2241 guardado en dataset_eb_tess_labeled_batch_2240.parquet (50000 filas)\n",
      "📝 Procesando batch #2242...\n",
      "✅ Batch #2242 guardado en dataset_eb_tess_labeled_batch_2241.parquet (50000 filas)\n",
      "📝 Procesando batch #2243...\n",
      "✅ Batch #2243 guardado en dataset_eb_tess_labeled_batch_2242.parquet (50000 filas)\n",
      "📝 Procesando batch #2244...\n",
      "✅ Batch #2244 guardado en dataset_eb_tess_labeled_batch_2243.parquet (50000 filas)\n",
      "📝 Procesando batch #2245...\n",
      "✅ Batch #2245 guardado en dataset_eb_tess_labeled_batch_2244.parquet (48576 filas)\n",
      "📝 Procesando batch #2246...\n",
      "✅ Batch #2246 guardado en dataset_eb_tess_labeled_batch_2245.parquet (50000 filas)\n",
      "📝 Procesando batch #2247...\n",
      "✅ Batch #2247 guardado en dataset_eb_tess_labeled_batch_2246.parquet (50000 filas)\n",
      "📝 Procesando batch #2248...\n",
      "✅ Batch #2248 guardado en dataset_eb_tess_labeled_batch_2247.parquet (50000 filas)\n",
      "📝 Procesando batch #2249...\n",
      "✅ Batch #2249 guardado en dataset_eb_tess_labeled_batch_2248.parquet (50000 filas)\n",
      "📝 Procesando batch #2250...\n",
      "✅ Batch #2250 guardado en dataset_eb_tess_labeled_batch_2249.parquet (50000 filas)\n",
      "📝 Procesando batch #2251...\n",
      "✅ Batch #2251 guardado en dataset_eb_tess_labeled_batch_2250.parquet (50000 filas)\n",
      "📝 Procesando batch #2252...\n",
      "✅ Batch #2252 guardado en dataset_eb_tess_labeled_batch_2251.parquet (50000 filas)\n",
      "📝 Procesando batch #2253...\n",
      "✅ Batch #2253 guardado en dataset_eb_tess_labeled_batch_2252.parquet (50000 filas)\n",
      "📝 Procesando batch #2254...\n",
      "✅ Batch #2254 guardado en dataset_eb_tess_labeled_batch_2253.parquet (50000 filas)\n",
      "📝 Procesando batch #2255...\n",
      "✅ Batch #2255 guardado en dataset_eb_tess_labeled_batch_2254.parquet (50000 filas)\n",
      "📝 Procesando batch #2256...\n",
      "✅ Batch #2256 guardado en dataset_eb_tess_labeled_batch_2255.parquet (50000 filas)\n",
      "📝 Procesando batch #2257...\n",
      "✅ Batch #2257 guardado en dataset_eb_tess_labeled_batch_2256.parquet (50000 filas)\n",
      "📝 Procesando batch #2258...\n",
      "✅ Batch #2258 guardado en dataset_eb_tess_labeled_batch_2257.parquet (50000 filas)\n",
      "📝 Procesando batch #2259...\n",
      "✅ Batch #2259 guardado en dataset_eb_tess_labeled_batch_2258.parquet (50000 filas)\n",
      "📝 Procesando batch #2260...\n",
      "✅ Batch #2260 guardado en dataset_eb_tess_labeled_batch_2259.parquet (50000 filas)\n",
      "📝 Procesando batch #2261...\n",
      "✅ Batch #2261 guardado en dataset_eb_tess_labeled_batch_2260.parquet (50000 filas)\n",
      "📝 Procesando batch #2262...\n",
      "✅ Batch #2262 guardado en dataset_eb_tess_labeled_batch_2261.parquet (50000 filas)\n",
      "📝 Procesando batch #2263...\n",
      "✅ Batch #2263 guardado en dataset_eb_tess_labeled_batch_2262.parquet (50000 filas)\n",
      "📝 Procesando batch #2264...\n",
      "✅ Batch #2264 guardado en dataset_eb_tess_labeled_batch_2263.parquet (50000 filas)\n",
      "📝 Procesando batch #2265...\n",
      "✅ Batch #2265 guardado en dataset_eb_tess_labeled_batch_2264.parquet (50000 filas)\n",
      "📝 Procesando batch #2266...\n",
      "✅ Batch #2266 guardado en dataset_eb_tess_labeled_batch_2265.parquet (48576 filas)\n",
      "📝 Procesando batch #2267...\n",
      "✅ Batch #2267 guardado en dataset_eb_tess_labeled_batch_2266.parquet (50000 filas)\n",
      "📝 Procesando batch #2268...\n",
      "✅ Batch #2268 guardado en dataset_eb_tess_labeled_batch_2267.parquet (50000 filas)\n",
      "📝 Procesando batch #2269...\n",
      "✅ Batch #2269 guardado en dataset_eb_tess_labeled_batch_2268.parquet (50000 filas)\n",
      "📝 Procesando batch #2270...\n",
      "✅ Batch #2270 guardado en dataset_eb_tess_labeled_batch_2269.parquet (50000 filas)\n",
      "📝 Procesando batch #2271...\n",
      "✅ Batch #2271 guardado en dataset_eb_tess_labeled_batch_2270.parquet (50000 filas)\n",
      "📝 Procesando batch #2272...\n",
      "✅ Batch #2272 guardado en dataset_eb_tess_labeled_batch_2271.parquet (50000 filas)\n",
      "📝 Procesando batch #2273...\n",
      "✅ Batch #2273 guardado en dataset_eb_tess_labeled_batch_2272.parquet (50000 filas)\n",
      "📝 Procesando batch #2274...\n",
      "✅ Batch #2274 guardado en dataset_eb_tess_labeled_batch_2273.parquet (50000 filas)\n",
      "📝 Procesando batch #2275...\n",
      "✅ Batch #2275 guardado en dataset_eb_tess_labeled_batch_2274.parquet (50000 filas)\n",
      "📝 Procesando batch #2276...\n",
      "✅ Batch #2276 guardado en dataset_eb_tess_labeled_batch_2275.parquet (50000 filas)\n",
      "📝 Procesando batch #2277...\n",
      "✅ Batch #2277 guardado en dataset_eb_tess_labeled_batch_2276.parquet (50000 filas)\n",
      "📝 Procesando batch #2278...\n",
      "✅ Batch #2278 guardado en dataset_eb_tess_labeled_batch_2277.parquet (50000 filas)\n",
      "📝 Procesando batch #2279...\n",
      "✅ Batch #2279 guardado en dataset_eb_tess_labeled_batch_2278.parquet (50000 filas)\n",
      "📝 Procesando batch #2280...\n",
      "✅ Batch #2280 guardado en dataset_eb_tess_labeled_batch_2279.parquet (50000 filas)\n",
      "📝 Procesando batch #2281...\n",
      "✅ Batch #2281 guardado en dataset_eb_tess_labeled_batch_2280.parquet (50000 filas)\n",
      "📝 Procesando batch #2282...\n",
      "✅ Batch #2282 guardado en dataset_eb_tess_labeled_batch_2281.parquet (50000 filas)\n",
      "📝 Procesando batch #2283...\n",
      "✅ Batch #2283 guardado en dataset_eb_tess_labeled_batch_2282.parquet (50000 filas)\n",
      "📝 Procesando batch #2284...\n",
      "✅ Batch #2284 guardado en dataset_eb_tess_labeled_batch_2283.parquet (50000 filas)\n",
      "📝 Procesando batch #2285...\n",
      "✅ Batch #2285 guardado en dataset_eb_tess_labeled_batch_2284.parquet (50000 filas)\n",
      "📝 Procesando batch #2286...\n",
      "✅ Batch #2286 guardado en dataset_eb_tess_labeled_batch_2285.parquet (50000 filas)\n",
      "📝 Procesando batch #2287...\n",
      "✅ Batch #2287 guardado en dataset_eb_tess_labeled_batch_2286.parquet (48576 filas)\n",
      "📝 Procesando batch #2288...\n",
      "✅ Batch #2288 guardado en dataset_eb_tess_labeled_batch_2287.parquet (50000 filas)\n",
      "📝 Procesando batch #2289...\n",
      "✅ Batch #2289 guardado en dataset_eb_tess_labeled_batch_2288.parquet (50000 filas)\n",
      "📝 Procesando batch #2290...\n",
      "✅ Batch #2290 guardado en dataset_eb_tess_labeled_batch_2289.parquet (50000 filas)\n",
      "📝 Procesando batch #2291...\n",
      "✅ Batch #2291 guardado en dataset_eb_tess_labeled_batch_2290.parquet (50000 filas)\n",
      "📝 Procesando batch #2292...\n",
      "✅ Batch #2292 guardado en dataset_eb_tess_labeled_batch_2291.parquet (50000 filas)\n",
      "📝 Procesando batch #2293...\n",
      "✅ Batch #2293 guardado en dataset_eb_tess_labeled_batch_2292.parquet (50000 filas)\n",
      "📝 Procesando batch #2294...\n",
      "✅ Batch #2294 guardado en dataset_eb_tess_labeled_batch_2293.parquet (50000 filas)\n",
      "📝 Procesando batch #2295...\n",
      "✅ Batch #2295 guardado en dataset_eb_tess_labeled_batch_2294.parquet (50000 filas)\n",
      "📝 Procesando batch #2296...\n",
      "✅ Batch #2296 guardado en dataset_eb_tess_labeled_batch_2295.parquet (50000 filas)\n",
      "📝 Procesando batch #2297...\n",
      "✅ Batch #2297 guardado en dataset_eb_tess_labeled_batch_2296.parquet (50000 filas)\n",
      "📝 Procesando batch #2298...\n",
      "✅ Batch #2298 guardado en dataset_eb_tess_labeled_batch_2297.parquet (50000 filas)\n",
      "📝 Procesando batch #2299...\n",
      "✅ Batch #2299 guardado en dataset_eb_tess_labeled_batch_2298.parquet (50000 filas)\n",
      "📝 Procesando batch #2300...\n",
      "✅ Batch #2300 guardado en dataset_eb_tess_labeled_batch_2299.parquet (50000 filas)\n",
      "📝 Procesando batch #2301...\n",
      "✅ Batch #2301 guardado en dataset_eb_tess_labeled_batch_2300.parquet (50000 filas)\n",
      "📝 Procesando batch #2302...\n",
      "✅ Batch #2302 guardado en dataset_eb_tess_labeled_batch_2301.parquet (50000 filas)\n",
      "📝 Procesando batch #2303...\n",
      "✅ Batch #2303 guardado en dataset_eb_tess_labeled_batch_2302.parquet (50000 filas)\n",
      "📝 Procesando batch #2304...\n",
      "✅ Batch #2304 guardado en dataset_eb_tess_labeled_batch_2303.parquet (50000 filas)\n",
      "📝 Procesando batch #2305...\n",
      "✅ Batch #2305 guardado en dataset_eb_tess_labeled_batch_2304.parquet (50000 filas)\n",
      "📝 Procesando batch #2306...\n",
      "✅ Batch #2306 guardado en dataset_eb_tess_labeled_batch_2305.parquet (50000 filas)\n",
      "📝 Procesando batch #2307...\n",
      "✅ Batch #2307 guardado en dataset_eb_tess_labeled_batch_2306.parquet (50000 filas)\n",
      "📝 Procesando batch #2308...\n",
      "✅ Batch #2308 guardado en dataset_eb_tess_labeled_batch_2307.parquet (48576 filas)\n",
      "📝 Procesando batch #2309...\n",
      "✅ Batch #2309 guardado en dataset_eb_tess_labeled_batch_2308.parquet (50000 filas)\n",
      "📝 Procesando batch #2310...\n",
      "✅ Batch #2310 guardado en dataset_eb_tess_labeled_batch_2309.parquet (50000 filas)\n",
      "📝 Procesando batch #2311...\n",
      "✅ Batch #2311 guardado en dataset_eb_tess_labeled_batch_2310.parquet (50000 filas)\n",
      "📝 Procesando batch #2312...\n",
      "✅ Batch #2312 guardado en dataset_eb_tess_labeled_batch_2311.parquet (50000 filas)\n",
      "📝 Procesando batch #2313...\n",
      "✅ Batch #2313 guardado en dataset_eb_tess_labeled_batch_2312.parquet (50000 filas)\n",
      "📝 Procesando batch #2314...\n",
      "✅ Batch #2314 guardado en dataset_eb_tess_labeled_batch_2313.parquet (50000 filas)\n",
      "📝 Procesando batch #2315...\n",
      "✅ Batch #2315 guardado en dataset_eb_tess_labeled_batch_2314.parquet (50000 filas)\n",
      "📝 Procesando batch #2316...\n",
      "✅ Batch #2316 guardado en dataset_eb_tess_labeled_batch_2315.parquet (50000 filas)\n",
      "📝 Procesando batch #2317...\n",
      "✅ Batch #2317 guardado en dataset_eb_tess_labeled_batch_2316.parquet (50000 filas)\n",
      "📝 Procesando batch #2318...\n",
      "✅ Batch #2318 guardado en dataset_eb_tess_labeled_batch_2317.parquet (50000 filas)\n",
      "📝 Procesando batch #2319...\n",
      "✅ Batch #2319 guardado en dataset_eb_tess_labeled_batch_2318.parquet (50000 filas)\n",
      "📝 Procesando batch #2320...\n",
      "✅ Batch #2320 guardado en dataset_eb_tess_labeled_batch_2319.parquet (50000 filas)\n",
      "📝 Procesando batch #2321...\n",
      "✅ Batch #2321 guardado en dataset_eb_tess_labeled_batch_2320.parquet (50000 filas)\n",
      "📝 Procesando batch #2322...\n",
      "✅ Batch #2322 guardado en dataset_eb_tess_labeled_batch_2321.parquet (50000 filas)\n",
      "📝 Procesando batch #2323...\n",
      "✅ Batch #2323 guardado en dataset_eb_tess_labeled_batch_2322.parquet (50000 filas)\n",
      "📝 Procesando batch #2324...\n",
      "✅ Batch #2324 guardado en dataset_eb_tess_labeled_batch_2323.parquet (50000 filas)\n",
      "📝 Procesando batch #2325...\n",
      "✅ Batch #2325 guardado en dataset_eb_tess_labeled_batch_2324.parquet (50000 filas)\n",
      "📝 Procesando batch #2326...\n",
      "✅ Batch #2326 guardado en dataset_eb_tess_labeled_batch_2325.parquet (50000 filas)\n",
      "📝 Procesando batch #2327...\n",
      "✅ Batch #2327 guardado en dataset_eb_tess_labeled_batch_2326.parquet (50000 filas)\n",
      "📝 Procesando batch #2328...\n",
      "✅ Batch #2328 guardado en dataset_eb_tess_labeled_batch_2327.parquet (50000 filas)\n",
      "📝 Procesando batch #2329...\n",
      "✅ Batch #2329 guardado en dataset_eb_tess_labeled_batch_2328.parquet (48576 filas)\n",
      "📝 Procesando batch #2330...\n",
      "✅ Batch #2330 guardado en dataset_eb_tess_labeled_batch_2329.parquet (50000 filas)\n",
      "📝 Procesando batch #2331...\n",
      "✅ Batch #2331 guardado en dataset_eb_tess_labeled_batch_2330.parquet (50000 filas)\n",
      "📝 Procesando batch #2332...\n",
      "✅ Batch #2332 guardado en dataset_eb_tess_labeled_batch_2331.parquet (50000 filas)\n",
      "📝 Procesando batch #2333...\n",
      "✅ Batch #2333 guardado en dataset_eb_tess_labeled_batch_2332.parquet (50000 filas)\n",
      "📝 Procesando batch #2334...\n",
      "✅ Batch #2334 guardado en dataset_eb_tess_labeled_batch_2333.parquet (50000 filas)\n",
      "📝 Procesando batch #2335...\n",
      "✅ Batch #2335 guardado en dataset_eb_tess_labeled_batch_2334.parquet (50000 filas)\n",
      "📝 Procesando batch #2336...\n",
      "✅ Batch #2336 guardado en dataset_eb_tess_labeled_batch_2335.parquet (50000 filas)\n",
      "📝 Procesando batch #2337...\n",
      "✅ Batch #2337 guardado en dataset_eb_tess_labeled_batch_2336.parquet (50000 filas)\n",
      "📝 Procesando batch #2338...\n",
      "✅ Batch #2338 guardado en dataset_eb_tess_labeled_batch_2337.parquet (50000 filas)\n",
      "📝 Procesando batch #2339...\n",
      "✅ Batch #2339 guardado en dataset_eb_tess_labeled_batch_2338.parquet (50000 filas)\n",
      "📝 Procesando batch #2340...\n",
      "✅ Batch #2340 guardado en dataset_eb_tess_labeled_batch_2339.parquet (50000 filas)\n",
      "📝 Procesando batch #2341...\n",
      "✅ Batch #2341 guardado en dataset_eb_tess_labeled_batch_2340.parquet (50000 filas)\n",
      "📝 Procesando batch #2342...\n",
      "✅ Batch #2342 guardado en dataset_eb_tess_labeled_batch_2341.parquet (50000 filas)\n",
      "📝 Procesando batch #2343...\n",
      "✅ Batch #2343 guardado en dataset_eb_tess_labeled_batch_2342.parquet (50000 filas)\n",
      "📝 Procesando batch #2344...\n",
      "✅ Batch #2344 guardado en dataset_eb_tess_labeled_batch_2343.parquet (50000 filas)\n",
      "📝 Procesando batch #2345...\n",
      "✅ Batch #2345 guardado en dataset_eb_tess_labeled_batch_2344.parquet (50000 filas)\n",
      "📝 Procesando batch #2346...\n",
      "✅ Batch #2346 guardado en dataset_eb_tess_labeled_batch_2345.parquet (50000 filas)\n",
      "📝 Procesando batch #2347...\n",
      "✅ Batch #2347 guardado en dataset_eb_tess_labeled_batch_2346.parquet (50000 filas)\n",
      "📝 Procesando batch #2348...\n",
      "✅ Batch #2348 guardado en dataset_eb_tess_labeled_batch_2347.parquet (50000 filas)\n",
      "📝 Procesando batch #2349...\n",
      "✅ Batch #2349 guardado en dataset_eb_tess_labeled_batch_2348.parquet (50000 filas)\n",
      "📝 Procesando batch #2350...\n",
      "✅ Batch #2350 guardado en dataset_eb_tess_labeled_batch_2349.parquet (48576 filas)\n",
      "📝 Procesando batch #2351...\n",
      "✅ Batch #2351 guardado en dataset_eb_tess_labeled_batch_2350.parquet (50000 filas)\n",
      "📝 Procesando batch #2352...\n",
      "✅ Batch #2352 guardado en dataset_eb_tess_labeled_batch_2351.parquet (50000 filas)\n",
      "📝 Procesando batch #2353...\n",
      "✅ Batch #2353 guardado en dataset_eb_tess_labeled_batch_2352.parquet (50000 filas)\n",
      "📝 Procesando batch #2354...\n",
      "✅ Batch #2354 guardado en dataset_eb_tess_labeled_batch_2353.parquet (50000 filas)\n",
      "📝 Procesando batch #2355...\n",
      "✅ Batch #2355 guardado en dataset_eb_tess_labeled_batch_2354.parquet (50000 filas)\n",
      "📝 Procesando batch #2356...\n",
      "✅ Batch #2356 guardado en dataset_eb_tess_labeled_batch_2355.parquet (50000 filas)\n",
      "📝 Procesando batch #2357...\n",
      "✅ Batch #2357 guardado en dataset_eb_tess_labeled_batch_2356.parquet (50000 filas)\n",
      "📝 Procesando batch #2358...\n",
      "✅ Batch #2358 guardado en dataset_eb_tess_labeled_batch_2357.parquet (50000 filas)\n",
      "📝 Procesando batch #2359...\n",
      "✅ Batch #2359 guardado en dataset_eb_tess_labeled_batch_2358.parquet (50000 filas)\n",
      "📝 Procesando batch #2360...\n",
      "✅ Batch #2360 guardado en dataset_eb_tess_labeled_batch_2359.parquet (50000 filas)\n",
      "📝 Procesando batch #2361...\n",
      "✅ Batch #2361 guardado en dataset_eb_tess_labeled_batch_2360.parquet (50000 filas)\n",
      "📝 Procesando batch #2362...\n",
      "✅ Batch #2362 guardado en dataset_eb_tess_labeled_batch_2361.parquet (13966 filas)\n",
      "📝 Procesando batch #2363...\n",
      "✅ Batch #2363 guardado en dataset_eb_tess_labeled_batch_2362.parquet (50000 filas)\n",
      "📝 Procesando batch #2364...\n",
      "✅ Batch #2364 guardado en dataset_eb_tess_labeled_batch_2363.parquet (50000 filas)\n",
      "📝 Procesando batch #2365...\n",
      "✅ Batch #2365 guardado en dataset_eb_tess_labeled_batch_2364.parquet (50000 filas)\n",
      "📝 Procesando batch #2366...\n",
      "✅ Batch #2366 guardado en dataset_eb_tess_labeled_batch_2365.parquet (50000 filas)\n",
      "📝 Procesando batch #2367...\n",
      "✅ Batch #2367 guardado en dataset_eb_tess_labeled_batch_2366.parquet (50000 filas)\n",
      "📝 Procesando batch #2368...\n",
      "✅ Batch #2368 guardado en dataset_eb_tess_labeled_batch_2367.parquet (50000 filas)\n",
      "📝 Procesando batch #2369...\n",
      "✅ Batch #2369 guardado en dataset_eb_tess_labeled_batch_2368.parquet (50000 filas)\n",
      "📝 Procesando batch #2370...\n",
      "✅ Batch #2370 guardado en dataset_eb_tess_labeled_batch_2369.parquet (50000 filas)\n",
      "📝 Procesando batch #2371...\n",
      "✅ Batch #2371 guardado en dataset_eb_tess_labeled_batch_2370.parquet (50000 filas)\n",
      "📝 Procesando batch #2372...\n",
      "✅ Batch #2372 guardado en dataset_eb_tess_labeled_batch_2371.parquet (50000 filas)\n",
      "📝 Procesando batch #2373...\n",
      "✅ Batch #2373 guardado en dataset_eb_tess_labeled_batch_2372.parquet (50000 filas)\n",
      "📝 Procesando batch #2374...\n",
      "✅ Batch #2374 guardado en dataset_eb_tess_labeled_batch_2373.parquet (50000 filas)\n",
      "📝 Procesando batch #2375...\n",
      "✅ Batch #2375 guardado en dataset_eb_tess_labeled_batch_2374.parquet (50000 filas)\n",
      "📝 Procesando batch #2376...\n",
      "✅ Batch #2376 guardado en dataset_eb_tess_labeled_batch_2375.parquet (50000 filas)\n",
      "📝 Procesando batch #2377...\n",
      "✅ Batch #2377 guardado en dataset_eb_tess_labeled_batch_2376.parquet (50000 filas)\n",
      "📝 Procesando batch #2378...\n",
      "✅ Batch #2378 guardado en dataset_eb_tess_labeled_batch_2377.parquet (50000 filas)\n",
      "📝 Procesando batch #2379...\n",
      "✅ Batch #2379 guardado en dataset_eb_tess_labeled_batch_2378.parquet (50000 filas)\n",
      "📝 Procesando batch #2380...\n",
      "✅ Batch #2380 guardado en dataset_eb_tess_labeled_batch_2379.parquet (50000 filas)\n",
      "📝 Procesando batch #2381...\n",
      "✅ Batch #2381 guardado en dataset_eb_tess_labeled_batch_2380.parquet (50000 filas)\n",
      "📝 Procesando batch #2382...\n",
      "✅ Batch #2382 guardado en dataset_eb_tess_labeled_batch_2381.parquet (50000 filas)\n",
      "📝 Procesando batch #2383...\n",
      "✅ Batch #2383 guardado en dataset_eb_tess_labeled_batch_2382.parquet (48576 filas)\n",
      "📝 Procesando batch #2384...\n",
      "✅ Batch #2384 guardado en dataset_eb_tess_labeled_batch_2383.parquet (50000 filas)\n",
      "📝 Procesando batch #2385...\n",
      "✅ Batch #2385 guardado en dataset_eb_tess_labeled_batch_2384.parquet (50000 filas)\n",
      "📝 Procesando batch #2386...\n",
      "✅ Batch #2386 guardado en dataset_eb_tess_labeled_batch_2385.parquet (50000 filas)\n",
      "📝 Procesando batch #2387...\n",
      "✅ Batch #2387 guardado en dataset_eb_tess_labeled_batch_2386.parquet (50000 filas)\n",
      "📝 Procesando batch #2388...\n",
      "✅ Batch #2388 guardado en dataset_eb_tess_labeled_batch_2387.parquet (50000 filas)\n",
      "📝 Procesando batch #2389...\n",
      "✅ Batch #2389 guardado en dataset_eb_tess_labeled_batch_2388.parquet (50000 filas)\n",
      "📝 Procesando batch #2390...\n",
      "✅ Batch #2390 guardado en dataset_eb_tess_labeled_batch_2389.parquet (50000 filas)\n",
      "📝 Procesando batch #2391...\n",
      "✅ Batch #2391 guardado en dataset_eb_tess_labeled_batch_2390.parquet (50000 filas)\n",
      "📝 Procesando batch #2392...\n",
      "✅ Batch #2392 guardado en dataset_eb_tess_labeled_batch_2391.parquet (50000 filas)\n",
      "📝 Procesando batch #2393...\n",
      "✅ Batch #2393 guardado en dataset_eb_tess_labeled_batch_2392.parquet (50000 filas)\n",
      "📝 Procesando batch #2394...\n",
      "✅ Batch #2394 guardado en dataset_eb_tess_labeled_batch_2393.parquet (50000 filas)\n",
      "📝 Procesando batch #2395...\n",
      "✅ Batch #2395 guardado en dataset_eb_tess_labeled_batch_2394.parquet (50000 filas)\n",
      "📝 Procesando batch #2396...\n",
      "✅ Batch #2396 guardado en dataset_eb_tess_labeled_batch_2395.parquet (50000 filas)\n",
      "📝 Procesando batch #2397...\n",
      "✅ Batch #2397 guardado en dataset_eb_tess_labeled_batch_2396.parquet (50000 filas)\n",
      "📝 Procesando batch #2398...\n",
      "✅ Batch #2398 guardado en dataset_eb_tess_labeled_batch_2397.parquet (50000 filas)\n",
      "📝 Procesando batch #2399...\n",
      "✅ Batch #2399 guardado en dataset_eb_tess_labeled_batch_2398.parquet (50000 filas)\n",
      "📝 Procesando batch #2400...\n",
      "✅ Batch #2400 guardado en dataset_eb_tess_labeled_batch_2399.parquet (50000 filas)\n",
      "📝 Procesando batch #2401...\n",
      "✅ Batch #2401 guardado en dataset_eb_tess_labeled_batch_2400.parquet (50000 filas)\n",
      "📝 Procesando batch #2402...\n",
      "✅ Batch #2402 guardado en dataset_eb_tess_labeled_batch_2401.parquet (50000 filas)\n",
      "📝 Procesando batch #2403...\n",
      "✅ Batch #2403 guardado en dataset_eb_tess_labeled_batch_2402.parquet (50000 filas)\n",
      "📝 Procesando batch #2404...\n",
      "✅ Batch #2404 guardado en dataset_eb_tess_labeled_batch_2403.parquet (48576 filas)\n",
      "📝 Procesando batch #2405...\n",
      "✅ Batch #2405 guardado en dataset_eb_tess_labeled_batch_2404.parquet (50000 filas)\n",
      "📝 Procesando batch #2406...\n",
      "✅ Batch #2406 guardado en dataset_eb_tess_labeled_batch_2405.parquet (50000 filas)\n",
      "📝 Procesando batch #2407...\n",
      "✅ Batch #2407 guardado en dataset_eb_tess_labeled_batch_2406.parquet (50000 filas)\n",
      "📝 Procesando batch #2408...\n",
      "✅ Batch #2408 guardado en dataset_eb_tess_labeled_batch_2407.parquet (50000 filas)\n",
      "📝 Procesando batch #2409...\n",
      "✅ Batch #2409 guardado en dataset_eb_tess_labeled_batch_2408.parquet (50000 filas)\n",
      "📝 Procesando batch #2410...\n",
      "✅ Batch #2410 guardado en dataset_eb_tess_labeled_batch_2409.parquet (50000 filas)\n",
      "📝 Procesando batch #2411...\n",
      "✅ Batch #2411 guardado en dataset_eb_tess_labeled_batch_2410.parquet (50000 filas)\n",
      "📝 Procesando batch #2412...\n",
      "✅ Batch #2412 guardado en dataset_eb_tess_labeled_batch_2411.parquet (50000 filas)\n",
      "📝 Procesando batch #2413...\n",
      "✅ Batch #2413 guardado en dataset_eb_tess_labeled_batch_2412.parquet (50000 filas)\n",
      "📝 Procesando batch #2414...\n",
      "✅ Batch #2414 guardado en dataset_eb_tess_labeled_batch_2413.parquet (50000 filas)\n",
      "📝 Procesando batch #2415...\n",
      "✅ Batch #2415 guardado en dataset_eb_tess_labeled_batch_2414.parquet (50000 filas)\n",
      "📝 Procesando batch #2416...\n",
      "✅ Batch #2416 guardado en dataset_eb_tess_labeled_batch_2415.parquet (50000 filas)\n",
      "📝 Procesando batch #2417...\n",
      "✅ Batch #2417 guardado en dataset_eb_tess_labeled_batch_2416.parquet (50000 filas)\n",
      "📝 Procesando batch #2418...\n",
      "✅ Batch #2418 guardado en dataset_eb_tess_labeled_batch_2417.parquet (50000 filas)\n",
      "📝 Procesando batch #2419...\n",
      "✅ Batch #2419 guardado en dataset_eb_tess_labeled_batch_2418.parquet (50000 filas)\n",
      "📝 Procesando batch #2420...\n",
      "✅ Batch #2420 guardado en dataset_eb_tess_labeled_batch_2419.parquet (50000 filas)\n",
      "📝 Procesando batch #2421...\n",
      "✅ Batch #2421 guardado en dataset_eb_tess_labeled_batch_2420.parquet (50000 filas)\n",
      "📝 Procesando batch #2422...\n",
      "✅ Batch #2422 guardado en dataset_eb_tess_labeled_batch_2421.parquet (50000 filas)\n",
      "📝 Procesando batch #2423...\n",
      "✅ Batch #2423 guardado en dataset_eb_tess_labeled_batch_2422.parquet (50000 filas)\n",
      "📝 Procesando batch #2424...\n",
      "✅ Batch #2424 guardado en dataset_eb_tess_labeled_batch_2423.parquet (50000 filas)\n",
      "📝 Procesando batch #2425...\n",
      "✅ Batch #2425 guardado en dataset_eb_tess_labeled_batch_2424.parquet (48576 filas)\n",
      "📝 Procesando batch #2426...\n",
      "✅ Batch #2426 guardado en dataset_eb_tess_labeled_batch_2425.parquet (25519 filas)\n",
      "🔗 Unificando 2426 batches en dataset_eb_tess_labeled_fixed.parquet...\n",
      "❌ Error procesando tess: malloc of size 524288 failed\n"
     ]
    }
   ],
   "source": [
    "%run src/script_5a_normalize_ids.py --mision tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a19d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Unificando 2426 batches por streaming...\n",
      "✅ 100 batches escritos...\n",
      "✅ 200 batches escritos...\n",
      "✅ 300 batches escritos...\n",
      "✅ 400 batches escritos...\n",
      "✅ 500 batches escritos...\n",
      "✅ 600 batches escritos...\n",
      "✅ 700 batches escritos...\n",
      "✅ 800 batches escritos...\n",
      "✅ 900 batches escritos...\n",
      "✅ 1000 batches escritos...\n",
      "✅ 1100 batches escritos...\n",
      "✅ 1200 batches escritos...\n",
      "✅ 1300 batches escritos...\n",
      "✅ 1400 batches escritos...\n",
      "✅ 1500 batches escritos...\n",
      "✅ 1600 batches escritos...\n",
      "✅ 1700 batches escritos...\n",
      "✅ 1800 batches escritos...\n",
      "✅ 1900 batches escritos...\n",
      "✅ 2000 batches escritos...\n",
      "✅ 2100 batches escritos...\n",
      "✅ 2200 batches escritos...\n",
      "✅ 2300 batches escritos...\n",
      "✅ 2400 batches escritos...\n",
      "✅ Escritura completada: data\\processed\\dataset_eb_tess_labeled_fixed.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyarrow import dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuración\n",
    "stem = \"dataset_eb_tess_labeled_batch\"\n",
    "temp_dir = Path(\"data/processed/normalized_temp\")\n",
    "output_path = Path(\"data/processed/dataset_eb_tess_labeled_fixed.parquet\")\n",
    "batch_files = sorted(temp_dir.glob(f\"{stem}_*.parquet\"))\n",
    "\n",
    "print(f\"🔗 Unificando {len(batch_files)} batches por streaming...\")\n",
    "\n",
    "# Crear dataset de entrada\n",
    "dataset = ds.dataset(batch_files, format=\"parquet\")\n",
    "\n",
    "# Inicializar escritor\n",
    "writer = None\n",
    "for i, batch in enumerate(dataset.to_batches()):\n",
    "    table = pa.Table.from_batches([batch])\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_path, table.schema)\n",
    "    writer.write_table(table)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"✅ {i+1} batches escritos...\")\n",
    "\n",
    "# Cerrar\n",
    "if writer:\n",
    "    writer.close()\n",
    "    print(f\"✅ Escritura completada: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be542f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/script_5a_normalize_ids.py --mision k2\n",
    "\n",
    "# Ejecutado en otro notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/script_5a_normalize_ids.py --mision vsx_tess\n",
    "\n",
    "# Ejecutado en otro notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db46103",
   "metadata": {},
   "source": [
    "#### ✅ **Verificación estructural completa de todos los .parquet**\n",
    "Podemos hacer un informe resumen automático para cada uno de los datasets que tienes en data/processed/, con foco en:\n",
    "\n",
    "Dataset\t¿id_objeto limpio?\t¿tic_id presente?\t¿ra/dec presentes?\t¿clase_variable?\t¿clase_variable_normalizada?\tNº de filas\n",
    "\n",
    "Esto te daría un estado global del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5004693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "📊 Inspección de 5 datasets...\n",
      "\n",
      "📂 Inspeccionando: dataset_eb_kepler_labeled_fixed.parquet\n",
      "📂 Inspeccionando: dataset_eb_tess_labeled_fixed.parquet\n",
      "📂 Inspeccionando: dataset_k2varcat_labeled_fixed.parquet\n",
      "📂 Inspeccionando: dataset_vsx_tess_labeled_fixed.parquet\n",
      "📂 Inspeccionando: dataset_gaia_dr3_vsx_tic_labeled.parquet\n",
      "\n",
      "⏱️ Tiempo total: 233.67 segundos\n",
      "\n",
      "| dataset                                  |      rows | id_objeto_ok   | mision   | ra    | dec   | clase_variable   | clase_variable_normalizada   | origen_etiqueta   |\n",
      "|:-----------------------------------------|----------:|:---------------|:---------|:------|:------|:-----------------|:-----------------------------|:------------------|\n",
      "| dataset_eb_kepler_labeled_fixed.parquet  |  71735011 | True           | True     | False | False | True             | False                        | False             |\n",
      "| dataset_eb_tess_labeled_fixed.parquet    | 120747997 | True           | True     | False | False | True             | False                        | False             |\n",
      "| dataset_k2varcat_labeled_fixed.parquet   |  17895159 | True           | True     | False | False | True             | False                        | True              |\n",
      "| dataset_vsx_tess_labeled_fixed.parquet   |  41143597 | True           | True     | False | False | True             | True                         | True              |\n",
      "| dataset_gaia_dr3_vsx_tic_labeled.parquet |      3987 | True           | True     | False | False | True             | False                        | True              |\n",
      "\n",
      "📄 Markdown exportado: processed\\summary\\summary_datasets_fixed.md\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate\n",
    "%run src/utils/inspect_all_parquets.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfdfe6",
   "metadata": {},
   "source": [
    "| dataset                                  |      rows | id_objeto_ok   | mision   | ra    | dec   | clase_variable   | clase_variable_normalizada   | origen_etiqueta   |\n",
    "|:-----------------------------------------|----------:|:---------------|:---------|:------|:------|:-----------------|:-----------------------------|:------------------|\n",
    "| dataset_eb_kepler_labeled_fixed.parquet  |  71735011 | True           | True     | False | False | True             | False                        | False             |\n",
    "| dataset_eb_tess_labeled_fixed.parquet    | 120747997 | True           | True     | False | False | True             | False                        | False             |\n",
    "| dataset_k2varcat_labeled_fixed.parquet   |  17895159 | True           | True     | False | False | True             | False                        | True              |\n",
    "| dataset_vsx_tess_labeled_fixed.parquet   |  41143597 | True           | True     | False | False | True             | True                         | True              |\n",
    "| dataset_gaia_dr3_vsx_tic_labeled.parquet |      3987 | True           | True     | False | False | True             | False                        | True              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945974d",
   "metadata": {},
   "source": [
    "#### ✅ Script 5. Fusionar todos los datasets\n",
    "\n",
    "Una vez tengas:\n",
    "\n",
    "| Dataset             | Contenido          | Etiquetas      |\n",
    "| ------------------- | ------------------ | -------------- |\n",
    "| Kepler              | Curvas reales      | `EB`           |\n",
    "| K2VARCAT            | Curvas reales      | Multiclase     |\n",
    "| TESS-EB             | Curvas reales      | `EB`           |\n",
    "| VSX-TIC (TESS) | Curvas reales TESS | Multiclase VSX |\n",
    "| Cruce Gaia DR3 | Curvas reales | Multiclase |\n",
    "\n",
    "Podremos unificar todos en un solo `.parquet` final para modelado.\n",
    "\n",
    "✅ Próximos pasos recomendados\n",
    "Consolidar todos los datasets etiquetados en un único .parquet global:\n",
    "\n",
    "- dataset_eb_kepler_labeled.parquet\n",
    "- dataset_eb_tess_labeled.parquet\n",
    "- dataset_k2varcat_labeled.parquet\n",
    "- dataset_vsx_tess_labeled.parquet\n",
    "- dataset_gaia_dr3_vsx_tic_labeled.parquet\n",
    "\n",
    "Normalizar todos los id_objeto al mismo formato.\n",
    "\n",
    "Preferiblemente: TIC_<id> cuando esté disponible.\n",
    "\n",
    "En Kepler/K2 puedes usar KIC_<id> y EPIC_<id> según misión\n",
    "\n",
    "Asignar columna mision y origen_etiqueta correctamente en cada dataset\n",
    "\n",
    "Guardar todo como: all_missions_labeled.parquet\n",
    "\n",
    "¿Quieres que prepare ahora el script script_5_unify_all_missions.py que realice esa consolidación automática con verificación y exportación final?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd49639",
   "metadata": {},
   "source": [
    "**Dudas antes de continuar con el script de unificacion**: \n",
    "1) No todos los dataset tienen la clase_variable_normalizada, quiza sería mejor normalizar primero, o incluir esa normalizacion en el script de unificacion. \n",
    "2) Al final no llegamos a hacer el cruce de Gaia DR3 con los parquet de las otras misiones porque no teniamos \"ra\" y \"dec\", ¿nos podemos plantear completar ese paso o es mejor abandonar la idea y seguir con lo que tenemos? \n",
    "3) ¿Como de complicado sería incorporar coordenadas al dataset? \n",
    "3) ¿Estamos seguros de que en los ficheros consolidados tenemos todas las variables independientes necesarias para la clasificacion? Si despues de varios días descargando curvas de luz y generando ficheros consolidados, ahora resulta que nos faltan datos sería un contratiempo enorme para mis fechas de entrega. \n",
    "\n",
    "#### ✅ 1) ¿Normalizar `clase_variable` antes o dentro del script de unificación?\n",
    "\n",
    "**Recomendación:** incluir la normalización **dentro** del script de unificación.\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "* Garantiza que todas las clases queden en un único campo `clase_variable_normalizada`\n",
    "* Evita inconsistencias por tener algunos ficheros ya normalizados y otros no\n",
    "* Permite adaptar un único diccionario de equivalencias (`EA` → `Eclipsing Binary`, `RRAB` → `RR Lyrae AB`, etc.)\n",
    "\n",
    "✳️ **Solución técnica:** en el script de consolidación, aplicar `map()` con un diccionario de clases estándar a cada dataset.\n",
    "\n",
    "#### ⚠️ 2) ¿Completar el cruce con Gaia DR3 para Kepler, K2 y TESS usando `ra`/`dec`?\n",
    "\n",
    "Actualmente **no se ha hecho**, porque:\n",
    "\n",
    "* Los ficheros de curvas no incluían coordenadas (`ra`, `dec`) desde el principio.\n",
    "* El cruce con Gaia requiere coordenadas ICRS para hacer el `CIRCLE(...)` de búsqueda.\n",
    "\n",
    "**Opciones ahora:**\n",
    "\n",
    "| Opción                                                       | Viable | Comentario                                                    |\n",
    "| ------------------------------------------------------------ | ------ | ------------------------------------------------------------- |\n",
    "| Recuperar `ra/dec` desde KIC/EPIC/TIC vía catálogos externos | ✅      | Se puede hacer con `astroquery` usando `KIC`, `EPIC`, o `TIC` |\n",
    "| Intentar cruce con Gaia para objetos que ya tienen TIC ID    | ✅      | VSX-TESS y Gaia-VSX ya tienen TIC, podemos complementarlos    |\n",
    "| Abandonar ese cruce y usar solo lo ya procesado              | ✅      | Sería más rápido y permite enfocarte en el modelo             |\n",
    "\n",
    "✳️ **Mi consejo: avanza con lo que tienes consolidado** (es suficiente para entrenar y justificar el modelo). Si sobra tiempo, el cruce Gaia puede añadirse como mejora posterior.\n",
    "\n",
    "#### ❓ 3) ¿Es complicado incorporar `ra`/`dec` ahora?\n",
    "\n",
    "No, **pero depende del origen** de los objetos:\n",
    "\n",
    "* **Kepler**: puedes recuperar coordenadas con `astroquery.mast` + `KIC_ID`\n",
    "* **K2**: igual, usando `EPIC_ID`\n",
    "* **TESS**: puedes usar `astroquery.mast.Catalogs.query_object(\"TIC <id>\", catalog=\"TIC\")`\n",
    "* **VSX-Gaia**: ya vienen de Gaia DR3, así que puedes completar desde `source_id`\n",
    "\n",
    "✳️ ¿Merece la pena hacerlo ahora? **Solo si necesitas coordenadas como feature de entrada o para el cruce con otros catálogos**.\n",
    "\n",
    "#### ✅ 4) ¿Tenemos todas las variables necesarias para entrenar?\n",
    "\n",
    "Sí, **si el objetivo es entrenar el modelo con curvas de luz + clase**, y no con metadatos adicionales.\n",
    "\n",
    "Tus datasets contienen:\n",
    "\n",
    "* `tiempo`, `magnitud`, `error` → necesarios para construir secuencias temporales\n",
    "* `id_objeto`, `mision`, `clase_variable` → para agrupar, etiquetar y evaluar\n",
    "* (algunos tienen `clase_variable_normalizada`, que podemos consolidar)\n",
    "\n",
    "✔️ Eso es suficiente para generar los ejemplos tokenizados y alimentar un Transformer (como ATAT o AstroConformer).\n",
    "\n",
    "#### 🧠 Conclusión operativa\n",
    "\n",
    "| Paso | Acción recomendada                                                               |\n",
    "| ---- | -------------------------------------------------------------------------------- |\n",
    "| 1    | Generar script de unificación con normalización integrada                        |\n",
    "| 2    | Dejar el cruce Gaia como mejora opcional post-entrenamiento                      |\n",
    "| 3    | No invertir tiempo en añadir coordenadas a todos los objetos ahora               |\n",
    "| 4    | Confirmar que el pipeline de clasificación puede iniciarse con lo ya consolidado |\n",
    "\n",
    "\n",
    "=> Preparamos el `script_6_unify_all_missions.py` incluyendo:\n",
    "\n",
    "* la normalización de clases\n",
    "* validación de columnas mínimas\n",
    "* exportación como `all_missions_labeled.parquet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f3a64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffd7409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Detectando tipos reales por columna...\n",
      "🧩 Inferiendo tipos compatibles por columna\n",
      "✅ Esquema global construido con 112 columnas\n",
      "\n",
      "📂 Procesando: dataset_eb_kepler_labeled_fixed.parquet\n",
      "📂 Procesando: dataset_eb_tess_labeled_fixed.parquet\n",
      "📂 Procesando: dataset_k2varcat_labeled_fixed.parquet\n",
      "📂 Procesando: dataset_vsx_tess_labeled_fixed.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n",
      "C:\\Users\\hav3f\\OneDrive - UNIR\\MASTER UNIR - INTELIGENCIA ARTIFICIAL\\ASIGNATURAS\\CUATRIMESTRE 2\\TRABAJO FIN DE MASTER\\IMPLEMENTACION\\src\\script_6_unify_all_missions.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[col] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Procesando: dataset_gaia_dr3_vsx_tic_labeled.parquet\n",
      "⚠️ dataset_gaia_dr3_vsx_tic_labeled.parquet no contiene columnas requeridas: ['tiempo', 'magnitud']. Se omitirá.\n",
      "✅ Dataset final guardado como: data\\processed\\all_missions_labeled.parquet\n",
      "📄 Resumen de clases exportado: processed\\summary\\clase_variable_normalizada_summary.csv\n"
     ]
    }
   ],
   "source": [
    "%run src/script_6_unify_all_missions.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31615a0",
   "metadata": {},
   "source": [
    "🎉 El resultado confirma que:\n",
    "\n",
    "#### ✅ El proceso de consolidación ha sido un éxito\n",
    "\n",
    "* **`all_missions_labeled.parquet`** se ha generado correctamente\n",
    "* Se ha consolidado el esquema completo con **112 columnas**\n",
    "* Se ha aplicado la normalización de clases de forma coherente\n",
    "* El fichero de resumen `clase_variable_normalizada_summary.csv` ha sido exportado correctamente\n",
    "\n",
    "#### ⚠️ Sobre los avisos:\n",
    "\n",
    "```text\n",
    "PerformanceWarning: DataFrame is highly fragmented...\n",
    "```\n",
    "Esto es **solo una advertencia de rendimiento** de Pandas, causada por insertar columnas (`df[col] = None`) repetidamente. **No afecta al resultado** pero podemos optimizarlo más adelante si hace falta velocidad.\n",
    "\n",
    "#### ⚠️ Sobre el mensaje:\n",
    "\n",
    "```\n",
    "⚠️ dataset_gaia_dr3_vsx_tic_labeled.parquet no contiene columnas requeridas: ['tiempo', 'magnitud']. Se omitirá.\n",
    "```\n",
    "Significa que el fichero:\n",
    "\n",
    "```\n",
    "dataset_gaia_dr3_vsx_tic_labeled.parquet\n",
    "```\n",
    "**fue excluido de la consolidación final**, porque no contiene las columnas mínimas necesarias para modelos basados en curvas de luz:\n",
    "\n",
    "* `tiempo`\n",
    "* `magnitud`\n",
    "\n",
    "¿Es correcto que se excluya? **Sí**, por dos razones:\n",
    "\n",
    "1. Este dataset contiene **estadísticas agregadas** de Gaia DR3 (`mean_mag`, `std_dev_mag`, etc.), pero **no curvas de luz crudas** (`tiempo`, `magnitud`, `error`...).\n",
    "2. Por tanto, **no puede ser usado como entrada directa para tu modelo Transformer**, que requiere series temporales completas.\n",
    "\n",
    "Opcionalmente, podríamos:\n",
    "\n",
    "* Usarlo como fuente de *features agregadas* (por ejemplo, concatenadas con embeddings de curvas reales)\n",
    "* Reutilizarlo para un enfoque alternativo basado en clasificación estática\n",
    "* O bien, **dejarlo como paso de exploración científica descartado** en la versión final\n",
    "\n",
    "#### ✅ Conclusión\n",
    "\n",
    "* La omisión es **correcta y deseable**\n",
    "* El dataset Gaia DR3 etiquetado **no se incorpora a `all_missions_labeled.parquet`**\n",
    "* La consolidación final incluye **solo curvas reales con `tiempo` y `magnitud`**\n",
    "\n",
    "\n",
    "#### 📊 Resumen de clases final:\n",
    "\n",
    "| Clase                | Recuento    |\n",
    "| -------------------- | ----------- |\n",
    "| Eclipsing Binary     | 194,215,921 |\n",
    "| Rotational           | 28,300,949  |\n",
    "| Other                | 22,272,945  |\n",
    "| Delta Scuti          | 2,794,650   |\n",
    "| Irregular            | 1,731,817   |\n",
    "| Variable             | 981,653     |\n",
    "| White Dwarf          | 508,231     |\n",
    "| RR Lyrae             | 362,043     |\n",
    "| Cataclysmic          | 215,248     |\n",
    "| Young Stellar Object | 138,307     |\n",
    "\n",
    "Esto da una visión clara del balance de clases antes de tokenizar o entrenar.\n",
    "\n",
    "#### ✅ ¿Siguiente paso?\n",
    "\n",
    "Preparar un `script_7_validate_final_dataset.py` para verificar:\n",
    "\n",
    "* Esquema final (`columns`, `dtypes`)\n",
    "* Valores nulos por campo\n",
    "* Recuento por misión (`mision`)\n",
    "* Recuento por dataset fuente (`source_dataset`)\n",
    "* Balance de clases (`clase_variable_normalizada`)?\n",
    "\n",
    "O pasar directamente al preprocesado para el modelo Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842bfa3",
   "metadata": {},
   "source": [
    "#### ✅ Script t de validación final del dataset `script_7_validate_final_dataset.py`:\n",
    "\n",
    "* Carga el `.parquet` final usando PyArrow en modo eficiente.\n",
    "* Muestra esquema y ejemplo de datos.\n",
    "* Calcular nulos, clases, misiones y datasets por lotes.\n",
    "* Presenta distribución por:\n",
    "  * `clase_variable_normalizada`\n",
    "  * `mision`\n",
    "  * `source_dataset` (si existe)\n",
    "  * Incluye gráficos de barras para:\n",
    "    - clase_variable_normalizada\n",
    "    - mision\n",
    "    - source_dataset (si existe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444093d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from seaborn) (2.2.5)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hav3f\\anaconda3\\envs\\astro_transformer\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "📂 Validando: all_missions_labeled.parquet\n",
      "\n",
      "🧩 Esquema detectado:\n",
      "- tiempo: double\n",
      "- magnitud: double\n",
      "- error: double\n",
      "- quality: double\n",
      "- timecorr: double\n",
      "- centroid_col: double\n",
      "- centroid_row: double\n",
      "- cadenceno: double\n",
      "- sap_flux: double\n",
      "- sap_flux_err: double\n",
      "- sap_bkg: double\n",
      "- sap_bkg_err: double\n",
      "- pdcsap_flux: double\n",
      "- pdcsap_flux_err: double\n",
      "- sap_quality: double\n",
      "- psf_centr1: double\n",
      "- psf_centr1_err: double\n",
      "- psf_centr2: double\n",
      "- psf_centr2_err: double\n",
      "- mom_centr1: double\n",
      "- mom_centr1_err: double\n",
      "- mom_centr2: double\n",
      "- mom_centr2_err: double\n",
      "- pos_corr1: double\n",
      "- pos_corr2: double\n",
      "- id_objeto: string\n",
      "- id_mision: string\n",
      "- mision: string\n",
      "- fecha_inicio: double\n",
      "- fecha_fin: double\n",
      "- clase_variable: string\n",
      "- clase_variable_normalizada: string\n",
      "- source_dataset: string\n",
      "- flux: double\n",
      "- flux_err: double\n",
      "- fcor: double\n",
      "- cbv01: double\n",
      "- cbv02: double\n",
      "- cbv03: double\n",
      "- cbv04: double\n",
      "- cbv05: double\n",
      "- cbv06: double\n",
      "- bkg: double\n",
      "- origen_etiqueta: string\n",
      "- solution_id: double\n",
      "- source_id: double\n",
      "- num_selected_g_fov: string\n",
      "- mean_obs_time_g_fov: double\n",
      "- time_duration_g_fov: string\n",
      "- min_mag_g_fov: string\n",
      "- max_mag_g_fov: string\n",
      "- mean_mag_g_fov: string\n",
      "- median_mag_g_fov: string\n",
      "- range_mag_g_fov: string\n",
      "- trimmed_range_mag_g_fov: string\n",
      "- std_dev_mag_g_fov: string\n",
      "- skewness_mag_g_fov: string\n",
      "- kurtosis_mag_g_fov: string\n",
      "- mad_mag_g_fov: string\n",
      "- abbe_mag_g_fov: string\n",
      "- iqr_mag_g_fov: string\n",
      "- stetson_mag_g_fov: string\n",
      "- std_dev_over_rms_err_mag_g_fov: string\n",
      "- outlier_median_g_fov: string\n",
      "- num_selected_bp: string\n",
      "- mean_obs_time_bp: double\n",
      "- time_duration_bp: string\n",
      "- min_mag_bp: string\n",
      "- max_mag_bp: string\n",
      "- mean_mag_bp: string\n",
      "- median_mag_bp: string\n",
      "- range_mag_bp: string\n",
      "- trimmed_range_mag_bp: string\n",
      "- std_dev_mag_bp: string\n",
      "- skewness_mag_bp: string\n",
      "- kurtosis_mag_bp: string\n",
      "- mad_mag_bp: string\n",
      "- abbe_mag_bp: string\n",
      "- iqr_mag_bp: string\n",
      "- stetson_mag_bp: string\n",
      "- std_dev_over_rms_err_mag_bp: string\n",
      "- outlier_median_bp: string\n",
      "- num_selected_rp: string\n",
      "- mean_obs_time_rp: double\n",
      "- time_duration_rp: string\n",
      "- min_mag_rp: string\n",
      "- max_mag_rp: string\n",
      "- mean_mag_rp: string\n",
      "- median_mag_rp: string\n",
      "- range_mag_rp: string\n",
      "- trimmed_range_mag_rp: string\n",
      "- std_dev_mag_rp: string\n",
      "- skewness_mag_rp: string\n",
      "- kurtosis_mag_rp: string\n",
      "- mad_mag_rp: string\n",
      "- abbe_mag_rp: string\n",
      "- iqr_mag_rp: string\n",
      "- stetson_mag_rp: string\n",
      "- std_dev_over_rms_err_mag_rp: string\n",
      "- outlier_median_rp: string\n",
      "- in_vari_classification_result: bool\n",
      "- in_vari_rrlyrae: bool\n",
      "- in_vari_cepheid: bool\n",
      "- in_vari_planetary_transit: bool\n",
      "- in_vari_short_timescale: bool\n",
      "- in_vari_long_period_variable: bool\n",
      "- in_vari_eclipsing_binary: bool\n",
      "- in_vari_rotation_modulation: bool\n",
      "- in_vari_ms_oscillator: bool\n",
      "- in_vari_agn: bool\n",
      "- in_vari_microlensing: bool\n",
      "- in_vari_compact_companion: bool\n",
      "\n",
      "🔢 Filas totales: 251,521,764\n",
      "\n",
      "🔍 Mostrando 5 filas de ejemplo:\n",
      "\n",
      "       tiempo  magnitud     error  quality  timecorr  centroid_col  \\\n",
      "0  131.512362  1.065252  0.000161      0.0  0.001369     860.09303   \n",
      "1  131.532797  0.974507  0.000158      0.0  0.001369     860.09343   \n",
      "2  131.553231  0.922745  0.000155      0.0  0.001370     860.09353   \n",
      "3  131.573665  0.899605  0.000153      0.0  0.001371     860.09328   \n",
      "4  131.594100  0.943326  0.000155      0.0  0.001372     860.09299   \n",
      "\n",
      "   centroid_row  cadenceno   sap_flux  sap_flux_err  ...  in_vari_cepheid  \\\n",
      "0      45.28267     1105.0  37874.980      5.785242  ...             None   \n",
      "1      45.28292     1106.0  36266.230      5.693093  ...             None   \n",
      "2      45.28252     1107.0  34152.457      5.569287  ...             None   \n",
      "3      45.28227     1108.0  33062.488      5.501959  ...             None   \n",
      "4      45.28302     1109.0  34355.070      5.580349  ...             None   \n",
      "\n",
      "   in_vari_planetary_transit  in_vari_short_timescale  \\\n",
      "0                       None                     None   \n",
      "1                       None                     None   \n",
      "2                       None                     None   \n",
      "3                       None                     None   \n",
      "4                       None                     None   \n",
      "\n",
      "   in_vari_long_period_variable  in_vari_eclipsing_binary  \\\n",
      "0                          None                      None   \n",
      "1                          None                      None   \n",
      "2                          None                      None   \n",
      "3                          None                      None   \n",
      "4                          None                      None   \n",
      "\n",
      "   in_vari_rotation_modulation  in_vari_ms_oscillator  in_vari_agn  \\\n",
      "0                         None                   None         None   \n",
      "1                         None                   None         None   \n",
      "2                         None                   None         None   \n",
      "3                         None                   None         None   \n",
      "4                         None                   None         None   \n",
      "\n",
      "   in_vari_microlensing  in_vari_compact_companion  \n",
      "0                  None                       None  \n",
      "1                  None                       None  \n",
      "2                  None                       None  \n",
      "3                  None                       None  \n",
      "4                  None                       None  \n",
      "\n",
      "[5 rows x 112 columns]\n",
      "\n",
      "🧼 Recuento de nulos:\n",
      "- id_objeto: 0 nulos\n",
      "- tiempo: 0 nulos\n",
      "- magnitud: 0 nulos\n",
      "- clase_variable: 468006 nulos\n",
      "- clase_variable_normalizada: 0 nulos\n",
      "- mision: 0 nulos\n",
      "\n",
      "📊 Recuento por clase normalizada:\n",
      "Eclipsing Binary        194215921\n",
      "Rotational               28300949\n",
      "Other                    22272945\n",
      "Delta Scuti               2794650\n",
      "Irregular                 1731817\n",
      "Variable                   981653\n",
      "White Dwarf                508231\n",
      "RR Lyrae                   362043\n",
      "Cataclysmic                215248\n",
      "Young Stellar Object       138307\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛰️ Recuento por mision:\n",
      "Tess      120747997\n",
      "Kepler     71735011\n",
      "TESS       41143597\n",
      "K2         17895159\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Recuento por dataset fuente:\n",
      "dataset_eb_tess_labeled_fixed.parquet      120747997\n",
      "dataset_eb_kepler_labeled_fixed.parquet     71735011\n",
      "dataset_vsx_tess_labeled_fixed.parquet      41143597\n",
      "dataset_k2varcat_labeled_fixed.parquet      17895159\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%run src/script_7_validate_final_dataset.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6173daa",
   "metadata": {},
   "source": [
    "🔍 Todo apunta a que la consolidación y validación han sido un éxito:\n",
    "\n",
    "#### ✅ Validación completada correctamente\n",
    "\n",
    "* **251 millones de curvas de luz** consolidadas.\n",
    "* **112 columnas unificadas**.\n",
    "* Distribución clara por clase, misión y dataset fuente.\n",
    "* `id_objeto`, `tiempo`, `magnitud`, `mision`, `clase_variable_normalizada` → **sin nulos**.\n",
    "* `clase_variable` tiene \\~468K nulos (esperado: son los casos etiquetados como `\"UNKNOWN\"` o `Other`).\n",
    "\n",
    "#### 📊 Distribuciones visuales\n",
    "\n",
    "Los tres gráficos reflejan:\n",
    "\n",
    "1. **Desbalance evidente** de clases (habrá que tratarlo en el modelo).\n",
    "2. **Duplicidad aparente** en la columna `mision` (`Tess` vs `TESS`). \n",
    "3. Dataset `vsx_tess` y `k2varcat` aportan proporciones menores, pero valiosas.\n",
    "\n",
    "Punto 2 se podría unificar antes de entrenar:\n",
    "\n",
    "```python\n",
    "df[\"mision\"] = df[\"mision\"].str.upper()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
