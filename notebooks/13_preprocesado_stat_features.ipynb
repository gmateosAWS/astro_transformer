{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155786fb",
   "metadata": {},
   "source": [
    "# 13. Preprocesado incorporando features estad√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0d3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de entorno aplicada.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n general para evitar errores de warnings y compatibilidad\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"RICH_NO_RICH\"] = \"1\"\n",
    "print(\"Configuraci√≥n de entorno aplicada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bb6288-931b-4b94-bfb9-b03502ae240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo le√≠do correctamente\n",
      "üßÆ N√∫mero de filas: 626,189,090\n",
      "üìä N√∫mero de columnas: 13\n",
      "üßæ Columnas: ['id', 'time', 'mag', 'mag_err', 'flux', 'flux_err', 'clase_variable', 'clase_variable_normalizada', 'mission', 'mission_id', 'source_dataset', 'label_source', 'band']\n",
      "                             id           time     mag  mag_err     flux  \\\n",
      "0  ASASSN-V J000000.19+320847.2            HJD     mag  mag_err     flux   \n",
      "1  ASASSN-V J000000.19+320847.2  2458017.73473  15.468  0.05599  2.35900   \n",
      "2  ASASSN-V J000000.19+320847.2  2458018.75446   15.39  0.05292  2.53600   \n",
      "3  ASASSN-V J000000.19+320847.2  2458034.87939  15.276  0.04876  2.81700   \n",
      "4  ASASSN-V J000000.19+320847.2  2458035.92739  15.405  0.05350  2.50100   \n",
      "\n",
      "   flux_err clase_variable clase_variable_normalizada mission    mission_id  \\\n",
      "0  flux_err             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "1   0.12152             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "2   0.12347             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "3   0.12638             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "4   0.12309             EW           Eclipsing Binary  ASASSN  ASASSN_gband   \n",
      "\n",
      "  source_dataset    label_source band  \n",
      "0   asassn_gband  ASASSN_Catalog    g  \n",
      "1   asassn_gband  ASASSN_Catalog    g  \n",
      "2   asassn_gband  ASASSN_Catalog    g  \n",
      "3   asassn_gband  ASASSN_Catalog    g  \n",
      "4   asassn_gband  ASASSN_Catalog    g  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta local del parquet consolidado\n",
    "parquet_path = \"/mnt/data/datasets/dataset_consolidado_optimizado.parquet\"\n",
    "\n",
    "# Leer solo la metadata (no carga datos)\n",
    "metadata = pq.read_metadata(parquet_path)\n",
    "\n",
    "print(\"‚úÖ Archivo le√≠do correctamente\")\n",
    "print(f\"üßÆ N√∫mero de filas: {metadata.num_rows:,}\")\n",
    "print(f\"üìä N√∫mero de columnas: {metadata.num_columns}\")\n",
    "print(f\"üßæ Columnas: {[metadata.schema.column(i).name for i in range(metadata.num_columns)]}\")\n",
    "\n",
    "# Abrir el archivo como ParquetFile\n",
    "pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "# Leer las primeras N filas del primer row group (sin cargar todo)\n",
    "sample_table = pf.read_row_group(0)\n",
    "sample_df = sample_table.to_pandas()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(sample_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ea8ef",
   "metadata": {},
   "source": [
    "#### Ajustes en `script_1_transformer_preprocessing_optimizado_2.py`\n",
    "\n",
    "1. **Mantener todo el flujo actual del script original**, sin modificar:\n",
    "\n",
    "   * Agrupaci√≥n por `id`\n",
    "   * Limpieza y normalizaci√≥n de curvas\n",
    "   * Aplicaci√≥n de filtros de descarte\n",
    "   * Generaci√≥n de secuencias normalizadas y m√°scaras\n",
    "   * Split y serializaci√≥n\n",
    "\n",
    "2. **A√±adir el c√°lculo de nuevas features por curva**, en `process_single_curve`:\n",
    "\n",
    "   * `median`, `iqr`, `amplitude`, `std`, `min`, `max`, `skew`, `kurtosis`, etc.\n",
    "   * Se almacenar√°n como array adicional por curva (ej. `stats = [std, median, iqr, amplitude, min, max, skew, kurtosis]`)\n",
    "\n",
    "3. **Modificar la clase `LightCurveDataset` para aceptar y devolver tambi√©n las features estad√≠sticas por curva**.\n",
    "\n",
    "4. **A√±adir las features estad√≠sticas como cuarto elemento en los datasets** (`train_dataset`, `val_dataset`, `test_dataset`).\n",
    "\n",
    "5. **Exportar los datasets como `.pt` igual que antes**, pero ahora con 4 elementos por muestra: `(secuencia, clase, m√°scara, features)`.\n",
    "\n",
    "| Feature     | Descripci√≥n breve           | Comentario           |\n",
    "| ----------- | --------------------------- | -------------------- |\n",
    "| `std`       | Dispersi√≥n general          | ‚úÖ √∫til               |\n",
    "| `iqr`       | Rango intercuart√≠lico       | ‚úÖ robusto            |\n",
    "| `amplitude` | Rango total (m√°x - m√≠n)     | ‚úÖ clave en variables |\n",
    "| `median`    | Nivel base de brillo        | ‚úÖ estable            |\n",
    "| `mad`       | Desviaci√≥n absoluta mediana | ‚úÖ robusto a outliers |\n",
    "| `skewness`    | Asimetr√≠a          | ‚úÖ √ötil para distinguir formas: eclipsantes vs sinusoidales. |\n",
    "| `kurtosis`    | Curtosis           | ‚úÖ Distingue si hay picos muy pronunciados o m√°s suavidad.   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd710f27-d4f6-49c8-8b89-2e9d7901a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# A√±adir la ra√≠z del proyecto al path\n",
    "ROOT_DIR = Path.cwd().parent  # <- sube un nivel para alcanzar la ra√≠z del proyecto\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "# Ignorar solo los RuntimeWarning de numpy (como overflows en reduce)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy\")\n",
    "\n",
    "from src.fase2.script_1_transformer_preprocessing_optimizado_2 import main as preprocessing_optimized_with_features\n",
    "\n",
    "max_per_class_override={\n",
    "    \"Irregular\": 9000,\n",
    "    \"Rotational\": 9000,\n",
    "    \"Eclipsing Binary\": 9000,\n",
    "    \"Delta Scuti\": None,            # 7.550 ‚Üí TODAS\n",
    "    \"RR Lyrae\": 9000,               # 41.208 ‚Üí TODAS NO\n",
    "    \"Young Stellar Object\": None,   # 9.809 ‚Üí TODAS\n",
    "    \"Cataclysmic\": None,            # 2.080 ‚Üí TODAS\n",
    "    \"White Dwarf\": 0,               # 0 ‚Üí LA ELIMINAMOS\n",
    "    \"Variable\": 0                   # 0 ‚Üí LA ELIMINAMOS\n",
    "}\n",
    "\n",
    "preprocessing_optimized_with_features(\n",
    "    seq_length=25000,\n",
    "    max_per_class=None, # usamos override completo\n",
    "    max_per_class_override=max_per_class_override,\n",
    "    parquet_batch_size=10_000_000,\n",
    "    dataloader_batch_size=128,\n",
    "    num_workers=16,\n",
    "    #errores_csv_path=Path(\"../outputs/errores_mal_clasificados.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd7a11b-9b0f-4475-9176-a5c2c57b1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature          mean           std        min           max  nan_count  \\\n",
      "0        std  1.126377e+12  5.036000e+13  -0.609101  2.252754e+15          0   \n",
      "1        iqr  1.532549e+01  4.931359e+02  -0.525688  2.115110e+04          0   \n",
      "2  amplitude  5.559076e-01  4.580447e+00  -0.619319  8.089397e+01          0   \n",
      "3     median -1.165967e-01  1.034070e+00  -5.776203  2.455565e+00          0   \n",
      "4        mad  2.262454e+00  8.560209e+01  -0.549252  3.829006e+03          0   \n",
      "5       skew  3.387466e-01  1.941506e+00 -11.842187  1.163362e+01          0   \n",
      "6   kurtosis  7.411687e-01  2.001349e+00  -0.886497  7.705595e+00          0   \n",
      "\n",
      "   inf_count  zeros_count  \n",
      "0          0            0  \n",
      "1          0            2  \n",
      "2          0            0  \n",
      "3          0            0  \n",
      "4          0            8  \n",
      "5          0            0  \n",
      "6          0            0  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset\n",
    "dataset = torch.load(\"../data/train/train_dataset.pt\")\n",
    "\n",
    "# Limitar a 2000 ejemplos como m√°ximo\n",
    "num_samples = min(len(dataset), 2000)\n",
    "\n",
    "# Extraer solo los vectores de features\n",
    "features_list = []\n",
    "for i in range(num_samples):\n",
    "    _, _, _, features = dataset[i]\n",
    "    if features.dim() == 1 and features.size(0) == 7:\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convertir a numpy array\n",
    "features_tensor = torch.stack(features_list)\n",
    "features_np = features_tensor.numpy()\n",
    "\n",
    "# Definir nombres\n",
    "feature_names = [\"std\", \"iqr\", \"amplitude\", \"median\", \"mad\", \"skew\", \"kurtosis\"]\n",
    "\n",
    "# Construir resumen\n",
    "summary = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"mean\": np.mean(features_np, axis=0),\n",
    "    \"std\": np.std(features_np, axis=0),\n",
    "    \"min\": np.min(features_np, axis=0),\n",
    "    \"max\": np.max(features_np, axis=0),\n",
    "    \"nan_count\": np.isnan(features_np).sum(axis=0),\n",
    "    \"inf_count\": np.isinf(features_np).sum(axis=0),\n",
    "    \"zeros_count\": np.sum(features_np == 0, axis=0)\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134de10-f4be-468d-a363-08da993f0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from src.utils.normalization_dict import normalize_label\n",
    "\n",
    "# Ruta al archivo en cach√©\n",
    "cache_path = \"../data/train/grouped_data.pkl\"\n",
    "\n",
    "# Cargar el archivo en cach√©\n",
    "with open(cache_path, \"rb\") as f:\n",
    "    grouped_data = pickle.load(f)\n",
    "\n",
    "# Contar las clases en el archivo\n",
    "class_counts = Counter(group.iloc[0][\"clase_variable_normalizada\"] for group in grouped_data.values())\n",
    "\n",
    "# Contar las clases en el archivo (normalizadas)\n",
    "class_counts = Counter(normalize_label(group.iloc[0][\"clase_variable_normalizada\"]) for group in grouped_data.values())\n",
    "\n",
    "# Mostrar el recuento de clases\n",
    "print(\"\\nüìä Recuento de clases en el archivo en cach√© (normalizadas):\")\n",
    "for clase, count in class_counts.items():\n",
    "    print(f\"{clase}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
